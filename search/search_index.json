{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning Course Website Course Information Sheet When and Where Lectures : Tuesdays 15:00-17:00 in SS 2117 Tutorials : Thursday 13:00-14:00 in SS 2117 Office hours : Wednesdays 11:30-12:30 in Bahen (BA) 2283","title":"About"},{"location":"#csc4122506-winter-2019-probabilistic-learning-and-reasoning","text":"Course Website Course Information Sheet","title":"CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning"},{"location":"#when-and-where","text":"Lectures : Tuesdays 15:00-17:00 in SS 2117 Tutorials : Thursday 13:00-14:00 in SS 2117 Office hours : Wednesdays 11:30-12:30 in Bahen (BA) 2283","title":"When and Where"},{"location":"sample_final/","text":"Sample Final These are question pulled from the lecture, assignments and the sample midterm, alongside questions that were written based on the study guide. These were not given by an instructor and are merely guesses as to what kind of questions might be on the final. Note See the study guide here . Week 1 Tutorial 1 Question 1 Recall that the definition of an exponential family model is: f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) where \\eta \\eta are the parameters T(x) T(x) are the sufficient statistics h(x) h(x) is the base measure g(\\eta) g(\\eta) is the normalizing constant Consider the univariate Gaussian, with mean \\mu \\mu and precision \\lambda = \\frac{1}{\\sigma^2} \\lambda = \\frac{1}{\\sigma^2} p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) What are \\eta \\eta and T(x) T(x) for this distribution when represented in exponential family form? ANSWER Start by expanding the terms in the exponent = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ from here, we can rearrange the exponent into \\eta^TT(x) \\eta^TT(x) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ where \\eta^T = \\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\eta^T = \\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix} Week 2: Introduction to Probabilistic Models Question 1 In this question, we'll fit a naive Bayes model to the MNIST digits using maximum likelihood. Naive Bayes defines the joint probability of the each datapoint x x and its class label c c as follows: \\begin{align} p(x, c | \\theta, \\pi) = p(c | \\pi) p(x | c, \\theta_c) = p(c | \\pi) \\prod_{d=1}^{784} p( x_d | c, \\theta_{cd}) \\end{align} For binary data, we can use the Bernoulli likelihood: \\begin{align} p( x_d | c, \\theta_{cd}) = Ber(x_d | \\theta_{cd}) = \\theta_{cd}^{x_d} ( 1 - \\theta_{cd})^{(1 - x_d)} \\end{align} Which is just a way of expressing that p(x_d = 1 | c, \\theta_{cd}) = \\theta_{cd} p(x_d = 1 | c, \\theta_{cd}) = \\theta_{cd} . For p(c | \\pi) p(c | \\pi) , we can just use a categorical distribution: \\begin{align} p(c | \\pi) = Cat(c|\\pi) = \\pi_c \\end{align} Note that we need \\sum_{i=0}^9 \\pi_{i} = 1 \\sum_{i=0}^9 \\pi_{i} = 1 . (a) Derive the maximum likelihood estimate (MLE) for the class-conditional pixel means \\theta \\theta . Hint: We saw in lecture that MLE can be thought of as 'counts' for the data, so what should \\hat \\theta_{cd} \\hat \\theta_{cd} be counting? (b) Derive the maximum a posteriori (MAP) estimate for the class-conditional pixel means \\theta \\theta , using a Beta(2, 2) Beta(2, 2) prior on each \\theta \\theta . Hint: it has a simple final form, and you can ignore the Beta normalizing constant. ANSWER (a) The maximum likelihood estimate of the class-conditional pixel means \\theta \\theta for class c c is given by \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\prod_{i=1}^N \\prod_{d=1}^{784}\\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\prod_{i=1}^N \\prod_{d=1}^{784}\\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} where N N is the number of training examples. Taking the log, we get \\begin{align*} = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\bigg ) \\Bigg ] \\\\ = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} x_d^{(i)} \\log \\theta_{cd} + (1-x_d^{(i)}) \\log (1 - \\theta_{cd}) \\Bigg ] \\\\ = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\Bigg ] \\\\ \\end{align*} \\begin{align*} &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\bigg ) \\Bigg ] \\\\ &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} x_d^{(i)} \\log \\theta_{cd} + (1-x_d^{(i)}) \\log (1 - \\theta_{cd}) \\Bigg ] \\\\ & = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\Bigg ] \\\\ \\end{align*} where n_c^{d=1} n_c^{d=1} is a vector of counts containing the number of training examples of class c c where d=1 d=1 for each pixel dimension d d in 784, n_c^{d=0} n_c^{d=0} is the corresponding count vector for the number of training examples of class c c where d = 0 d = 0 , and \\cdot \\cdot denotes the dot product. Taking the derivative of this expression and setting it to 0, we can solve for the MLE of the parameters \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\bigg ] = 0 \\\\ \\frac{n_c^{d=1}}{\\theta_{c}} - \\frac{n_c^{d=0}}{1 - \\theta_{c}} = 0 \\end{align*} \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\bigg ] &= 0 \\\\ \\frac{n_c^{d=1}}{\\theta_{c}} - \\frac{n_c^{d=0}}{1 - \\theta_{c}} & = 0 \\end{align*} Rearranging, we get \\hat \\theta_c = \\frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \\frac{n_c^{d=1}}{N} \\hat \\theta_c = \\frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \\frac{n_c^{d=1}}{N} therefore, the MLE for the class-conditional pixel means \\theta \\theta of class c c , \\hat \\theta_c \\hat \\theta_c , is given by the number of examples of class c c where d=1 d=1 divided by the total number of examples of class c c , as expected. (b) The prior probability of our class-conditional pixel means, \\theta \\theta for class c c is given by f(\\theta_c; \\alpha, \\beta) = f(\\theta_c; 2, 2) = \\frac{1}{B(2, 2)}\\theta_c^{2 - 1}(1-\\theta_c)^{2-1} = \\theta_c(1-\\theta_c) f(\\theta_c; \\alpha, \\beta) = f(\\theta_c; 2, 2) = \\frac{1}{B(2, 2)}\\theta_c^{2 - 1}(1-\\theta_c)^{2-1} = \\theta_c(1-\\theta_c) where we have ignored the Beta normalizing constant. The MAP estimate of the class-conditional pixel means \\theta \\theta for class c c is given by \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\theta_c(1-\\theta_c) \\prod_{i=1}^N \\prod_{d=1}^{784} \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ] \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\theta_c(1-\\theta_c) \\prod_{i=1}^N \\prod_{d=1}^{784} \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ] Taking the log, we get \\begin{align*} \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log \\big ( \\theta_c(1-\\theta_c) \\big ) + \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\Bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ) \\Bigg ] \\\\ = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\Bigg ] \\end{align*} \\begin{align*} \\hat \\theta_c &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log \\big ( \\theta_c(1-\\theta_c) \\big ) + \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\Bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ) \\Bigg ] \\\\ &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\Bigg ] \\end{align*} taking the derivative of this expression and setting it to 0, we can solve for the MAP estimate of the parameters \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\bigg ] = 0 \\\\ \\frac{1 + n_c^{d=1}}{\\theta_c} - \\frac{1 + n_c^{d=0}}{(1-\\theta_c)} = 0 \\\\ \\end{align*} \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\bigg ] &= 0 \\\\ \\frac{1 + n_c^{d=1}}{\\theta_c} - \\frac{1 + n_c^{d=0}}{(1-\\theta_c)} &= 0 \\\\ \\end{align*} Rearranging, we get \\hat \\theta_c = \\frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \\frac{n_c^{d=1} + 1}{N + 2} \\hat \\theta_c = \\frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \\frac{n_c^{d=1} + 1}{N + 2} Week 3: Directed Graphical Models Question 1 When we condition on y y , are x x and z z independent? (a) (b) (c) ANSWER (a) From the graph, we get P(x, y, z) = P(x)P(y|x)P(z|y) P(x, y, z) = P(x)P(y|x)P(z|y) which implies \\begin{align} P(z | x, y) = \\frac{P(x, y, z)}{P(x, y)} \\\\ = \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ = P(z | y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x, y, z)}{P(x, y)} \\\\ &= \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ &= P(z | y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) = P(z | y) P(z | x, y) = P(z | y) and so by \\star\\star \\star\\star , x \\bot z | y x \\bot z | y . (b) From the graph, we get P(x, y, z) = P(y)P(x|y)P(z|y) P(x, y, z) = P(y)P(x|y)P(z|y) which implies \\begin{align} P(x, z | y) = \\frac{P(x, y, z)}{P(y)} \\\\ = \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ = P(x|y)P(z|y) \\\\ \\end{align} \\begin{align} P(x, z | y) &= \\frac{P(x, y, z)}{P(y)} \\\\ &= \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ &= P(x|y)P(z|y) \\\\ \\end{align} \\therefore \\therefore P(x, z| y) = P(x|y)P(z|y) P(x, z| y) = P(x|y)P(z|y) and so by \\star \\star , x \\bot z | y x \\bot z | y . (c) From the graph, we get P(x, y, z) = P(x)P(z)P(y|x, z) P(x, y, z) = P(x)P(z)P(y|x, z) which implies \\begin{align} P(z | x, y) = \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ = \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ \\not = P(z|y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ &= \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ &\\not = P(z|y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) \\not = P(z|y) P(z | x, y) \\not = P(z|y) and so by \\star\\star \\star\\star , x \\not \\bot z | y x \\not \\bot z | y . In fact, x x and z z are marginally independent , but given y y they are conditionally dependent . This important effect is called explaining away ( Berkson\u2019s paradox ). Example Imaging flipping two coins independently, represented by events x x and z z . Furthermore, let y=1 y=1 if the coins come up the same and y=0 y=0 if they come up differently. Clearly, x x and z z are independent, but if I tell you y y , they become coupled! Question 2 (a) In the following graph, is x_1 \\bot x_6 | \\{x_2, x_3\\} x_1 \\bot x_6 | \\{x_2, x_3\\} ? (b) In the following graph, is x_2 \\bot x_3 | \\{x_1, x_6\\} x_2 \\bot x_3 | \\{x_1, x_6\\} ? ANSWER (a) Yes, by the Bayes Balls algorithm. (b) No, by the Bayes Balls algorithm. Question 3 Consider the following directed graphical model: (a) List all variables that are independent of A A given evidence on B B (b) Write down the factorized normalized joint distribution that this graphical model represents. (c) If each node is a single discrete random variable in {1, ..., K} {1, ..., K} how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set? ANSWER (a) By Bayes' Balls, no variables are conditionally independent of A A given evidence on B B . (b) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) (c) For each node (random variable) there is k k states. There are k^n k^n possible configurations where k k is the number of states and n n the number of nodes ( x_{\\pi_i} x_{\\pi_i} ) \\therefore \\text{number of possible configurations} = k^9 \\therefore \\text{number of possible configurations} = k^9 Question 4 Consider the Hidden Markov Model (a) Assume you are able to sample from these conditional distributions, i.e. x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) Write down a step-by-step process to produce a sample observation from this model, i.e. (x_1, x_2, x_3, ..., x_T) (x_1, x_2, x_3, ..., x_T) in terms of samples from the individual factors. ANSWER We want to sample a sequence of observations x_1, x_2, x_3, ..., x_T x_1, x_2, x_3, ..., x_T from the model according to x_{1:T} \\sim \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) x_{1:T} \\sim \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) since observations x_t x_t are independent of one another. Notice that this forms a chain, with probability p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] Step-by-step Start with t=1 t=1 Sample z_t z_t according to z_t \\sim p(z_1) \\prod_{i=2}^{t} p(Z_i | z_{i-1}) z_t \\sim p(z_1) \\prod_{i=2}^{t} p(Z_i | z_{i-1}) Given the sampled z_t z_t , sample x_t x_t according to x_t \\sim \\ p(X_t | z_t) x_t \\sim \\ p(X_t | z_t) Increment t t by 1 Repeat steps 2-4 until t=T t=T Week 4: Undirected Graphical Models Question 1 (a) State the Global, Local and Pairwise Markov properties used to determine conditional independence in a undirected graphical model. Given the following UGMs: (b) (c) use each to Markov property to give an example of two sets of conditionally independent nodes in the graph. ANSWER (a) def . Global Markov Property (G): X_A \\bot X_B | X_C X_A \\bot X_B | X_C iff X_C X_C separates X_A X_A from X_B X_B def . Local Markov Property (Markov Blanket) (L): The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph t \\bot \\mathcal V \\setminus cl(t) | mb(t) t \\bot \\mathcal V \\setminus cl(t) | mb(t) def . Pairwise (Markov) Property (P): The set of nodes that renders two nodes, s s and t t , conditionally independent of each other. s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 (b) Global: \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} Local: 1 \\bot \\text{rest} | \\{2, 3\\} 1 \\bot \\text{rest} | \\{2, 3\\} Pairwise: 1 \\bot 7 | \\text{rest} 1 \\bot 7 | \\text{rest} (c) Global: \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} Local: 1 \\bot \\text{rest} | \\{X_2, X_6\\} 1 \\bot \\text{rest} | \\{X_2, X_6\\} Pairwise: 1 \\bot 20 | \\text{rest} 1 \\bot 20 | \\text{rest} Question 2 Given the following graph: (a) What is a maximal clique? State one example from the graph. (b) What is a maximum clique? State on example from the graph. (c) Write down the factorized joint distribution that this graphical model represents ANSWER (a) def . A maximal clique is a clique that cannot be extended by including one more adjacent vertex. def . A maximum clique is a clique of the largest possible size in a given graph. (b) A maximal clique is show in blue, while a maximum clique is shown in green. (c) p(x_1, ..., x_7) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) p(x_1, ..., x_7) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) Question 3 Compare and contrast directed vs undirected graphical models: ANSWER DGMs UGMs Represented by Directed graph Undirected Graphs Nodes specify Random variables Random variables Edges specify Conditional dependence between variables Probabilistic interactions Graph factorizes according to Local conditional probabilities Potential functions (or factors), one per maximal clique Parameterized by Conditional probability tables (if random variables are discrete) Tables of non-negative, relative affinities (if random variables are discrete) Week 5: Exact Inference Question 1 Given the graph (a) Suppose we want to compute the partition function ( Z(\\theta) Z(\\theta) , see here ) using the elimination ordering \\prec= (1, 2, 3, 4, 5, 6) \\prec= (1, 2, 3, 4, 5, 6) . If we use the variable elimination algorithm , we will create new intermediate factors. What is the largest intermediate factor? (b) Add an edge to the original MRF between every pair of variables that end up in the same factor (These are called fill in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph? (c) Now consider elimination ordering \\prec= (4, 1, 2, 3, 5, 6) \\prec= (4, 1, 2, 3, 5, 6) . If we use the variable elimination algorithm , we will create new intermediate factors. What is the largest intermediate factor? (d) Add an edge to the original MRF between every pair of variables that end up in the same factor (These are called fill in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph? ANSWER a) The size of the maximum factor is 3. The set of potentials given by the graph is \\Psi = \\{\\psi_{X_1, X_2}(X_1, X_2), \\psi_{X_1, X_3}(X_1, X_3), \\psi_{X_2, X_4}(X_2, X_4), \\psi_{X_3, X_4}(X_3, X_4), \\psi_{X_4, X_5}(X_4, X_5), \\psi_{X_5, X_6}(X_5, X_6) \\} \\Psi = \\{\\psi_{X_1, X_2}(X_1, X_2), \\psi_{X_1, X_3}(X_1, X_3), \\psi_{X_2, X_4}(X_2, X_4), \\psi_{X_3, X_4}(X_3, X_4), \\psi_{X_4, X_5}(X_4, X_5), \\psi_{X_5, X_6}(X_5, X_6) \\} and the joint probability is therefore p(X_1, ..., X_6) \\propto \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) p(X_1, ..., X_6) \\propto \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) finally, the partition function with elimination ordering \\prec= (1, 2, 3, 4, 5, 6) \\prec= (1, 2, 3, 4, 5, 6) is given by \\begin{align} \\tau(X) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ = \\sum_{x_6}\\sum_{x_5}\\sum_{x_4}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} \\begin{align} \\tau(X) &= \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ &= \\sum_{x_6}\\sum_{x_5}\\sum_{x_4}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} Carrying out the elimination, (not shown here), we get intermediate factors \\{\\tau_1(x_2, x_3), \\tau_2(x_3, x_4), \\tau_3(x_4), \\tau_4(x_5), \\tau_5(x_6)\\} \\{\\tau_1(x_2, x_3), \\tau_2(x_3, x_4), \\tau_3(x_4), \\tau_4(x_5), \\tau_5(x_6)\\} \\therefore \\therefore the maximum factor is of size 3. b) The only edge that does not already exist is the edge between X_2 X_2 and X_3 X_3 (added by intermediate factor \\tau_1(x_2, x_3) \\tau_1(x_2, x_3) ). The largest maximal clique is now of size 3 ( \\{x_2, x_3, x_4\\} \\{x_2, x_3, x_4\\} ). (c) The partition function with elimination ordering \\prec= (4, 1, 2, 3, 5, 6) \\prec= (4, 1, 2, 3, 5, 6) is given by \\begin{align} \\tau(X) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ = \\sum_{x_6}\\sum_{x_5}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1}\\sum_{x_4} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} \\begin{align} \\tau(X) &= \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ &= \\sum_{x_6}\\sum_{x_5}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1}\\sum_{x_4} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} Carrying out the elimination, (not shown here), we get intermediate factors \\{\\tau_1(x_2, x_3, x_5), \\tau_2(x_2, x_3, x_5), \\tau_3(x_3, x_5), \\tau_4(x_5), \\tau_5(x_6)\\} \\{\\tau_1(x_2, x_3, x_5), \\tau_2(x_2, x_3, x_5), \\tau_3(x_3, x_5), \\tau_4(x_5), \\tau_5(x_6)\\} \\therefore \\therefore the maximum factor is of size 4. d) The added edges are between X_2 X_2 and X_3 X_3 (added by intermediate factors \\tau_1(x_2, x_3, x_5) \\tau_1(x_2, x_3, x_5) / \\tau_2(x_2, x_3, x_5) \\tau_2(x_2, x_3, x_5) ) X_2 X_2 and X_5 X_5 (added by intermediate factors \\tau_1(x_2, x_3, x_5) \\tau_1(x_2, x_3, x_5) / \\tau_2(x_2, x_3, x_5) \\tau_2(x_2, x_3, x_5) ) X_3 X_3 and X_5 X_5 (added by intermediate factors \\tau_1(x_2, x_3, x_5) \\tau_1(x_2, x_3, x_5) , \\tau_2(x_2, x_3, x_5) \\tau_2(x_2, x_3, x_5) , \\tau_3(x_3, x_5) \\tau_3(x_3, x_5) ) The largest maximal clique is now of size 4 ( \\{x_2, x_3, x_4, x_5\\} \\{x_2, x_3, x_4, x_5\\} ). Week 6 Week 8: Sampling and Monte Carlo Methods Question 1 Given some data \\{x^{(r)}\\}^R_{r=1} \\sim p(x) \\{x^{(r)}\\}^R_{r=1} \\sim p(x) , the simple Monte Carlo estimator is \\Phi = \\mathbb E_{x \\sim p(x)}[\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi \\Phi = \\mathbb E_{x \\sim p(x)}[\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi (a) Show that this estimator is unbiased. (b) Show that, as the number of samples of R R increases, the variance of \\hat \\Phi \\hat \\Phi will decrease proportional to \\frac{1}{R} \\frac{1}{R} . ANSWER (a) To show that \\hat \\Phi \\hat \\Phi is an unbiased estimator of \\Phi \\Phi we must show that for random samples \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} generated from p(x) p(x) , the expectation of \\hat \\Phi \\hat \\Phi is \\Phi \\Phi : \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square (b) \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square Question 2 Starting from \\Phi = \\int \\phi(x)p(x)dx \\Phi = \\int \\phi(x)p(x)dx , derive the importance weighted estimator \\hat \\Phi_{iw} \\hat \\Phi_{iw} given \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} ANSWER \\begin{align} \\Phi = \\int \\phi(x)p(x)dx \\\\ = \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} \\\\ = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ = \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ = \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ = \\hat \\Phi_{iw} \\end{align} \\begin{align} \\Phi &= \\int \\phi(x)p(x)dx \\\\ &= \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ &\\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} \\\\ &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ &= \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ &= \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ &= \\hat \\Phi_{iw} \\end{align} where \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r , w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} and \\hat \\Phi_{iw} \\hat \\Phi_{iw} is our importance weighted estimator. Week 9: Hidden Markov Models Question 1 Assume x x is a discrete random variable with k k states. How many parameters are needed to parameterize (a) x_t x_t ? (b) A first-order Markov chain? (c) An m m -ordered Markov chain? ANSWER (a) k-1 k-1 , as the last state is implicit. (b) k(k-1) k(k-1) , as we need k k number of parameters for each parameter of x_t x_t (c) k^m(k-1) k^m(k-1) , as we need k^m k^m number of parameters for each parameter of x_t x_t Question 2 Say we have the following simple chain where x_t \\in [N, Z, A] x_t \\in [N, Z, A] z_t \\in [H, S] z_t \\in [H, S] where our observed states are whether or not we are watching Netflix ( N N ), sleeping ( Z Z ), or working on the assignment ( A A ) and our hidden states are whether we are happy ( H H ) or sad ( S S ). Say futher that we are given the initial ( \\pi \\pi ), transition ( T T ), and emission probabilities ( \\varepsilon \\varepsilon ) \\pi \\pi H 0.70 S 0.30 \\varepsilon \\varepsilon N Z A H 0.40 0.50 0.10 S 0.10 0.30 0.60 T H S H 0.80 0.20 S 0.10 0.90 Note It is the rows of these tables that need to sum to 1, not the columns! From these conditional probabilities, compute p(z_3 = H | z_1 = S) = ? p(z_3 = H | z_1 = S) = ? p(x_3 = A | z_1 = S) = ? p(x_3 = A | z_1 = S) = ? ANSWER p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ and p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515 p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515 Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI) Question 1 Starting from the Kullback\u2013Leibler divergence ( D_{KL} D_{KL} ), derive the Evidence Lower Bound (ELBO) for a true distribution p_\\theta (z | x) p_\\theta (z | x) and approximate distribution q_\\phi(z|x) q_\\phi(z|x) and show that maximizing the ELBO is equivalent to minimizing D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) . ANSWER \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ = E_{z_\\phi \\sim q_\\phi} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ = E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + E_{z_\\phi \\sim q_\\phi} \\log p_\\theta(x) \\\\ = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ &= E_{z_\\phi \\sim q_\\phi} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ &= E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + E_{z_\\phi \\sim q_\\phi} \\log p_\\theta(x) \\\\ &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} Where \\mathcal L(\\theta, \\phi ; x) \\mathcal L(\\theta, \\phi ; x) is the ELBO . Rearranging, \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= \\log p_\\theta(x) \\\\ \\end{align} because D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\therefore \\therefore maximizing the ELBO \\Rightarrow \\Rightarrow minimizing D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) . Week 12: Generative Adversarial Networks (GANs) Question 1 Compare and contrast explicit vs. implicit density models. ANSWER Explicit density models define an explicit density function p_{model}(x ; \\theta) p_{model}(x ; \\theta) which is used to train the model, typically via maximum likelihood estimation. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill. e.g. for the i.i.d case: \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) which is equivalent to the minimizing the KL divergence between p_{model}(x) p_{model}(x) and p_{model}(x ; \\theta) p_{model}(x ; \\theta) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) when p_{model}\\theta(x ; \\theta) p_{model}\\theta(x ; \\theta) is intractable, we use variational (e.g. VAEs) or MCMC approximations to compute the likelihood. In contrast, implicit density models are trained without explicitly defining a density function. The way we interact with p_{model} p_{model} is through samples. An example a General Adversarial Networks (GANs). Question 2 The goal of the discriminator is to minimize J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] with respect to \\theta_D \\theta_D . Imagine that the discriminator can be optimized in function space, so the value of D(x) D(x) is specified independently for every value of x. (a) What is the optimal strategy for the discriminator, D D ? (b) What assumptions need to be made to obtain this result? ANSWER (b) We begin by assuming that both p_{data} p_{data} and p_{model} p_{model} are nonzero everywhere. Note If we do not make this assumption, then some points are never visited during training, and have undefined behavior. (a) To minimize J^{(D)} J^{(D)} with respect to D D , we can write down the functional derivatives with respect to a single entry D^{(x)} D^{(x)} , and set them equal to zero: \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 By solving this equation, we obtain D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} Estimating this ratio is the key approximation mechanism used by GANs. Question 3 The goal of the generator is to minimize J^{(G)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(G)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] with respect to \\theta_G \\theta_G . What is the optimal strategy for the generator, G G ? ANSWER The optimal behavior of the generator is to try and confuse the discriminator . We can frame this as a zero-sum game , in which the sum of all player\u2019s costs is always zero. In this version of the game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} Because J^{(G)} J^{(G)} is tied directly to J^{(D)} J^{(D)} , we can summarize the entire game with a value function specifying the discriminator\u2019s payoff: V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) Zero-sum games are also called minimax games because their solution involves minimization in an outer loop and maximization in an inner loop: \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) Where we want to maximize \\theta_D \\theta_D such that D_{\\theta_D}(x) = 1 D_{\\theta_D}(x) = 1 D_{\\theta_D}(G(z)) = 0 D_{\\theta_D}(G(z)) = 0 and minimize \\theta_G \\theta_G such that D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1 D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1 Question 4 The cost used for the generator in the minimax game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} is useful for theoretical analysis, but does not perform especially well in practice. (a) Explain what the saturating problem in GANs is. (b) Explain the heuristic, non-saturating game used to solve this problem. ANSWER (a) In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, e.g. D(G(z)) = 0 D(G(z)) = 0 , the generator\u2019s gradient vanishes (the cost function becomes flat!). This is known as the saturation problem . (b) To solve this problem, we introduce the non-saturating game, in which we continue to use cross-entropy minimization for the generator, but instead of flipping the sign on the discriminator\u2019s cost to obtain a cost for the generator, we flip the target used to construct the cross-entropy cost J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) This leads to a strong gradient signal at D(G(z)) = 0 D(G(z)) = 0 . In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the log- probability of the discriminator being mistaken.","title":"Sample Final"},{"location":"sample_final/#sample-final","text":"These are question pulled from the lecture, assignments and the sample midterm, alongside questions that were written based on the study guide. These were not given by an instructor and are merely guesses as to what kind of questions might be on the final. Note See the study guide here .","title":"Sample Final"},{"location":"sample_final/#week-1","text":"","title":"Week 1"},{"location":"sample_final/#tutorial-1","text":"","title":"Tutorial 1"},{"location":"sample_final/#question-1","text":"Recall that the definition of an exponential family model is: f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) where \\eta \\eta are the parameters T(x) T(x) are the sufficient statistics h(x) h(x) is the base measure g(\\eta) g(\\eta) is the normalizing constant Consider the univariate Gaussian, with mean \\mu \\mu and precision \\lambda = \\frac{1}{\\sigma^2} \\lambda = \\frac{1}{\\sigma^2} p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) What are \\eta \\eta and T(x) T(x) for this distribution when represented in exponential family form? ANSWER Start by expanding the terms in the exponent = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ from here, we can rearrange the exponent into \\eta^TT(x) \\eta^TT(x) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ where \\eta^T = \\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\eta^T = \\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}","title":"Question 1"},{"location":"sample_final/#week-2-introduction-to-probabilistic-models","text":"","title":"Week 2: Introduction to Probabilistic Models"},{"location":"sample_final/#question-1_1","text":"In this question, we'll fit a naive Bayes model to the MNIST digits using maximum likelihood. Naive Bayes defines the joint probability of the each datapoint x x and its class label c c as follows: \\begin{align} p(x, c | \\theta, \\pi) = p(c | \\pi) p(x | c, \\theta_c) = p(c | \\pi) \\prod_{d=1}^{784} p( x_d | c, \\theta_{cd}) \\end{align} For binary data, we can use the Bernoulli likelihood: \\begin{align} p( x_d | c, \\theta_{cd}) = Ber(x_d | \\theta_{cd}) = \\theta_{cd}^{x_d} ( 1 - \\theta_{cd})^{(1 - x_d)} \\end{align} Which is just a way of expressing that p(x_d = 1 | c, \\theta_{cd}) = \\theta_{cd} p(x_d = 1 | c, \\theta_{cd}) = \\theta_{cd} . For p(c | \\pi) p(c | \\pi) , we can just use a categorical distribution: \\begin{align} p(c | \\pi) = Cat(c|\\pi) = \\pi_c \\end{align} Note that we need \\sum_{i=0}^9 \\pi_{i} = 1 \\sum_{i=0}^9 \\pi_{i} = 1 . (a) Derive the maximum likelihood estimate (MLE) for the class-conditional pixel means \\theta \\theta . Hint: We saw in lecture that MLE can be thought of as 'counts' for the data, so what should \\hat \\theta_{cd} \\hat \\theta_{cd} be counting? (b) Derive the maximum a posteriori (MAP) estimate for the class-conditional pixel means \\theta \\theta , using a Beta(2, 2) Beta(2, 2) prior on each \\theta \\theta . Hint: it has a simple final form, and you can ignore the Beta normalizing constant. ANSWER (a) The maximum likelihood estimate of the class-conditional pixel means \\theta \\theta for class c c is given by \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\prod_{i=1}^N \\prod_{d=1}^{784}\\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\prod_{i=1}^N \\prod_{d=1}^{784}\\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} where N N is the number of training examples. Taking the log, we get \\begin{align*} = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\bigg ) \\Bigg ] \\\\ = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} x_d^{(i)} \\log \\theta_{cd} + (1-x_d^{(i)}) \\log (1 - \\theta_{cd}) \\Bigg ] \\\\ = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\Bigg ] \\\\ \\end{align*} \\begin{align*} &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\bigg ) \\Bigg ] \\\\ &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\sum_{i=1}^N \\sum_{d=1}^{784} x_d^{(i)} \\log \\theta_{cd} + (1-x_d^{(i)}) \\log (1 - \\theta_{cd}) \\Bigg ] \\\\ & = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\Bigg ] \\\\ \\end{align*} where n_c^{d=1} n_c^{d=1} is a vector of counts containing the number of training examples of class c c where d=1 d=1 for each pixel dimension d d in 784, n_c^{d=0} n_c^{d=0} is the corresponding count vector for the number of training examples of class c c where d = 0 d = 0 , and \\cdot \\cdot denotes the dot product. Taking the derivative of this expression and setting it to 0, we can solve for the MLE of the parameters \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\bigg ] = 0 \\\\ \\frac{n_c^{d=1}}{\\theta_{c}} - \\frac{n_c^{d=0}}{1 - \\theta_{c}} = 0 \\end{align*} \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ n_c^{d=1} \\cdot \\log \\theta_{c} + n_c^{d=0} \\cdot \\log (1 - \\theta_{c}) \\bigg ] &= 0 \\\\ \\frac{n_c^{d=1}}{\\theta_{c}} - \\frac{n_c^{d=0}}{1 - \\theta_{c}} & = 0 \\end{align*} Rearranging, we get \\hat \\theta_c = \\frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \\frac{n_c^{d=1}}{N} \\hat \\theta_c = \\frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \\frac{n_c^{d=1}}{N} therefore, the MLE for the class-conditional pixel means \\theta \\theta of class c c , \\hat \\theta_c \\hat \\theta_c , is given by the number of examples of class c c where d=1 d=1 divided by the total number of examples of class c c , as expected. (b) The prior probability of our class-conditional pixel means, \\theta \\theta for class c c is given by f(\\theta_c; \\alpha, \\beta) = f(\\theta_c; 2, 2) = \\frac{1}{B(2, 2)}\\theta_c^{2 - 1}(1-\\theta_c)^{2-1} = \\theta_c(1-\\theta_c) f(\\theta_c; \\alpha, \\beta) = f(\\theta_c; 2, 2) = \\frac{1}{B(2, 2)}\\theta_c^{2 - 1}(1-\\theta_c)^{2-1} = \\theta_c(1-\\theta_c) where we have ignored the Beta normalizing constant. The MAP estimate of the class-conditional pixel means \\theta \\theta for class c c is given by \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\theta_c(1-\\theta_c) \\prod_{i=1}^N \\prod_{d=1}^{784} \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ] \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\theta_c(1-\\theta_c) \\prod_{i=1}^N \\prod_{d=1}^{784} \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ] Taking the log, we get \\begin{align*} \\hat \\theta_c = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log \\big ( \\theta_c(1-\\theta_c) \\big ) + \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\Bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ) \\Bigg ] \\\\ = \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\Bigg ] \\end{align*} \\begin{align*} \\hat \\theta_c &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log \\big ( \\theta_c(1-\\theta_c) \\big ) + \\sum_{i=1}^N \\sum_{d=1}^{784} \\log \\Bigg ( \\theta^{x_d^{(i)}}_{cd}(1 - \\theta_{cd})^{(1-x_d^{(i)})} \\Bigg ) \\Bigg ] \\\\ &= \\operatorname*{argmax}_{\\theta_c} \\Bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\Bigg ] \\end{align*} taking the derivative of this expression and setting it to 0, we can solve for the MAP estimate of the parameters \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\bigg ] = 0 \\\\ \\frac{1 + n_c^{d=1}}{\\theta_c} - \\frac{1 + n_c^{d=0}}{(1-\\theta_c)} = 0 \\\\ \\end{align*} \\begin{align*} \\Rightarrow \\frac{\\partial}{\\partial \\theta_c} \\bigg [ \\log(\\theta_c) + \\log(1-\\theta_c) + n_c^{d=1} \\cdot \\log(\\theta_c) + n_c^{d=0} \\cdot \\log(1 - \\theta_c) \\bigg ] &= 0 \\\\ \\frac{1 + n_c^{d=1}}{\\theta_c} - \\frac{1 + n_c^{d=0}}{(1-\\theta_c)} &= 0 \\\\ \\end{align*} Rearranging, we get \\hat \\theta_c = \\frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \\frac{n_c^{d=1} + 1}{N + 2} \\hat \\theta_c = \\frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \\frac{n_c^{d=1} + 1}{N + 2}","title":"Question 1"},{"location":"sample_final/#week-3-directed-graphical-models","text":"","title":"Week 3: Directed Graphical Models"},{"location":"sample_final/#question-1_2","text":"When we condition on y y , are x x and z z independent? (a) (b) (c) ANSWER (a) From the graph, we get P(x, y, z) = P(x)P(y|x)P(z|y) P(x, y, z) = P(x)P(y|x)P(z|y) which implies \\begin{align} P(z | x, y) = \\frac{P(x, y, z)}{P(x, y)} \\\\ = \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ = P(z | y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x, y, z)}{P(x, y)} \\\\ &= \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ &= P(z | y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) = P(z | y) P(z | x, y) = P(z | y) and so by \\star\\star \\star\\star , x \\bot z | y x \\bot z | y . (b) From the graph, we get P(x, y, z) = P(y)P(x|y)P(z|y) P(x, y, z) = P(y)P(x|y)P(z|y) which implies \\begin{align} P(x, z | y) = \\frac{P(x, y, z)}{P(y)} \\\\ = \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ = P(x|y)P(z|y) \\\\ \\end{align} \\begin{align} P(x, z | y) &= \\frac{P(x, y, z)}{P(y)} \\\\ &= \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ &= P(x|y)P(z|y) \\\\ \\end{align} \\therefore \\therefore P(x, z| y) = P(x|y)P(z|y) P(x, z| y) = P(x|y)P(z|y) and so by \\star \\star , x \\bot z | y x \\bot z | y . (c) From the graph, we get P(x, y, z) = P(x)P(z)P(y|x, z) P(x, y, z) = P(x)P(z)P(y|x, z) which implies \\begin{align} P(z | x, y) = \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ = \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ \\not = P(z|y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ &= \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ &\\not = P(z|y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) \\not = P(z|y) P(z | x, y) \\not = P(z|y) and so by \\star\\star \\star\\star , x \\not \\bot z | y x \\not \\bot z | y . In fact, x x and z z are marginally independent , but given y y they are conditionally dependent . This important effect is called explaining away ( Berkson\u2019s paradox ). Example Imaging flipping two coins independently, represented by events x x and z z . Furthermore, let y=1 y=1 if the coins come up the same and y=0 y=0 if they come up differently. Clearly, x x and z z are independent, but if I tell you y y , they become coupled!","title":"Question 1"},{"location":"sample_final/#question-2","text":"(a) In the following graph, is x_1 \\bot x_6 | \\{x_2, x_3\\} x_1 \\bot x_6 | \\{x_2, x_3\\} ? (b) In the following graph, is x_2 \\bot x_3 | \\{x_1, x_6\\} x_2 \\bot x_3 | \\{x_1, x_6\\} ? ANSWER (a) Yes, by the Bayes Balls algorithm. (b) No, by the Bayes Balls algorithm.","title":"Question 2"},{"location":"sample_final/#question-3","text":"Consider the following directed graphical model: (a) List all variables that are independent of A A given evidence on B B (b) Write down the factorized normalized joint distribution that this graphical model represents. (c) If each node is a single discrete random variable in {1, ..., K} {1, ..., K} how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set? ANSWER (a) By Bayes' Balls, no variables are conditionally independent of A A given evidence on B B . (b) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) (c) For each node (random variable) there is k k states. There are k^n k^n possible configurations where k k is the number of states and n n the number of nodes ( x_{\\pi_i} x_{\\pi_i} ) \\therefore \\text{number of possible configurations} = k^9 \\therefore \\text{number of possible configurations} = k^9","title":"Question 3"},{"location":"sample_final/#question-4","text":"Consider the Hidden Markov Model (a) Assume you are able to sample from these conditional distributions, i.e. x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) Write down a step-by-step process to produce a sample observation from this model, i.e. (x_1, x_2, x_3, ..., x_T) (x_1, x_2, x_3, ..., x_T) in terms of samples from the individual factors. ANSWER We want to sample a sequence of observations x_1, x_2, x_3, ..., x_T x_1, x_2, x_3, ..., x_T from the model according to x_{1:T} \\sim \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) x_{1:T} \\sim \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) since observations x_t x_t are independent of one another. Notice that this forms a chain, with probability p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] Step-by-step Start with t=1 t=1 Sample z_t z_t according to z_t \\sim p(z_1) \\prod_{i=2}^{t} p(Z_i | z_{i-1}) z_t \\sim p(z_1) \\prod_{i=2}^{t} p(Z_i | z_{i-1}) Given the sampled z_t z_t , sample x_t x_t according to x_t \\sim \\ p(X_t | z_t) x_t \\sim \\ p(X_t | z_t) Increment t t by 1 Repeat steps 2-4 until t=T t=T","title":"Question 4"},{"location":"sample_final/#week-4-undirected-graphical-models","text":"","title":"Week 4: Undirected Graphical Models"},{"location":"sample_final/#question-1_3","text":"(a) State the Global, Local and Pairwise Markov properties used to determine conditional independence in a undirected graphical model. Given the following UGMs: (b) (c) use each to Markov property to give an example of two sets of conditionally independent nodes in the graph. ANSWER (a) def . Global Markov Property (G): X_A \\bot X_B | X_C X_A \\bot X_B | X_C iff X_C X_C separates X_A X_A from X_B X_B def . Local Markov Property (Markov Blanket) (L): The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph t \\bot \\mathcal V \\setminus cl(t) | mb(t) t \\bot \\mathcal V \\setminus cl(t) | mb(t) def . Pairwise (Markov) Property (P): The set of nodes that renders two nodes, s s and t t , conditionally independent of each other. s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 (b) Global: \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} Local: 1 \\bot \\text{rest} | \\{2, 3\\} 1 \\bot \\text{rest} | \\{2, 3\\} Pairwise: 1 \\bot 7 | \\text{rest} 1 \\bot 7 | \\text{rest} (c) Global: \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} Local: 1 \\bot \\text{rest} | \\{X_2, X_6\\} 1 \\bot \\text{rest} | \\{X_2, X_6\\} Pairwise: 1 \\bot 20 | \\text{rest} 1 \\bot 20 | \\text{rest}","title":"Question 1"},{"location":"sample_final/#question-2_1","text":"Given the following graph: (a) What is a maximal clique? State one example from the graph. (b) What is a maximum clique? State on example from the graph. (c) Write down the factorized joint distribution that this graphical model represents ANSWER (a) def . A maximal clique is a clique that cannot be extended by including one more adjacent vertex. def . A maximum clique is a clique of the largest possible size in a given graph. (b) A maximal clique is show in blue, while a maximum clique is shown in green. (c) p(x_1, ..., x_7) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) p(x_1, ..., x_7) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7)","title":"Question 2"},{"location":"sample_final/#question-3_1","text":"Compare and contrast directed vs undirected graphical models: ANSWER DGMs UGMs Represented by Directed graph Undirected Graphs Nodes specify Random variables Random variables Edges specify Conditional dependence between variables Probabilistic interactions Graph factorizes according to Local conditional probabilities Potential functions (or factors), one per maximal clique Parameterized by Conditional probability tables (if random variables are discrete) Tables of non-negative, relative affinities (if random variables are discrete)","title":"Question 3"},{"location":"sample_final/#week-5-exact-inference","text":"","title":"Week 5: Exact Inference"},{"location":"sample_final/#question-1_4","text":"Given the graph (a) Suppose we want to compute the partition function ( Z(\\theta) Z(\\theta) , see here ) using the elimination ordering \\prec= (1, 2, 3, 4, 5, 6) \\prec= (1, 2, 3, 4, 5, 6) . If we use the variable elimination algorithm , we will create new intermediate factors. What is the largest intermediate factor? (b) Add an edge to the original MRF between every pair of variables that end up in the same factor (These are called fill in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph? (c) Now consider elimination ordering \\prec= (4, 1, 2, 3, 5, 6) \\prec= (4, 1, 2, 3, 5, 6) . If we use the variable elimination algorithm , we will create new intermediate factors. What is the largest intermediate factor? (d) Add an edge to the original MRF between every pair of variables that end up in the same factor (These are called fill in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph? ANSWER a) The size of the maximum factor is 3. The set of potentials given by the graph is \\Psi = \\{\\psi_{X_1, X_2}(X_1, X_2), \\psi_{X_1, X_3}(X_1, X_3), \\psi_{X_2, X_4}(X_2, X_4), \\psi_{X_3, X_4}(X_3, X_4), \\psi_{X_4, X_5}(X_4, X_5), \\psi_{X_5, X_6}(X_5, X_6) \\} \\Psi = \\{\\psi_{X_1, X_2}(X_1, X_2), \\psi_{X_1, X_3}(X_1, X_3), \\psi_{X_2, X_4}(X_2, X_4), \\psi_{X_3, X_4}(X_3, X_4), \\psi_{X_4, X_5}(X_4, X_5), \\psi_{X_5, X_6}(X_5, X_6) \\} and the joint probability is therefore p(X_1, ..., X_6) \\propto \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) p(X_1, ..., X_6) \\propto \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) finally, the partition function with elimination ordering \\prec= (1, 2, 3, 4, 5, 6) \\prec= (1, 2, 3, 4, 5, 6) is given by \\begin{align} \\tau(X) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ = \\sum_{x_6}\\sum_{x_5}\\sum_{x_4}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} \\begin{align} \\tau(X) &= \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ &= \\sum_{x_6}\\sum_{x_5}\\sum_{x_4}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} Carrying out the elimination, (not shown here), we get intermediate factors \\{\\tau_1(x_2, x_3), \\tau_2(x_3, x_4), \\tau_3(x_4), \\tau_4(x_5), \\tau_5(x_6)\\} \\{\\tau_1(x_2, x_3), \\tau_2(x_3, x_4), \\tau_3(x_4), \\tau_4(x_5), \\tau_5(x_6)\\} \\therefore \\therefore the maximum factor is of size 3. b) The only edge that does not already exist is the edge between X_2 X_2 and X_3 X_3 (added by intermediate factor \\tau_1(x_2, x_3) \\tau_1(x_2, x_3) ). The largest maximal clique is now of size 3 ( \\{x_2, x_3, x_4\\} \\{x_2, x_3, x_4\\} ). (c) The partition function with elimination ordering \\prec= (4, 1, 2, 3, 5, 6) \\prec= (4, 1, 2, 3, 5, 6) is given by \\begin{align} \\tau(X) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ = \\sum_{x_6}\\sum_{x_5}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1}\\sum_{x_4} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} \\begin{align} \\tau(X) &= \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, x_{Scope[\\psi] \\cap X}) \\\\ &= \\sum_{x_6}\\sum_{x_5}\\sum_{x_3}\\sum_{x_2}\\sum_{x_1}\\sum_{x_4} \\psi_{X_1, X_2}(X_1, X_2)\\psi_{X_1, X_3}(X_1, X_3)\\psi_{X_2, X_4}(X_2, X_4)\\psi_{X_3, X_4}(X_3, X_4)\\psi_{X_4, X_5}(X_4, X_5)\\psi_{X_5, X_6}(X_5, X_6) \\end{align} Carrying out the elimination, (not shown here), we get intermediate factors \\{\\tau_1(x_2, x_3, x_5), \\tau_2(x_2, x_3, x_5), \\tau_3(x_3, x_5), \\tau_4(x_5), \\tau_5(x_6)\\} \\{\\tau_1(x_2, x_3, x_5), \\tau_2(x_2, x_3, x_5), \\tau_3(x_3, x_5), \\tau_4(x_5), \\tau_5(x_6)\\} \\therefore \\therefore the maximum factor is of size 4. d) The added edges are between X_2 X_2 and X_3 X_3 (added by intermediate factors \\tau_1(x_2, x_3, x_5) \\tau_1(x_2, x_3, x_5) / \\tau_2(x_2, x_3, x_5) \\tau_2(x_2, x_3, x_5) ) X_2 X_2 and X_5 X_5 (added by intermediate factors \\tau_1(x_2, x_3, x_5) \\tau_1(x_2, x_3, x_5) / \\tau_2(x_2, x_3, x_5) \\tau_2(x_2, x_3, x_5) ) X_3 X_3 and X_5 X_5 (added by intermediate factors \\tau_1(x_2, x_3, x_5) \\tau_1(x_2, x_3, x_5) , \\tau_2(x_2, x_3, x_5) \\tau_2(x_2, x_3, x_5) , \\tau_3(x_3, x_5) \\tau_3(x_3, x_5) ) The largest maximal clique is now of size 4 ( \\{x_2, x_3, x_4, x_5\\} \\{x_2, x_3, x_4, x_5\\} ).","title":"Question 1"},{"location":"sample_final/#week-6","text":"","title":"Week 6"},{"location":"sample_final/#week-8-sampling-and-monte-carlo-methods","text":"","title":"Week 8: Sampling and Monte Carlo Methods"},{"location":"sample_final/#question-1_5","text":"Given some data \\{x^{(r)}\\}^R_{r=1} \\sim p(x) \\{x^{(r)}\\}^R_{r=1} \\sim p(x) , the simple Monte Carlo estimator is \\Phi = \\mathbb E_{x \\sim p(x)}[\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi \\Phi = \\mathbb E_{x \\sim p(x)}[\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi (a) Show that this estimator is unbiased. (b) Show that, as the number of samples of R R increases, the variance of \\hat \\Phi \\hat \\Phi will decrease proportional to \\frac{1}{R} \\frac{1}{R} . ANSWER (a) To show that \\hat \\Phi \\hat \\Phi is an unbiased estimator of \\Phi \\Phi we must show that for random samples \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} generated from p(x) p(x) , the expectation of \\hat \\Phi \\hat \\Phi is \\Phi \\Phi : \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square (b) \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square","title":"Question 1"},{"location":"sample_final/#question-2_2","text":"Starting from \\Phi = \\int \\phi(x)p(x)dx \\Phi = \\int \\phi(x)p(x)dx , derive the importance weighted estimator \\hat \\Phi_{iw} \\hat \\Phi_{iw} given \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} ANSWER \\begin{align} \\Phi = \\int \\phi(x)p(x)dx \\\\ = \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} \\\\ = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ = \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ = \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ = \\hat \\Phi_{iw} \\end{align} \\begin{align} \\Phi &= \\int \\phi(x)p(x)dx \\\\ &= \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ &\\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} \\\\ &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ &= \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ &= \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ &= \\hat \\Phi_{iw} \\end{align} where \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r , w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} and \\hat \\Phi_{iw} \\hat \\Phi_{iw} is our importance weighted estimator.","title":"Question 2"},{"location":"sample_final/#week-9-hidden-markov-models","text":"","title":"Week 9: Hidden Markov Models"},{"location":"sample_final/#question-1_6","text":"Assume x x is a discrete random variable with k k states. How many parameters are needed to parameterize (a) x_t x_t ? (b) A first-order Markov chain? (c) An m m -ordered Markov chain? ANSWER (a) k-1 k-1 , as the last state is implicit. (b) k(k-1) k(k-1) , as we need k k number of parameters for each parameter of x_t x_t (c) k^m(k-1) k^m(k-1) , as we need k^m k^m number of parameters for each parameter of x_t x_t","title":"Question 1"},{"location":"sample_final/#question-2_3","text":"Say we have the following simple chain where x_t \\in [N, Z, A] x_t \\in [N, Z, A] z_t \\in [H, S] z_t \\in [H, S] where our observed states are whether or not we are watching Netflix ( N N ), sleeping ( Z Z ), or working on the assignment ( A A ) and our hidden states are whether we are happy ( H H ) or sad ( S S ). Say futher that we are given the initial ( \\pi \\pi ), transition ( T T ), and emission probabilities ( \\varepsilon \\varepsilon ) \\pi \\pi H 0.70 S 0.30 \\varepsilon \\varepsilon N Z A H 0.40 0.50 0.10 S 0.10 0.30 0.60 T H S H 0.80 0.20 S 0.10 0.90 Note It is the rows of these tables that need to sum to 1, not the columns! From these conditional probabilities, compute p(z_3 = H | z_1 = S) = ? p(z_3 = H | z_1 = S) = ? p(x_3 = A | z_1 = S) = ? p(x_3 = A | z_1 = S) = ? ANSWER p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ and p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515 p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515","title":"Question 2"},{"location":"sample_final/#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi","text":"","title":"Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)"},{"location":"sample_final/#question-1_7","text":"Starting from the Kullback\u2013Leibler divergence ( D_{KL} D_{KL} ), derive the Evidence Lower Bound (ELBO) for a true distribution p_\\theta (z | x) p_\\theta (z | x) and approximate distribution q_\\phi(z|x) q_\\phi(z|x) and show that maximizing the ELBO is equivalent to minimizing D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) . ANSWER \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ = E_{z_\\phi \\sim q_\\phi} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ = E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + E_{z_\\phi \\sim q_\\phi} \\log p_\\theta(x) \\\\ = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ &= E_{z_\\phi \\sim q_\\phi} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ &= E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + E_{z_\\phi \\sim q_\\phi} \\log p_\\theta(x) \\\\ &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} Where \\mathcal L(\\theta, \\phi ; x) \\mathcal L(\\theta, \\phi ; x) is the ELBO . Rearranging, \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= \\log p_\\theta(x) \\\\ \\end{align} because D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\therefore \\therefore maximizing the ELBO \\Rightarrow \\Rightarrow minimizing D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) .","title":"Question 1"},{"location":"sample_final/#week-12-generative-adversarial-networks-gans","text":"","title":"Week 12: Generative Adversarial Networks (GANs)"},{"location":"sample_final/#question-1_8","text":"Compare and contrast explicit vs. implicit density models. ANSWER Explicit density models define an explicit density function p_{model}(x ; \\theta) p_{model}(x ; \\theta) which is used to train the model, typically via maximum likelihood estimation. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill. e.g. for the i.i.d case: \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) which is equivalent to the minimizing the KL divergence between p_{model}(x) p_{model}(x) and p_{model}(x ; \\theta) p_{model}(x ; \\theta) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) when p_{model}\\theta(x ; \\theta) p_{model}\\theta(x ; \\theta) is intractable, we use variational (e.g. VAEs) or MCMC approximations to compute the likelihood. In contrast, implicit density models are trained without explicitly defining a density function. The way we interact with p_{model} p_{model} is through samples. An example a General Adversarial Networks (GANs).","title":"Question 1"},{"location":"sample_final/#question-2_4","text":"The goal of the discriminator is to minimize J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] with respect to \\theta_D \\theta_D . Imagine that the discriminator can be optimized in function space, so the value of D(x) D(x) is specified independently for every value of x. (a) What is the optimal strategy for the discriminator, D D ? (b) What assumptions need to be made to obtain this result? ANSWER (b) We begin by assuming that both p_{data} p_{data} and p_{model} p_{model} are nonzero everywhere. Note If we do not make this assumption, then some points are never visited during training, and have undefined behavior. (a) To minimize J^{(D)} J^{(D)} with respect to D D , we can write down the functional derivatives with respect to a single entry D^{(x)} D^{(x)} , and set them equal to zero: \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 By solving this equation, we obtain D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} Estimating this ratio is the key approximation mechanism used by GANs.","title":"Question 2"},{"location":"sample_final/#question-3_2","text":"The goal of the generator is to minimize J^{(G)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(G)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] with respect to \\theta_G \\theta_G . What is the optimal strategy for the generator, G G ? ANSWER The optimal behavior of the generator is to try and confuse the discriminator . We can frame this as a zero-sum game , in which the sum of all player\u2019s costs is always zero. In this version of the game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} Because J^{(G)} J^{(G)} is tied directly to J^{(D)} J^{(D)} , we can summarize the entire game with a value function specifying the discriminator\u2019s payoff: V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) Zero-sum games are also called minimax games because their solution involves minimization in an outer loop and maximization in an inner loop: \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) Where we want to maximize \\theta_D \\theta_D such that D_{\\theta_D}(x) = 1 D_{\\theta_D}(x) = 1 D_{\\theta_D}(G(z)) = 0 D_{\\theta_D}(G(z)) = 0 and minimize \\theta_G \\theta_G such that D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1 D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1","title":"Question 3"},{"location":"sample_final/#question-4_1","text":"The cost used for the generator in the minimax game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} is useful for theoretical analysis, but does not perform especially well in practice. (a) Explain what the saturating problem in GANs is. (b) Explain the heuristic, non-saturating game used to solve this problem. ANSWER (a) In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, e.g. D(G(z)) = 0 D(G(z)) = 0 , the generator\u2019s gradient vanishes (the cost function becomes flat!). This is known as the saturation problem . (b) To solve this problem, we introduce the non-saturating game, in which we continue to use cross-entropy minimization for the generator, but instead of flipping the sign on the discriminator\u2019s cost to obtain a cost for the generator, we flip the target used to construct the cross-entropy cost J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) This leads to a strong gradient signal at D(G(z)) = 0 D(G(z)) = 0 . In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the log- probability of the discriminator being mistaken.","title":"Question 4"},{"location":"sample_midterm/","text":"Sample Midterm Things to know for the midterm Bayes' rule, sum and product rules of probability, expectations Conditioning, normalization, marginalization Exponential family distributions, maximum likelihood Logistic regression, Naive Bayes Converting graphical models to pdfs and back Determining conditional independence DAGs vs UGMs vs factor graphs Computational complexity of inference Note Take this with a grain of salt. Appears to be a list given in the previous version of the course (we did not discuss logistic regression in lecture, for example). Question 1 Recall that the definition of an exponential family model is: f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) where \\eta \\eta are the parameters T(x) T(x) are the sufficient statistics h(x) h(x) is the base measure g(\\eta) g(\\eta) is the normalizing constant Consider the univariate Gaussian, with mean \\mu \\mu and precision \\lambda = \\frac{1}{\\sigma^2} \\lambda = \\frac{1}{\\sigma^2} p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) What are \\eta \\eta and T(x) T(x) for this distribution when represented in exponential family form? ANSWER Start by expanding the terms in the exponent = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ from here, we can rearrange the exponent into \\eta^TT(x) \\eta^TT(x) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ where \\eta^T = \\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\eta^T = \\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix} Question 2 Consider the following directed graphical model: (a) List all variables that are independent of A A given evidence on B B By Bayes' Balls, no variables are conditionally independent of A A given evidence on B B . (b) Write down the factorized normalized joint distribution that this graphical model represents. p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) (c) If each node is a single discrete random variable in {1, ..., K} {1, ..., K} how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set? For each node (random variable) there is k k states. There are k^n k^n possible configurations where k k is the number of states and n n the number of nodes ( x_{\\pi_i} x_{\\pi_i} ) \\therefore \\text{number of possible configurations} = k^9 \\therefore \\text{number of possible configurations} = k^9 Question 3 ANSWER a) The size of the maximum factor is 3. b) The size of the maximum clique is 3. c) The size of the maximum factor is 4. d) The size of the maximum clique is 4. Question 4 Consider the Hidden Markov Model (a) Assume you are able to sample from these conditional distributions, i.e. x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) Write down a step-by-step process to produce a sample observation from this model, i.e. (x_1, x_2, x_3, ..., x_T) (x_1, x_2, x_3, ..., x_T) in terms of samples from the individual factors. ANSWER We want to sample a sequence of observations x_1, x_2, x_3, ..., x_T x_1, x_2, x_3, ..., x_T from the model according to x_{1:T} = \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) x_{1:T} = \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) since observations x_t x_t are independent of one another. Notice that this forms a chain, with probability p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] Step-by-step Start with t=1 t=1 Sample z_t z_t according to z_t \\sim p(z_1) \\prod_{i=t}^{t + 1} p(Z_i | z_{i-1}) z_t \\sim p(z_1) \\prod_{i=t}^{t + 1} p(Z_i | z_{i-1}) Given the sampled z_t z_t , sample x_t x_t according to x_t \\sim \\ p(X_t | z_t) x_t \\sim \\ p(X_t | z_t) Increment t t by 1 Repeat steps 2-4 until t=T t=T","title":"Sample Midterm"},{"location":"sample_midterm/#sample-midterm","text":"Things to know for the midterm Bayes' rule, sum and product rules of probability, expectations Conditioning, normalization, marginalization Exponential family distributions, maximum likelihood Logistic regression, Naive Bayes Converting graphical models to pdfs and back Determining conditional independence DAGs vs UGMs vs factor graphs Computational complexity of inference Note Take this with a grain of salt. Appears to be a list given in the previous version of the course (we did not discuss logistic regression in lecture, for example).","title":"Sample Midterm"},{"location":"sample_midterm/#question-1","text":"Recall that the definition of an exponential family model is: f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) f(x | \\eta) = h(x)g(\\eta)\\exp(\\eta^TT(x)) where \\eta \\eta are the parameters T(x) T(x) are the sufficient statistics h(x) h(x) is the base measure g(\\eta) g(\\eta) is the normalizing constant Consider the univariate Gaussian, with mean \\mu \\mu and precision \\lambda = \\frac{1}{\\sigma^2} \\lambda = \\frac{1}{\\sigma^2} p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) p(D | \\mu \\lambda) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(-\\frac{\\lambda}{2}(x_i - \\mu)^2) What are \\eta \\eta and T(x) T(x) for this distribution when represented in exponential family form? ANSWER Start by expanding the terms in the exponent = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}} \\exp(\\sum_{i=1}^N -\\frac{\\lambda}{2}x_i^2 + \\lambda u x_i - \\frac{\\lambda}{2}\\mu^2) \\\\ from here, we can rearrange the exponent into \\eta^TT(x) \\eta^TT(x) = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ = \\prod^N_{i=1}(\\frac{\\lambda}{2\\pi})^{\\frac{1}{2}}\\exp(\\sum_{i=1}^N - \\frac{\\lambda}{2}\\mu^2)\\exp(\\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}) \\\\ where \\eta^T = \\begin{bmatrix}\\lambda u -\\frac{\\lambda}{2} \\dotsc \\lambda u -\\frac{\\lambda}{2}\\end{bmatrix} \\eta^T = \\begin{bmatrix}\\lambda u & -\\frac{\\lambda}{2} & \\dotsc & \\lambda u & -\\frac{\\lambda}{2}\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix} T(x) = \\begin{bmatrix}x_1 \\\\ x_1^2 \\\\ \\vdots \\\\ x_N \\\\ x_N^2\\end{bmatrix}","title":"Question 1"},{"location":"sample_midterm/#question-2","text":"Consider the following directed graphical model: (a) List all variables that are independent of A A given evidence on B B By Bayes' Balls, no variables are conditionally independent of A A given evidence on B B . (b) Write down the factorized normalized joint distribution that this graphical model represents. p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) (c) If each node is a single discrete random variable in {1, ..., K} {1, ..., K} how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set? For each node (random variable) there is k k states. There are k^n k^n possible configurations where k k is the number of states and n n the number of nodes ( x_{\\pi_i} x_{\\pi_i} ) \\therefore \\text{number of possible configurations} = k^9 \\therefore \\text{number of possible configurations} = k^9","title":"Question 2"},{"location":"sample_midterm/#question-3","text":"ANSWER a) The size of the maximum factor is 3. b) The size of the maximum clique is 3. c) The size of the maximum factor is 4. d) The size of the maximum clique is 4.","title":"Question 3"},{"location":"sample_midterm/#question-4","text":"Consider the Hidden Markov Model (a) Assume you are able to sample from these conditional distributions, i.e. x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) x_i \\sim p(X_i \\ | \\ \\text{parents of } X_i) Write down a step-by-step process to produce a sample observation from this model, i.e. (x_1, x_2, x_3, ..., x_T) (x_1, x_2, x_3, ..., x_T) in terms of samples from the individual factors. ANSWER We want to sample a sequence of observations x_1, x_2, x_3, ..., x_T x_1, x_2, x_3, ..., x_T from the model according to x_{1:T} = \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) x_{1:T} = \\prod_{t=1}^T p(X_t \\ | \\ \\text{parents of } X_t) since observations x_t x_t are independent of one another. Notice that this forms a chain, with probability p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] p(x_{1:T}) \\sim \\bigg [ \\prod_{t=1}^T p(X_t | z_t) \\bigg ] \\bigg [ p(z_1) \\prod_{t=2}^T p(Z_t | z_{t-1}) \\bigg ] Step-by-step Start with t=1 t=1 Sample z_t z_t according to z_t \\sim p(z_1) \\prod_{i=t}^{t + 1} p(Z_i | z_{i-1}) z_t \\sim p(z_1) \\prod_{i=t}^{t + 1} p(Z_i | z_{i-1}) Given the sampled z_t z_t , sample x_t x_t according to x_t \\sim \\ p(X_t | z_t) x_t \\sim \\ p(X_t | z_t) Increment t t by 1 Repeat steps 2-4 until t=T t=T","title":"Question 4"},{"location":"lectures/week_1/","text":"Week 1: Introduction Assigned Reading Murphy: Chapters 1 and 2 Chapter 2 of David Mackay's textbook Overview Course information Overview of ML with examples Ungraded, anonymous background quiz Textbook and Resources There is no required textbook, but optional reading will be assigned each week Kevin Murphy (2012), Machine Learning: A Probabilistic Perspective . David MacKay (2003) Information Theory, Inference, and Learning Algorithms . The David MacKay textbook is recommended, although 100% of tested material come from class. In this course, lecture slides are more a supplement then main content. The most important stuff will be done on the blackboard, so it is important to come to class with pen and paper. Tutorial slides are relevant, but probably won't be tested. Assignments Assignments must be your own individual work. You can collaborate with up to 2 other students. You should name these people in your submission. Code should be readable . Make sure to put all plots and important results in your PDF submission. Related Courses CSC411: List of methods, (K-NN, Decision trees), more focus on computation STA302: Linear regression and classical stats ECE521: Similar material, more focus on computation STA414: Mostly same material, slightly more emphasis on theory than coding CSC321: Neural networks - about 30% overlap Stats vs Machine Learning Statisticians look at the data, consider the problem, and design a model we can understand. They Analyze methods to give guarantees Want to make few assumptions In machine learning , We only care about making good predictions! The basic idea is to learn a general procedure that works for lots of datasets. Often, there is no way around making assumptions, so we make our model large enough to hopefully learn something close to the truth. We can't use bounds in practice, so we evaluate and empirically choose model details. Sometimes, we end up with interpretable models anyways! In short, statistics starts with a model based on the data , machine learning aims to learn a model from the data. Types of Learning Unsupervised Learning: Given unlabeled data instances x_1 x_1 , x_2 x_2 , x_3 x_3 ... build a statistical model of x x , which can be used for making predictions, decisions. Supervised Learning: Given input-output pairs (x,y) (x,y) the goal is to predict correct output given a new input. Semi-supervised Learning: We are given only a limited amount of (x, y) (x, y) pairs, but lots of unlabeled x x 's Active learning and RL: Also get to choose actions that influence (x, y) (x, y) pairs, but lots of unlabeled x x \u2019s. future information + reward. Can just use basic decision theory. Note that these are all just special cases of estimating distributions from data: p(y | x) p(y | x) , p(x) p(x) , p(x, y) p(x, y) ! Finding Structure in Data With a big enough dataset, we can identify structure in the data. Take a large newswire corpus, for example. A simple model based on the word counts of webpages \\[P(x) = \\frac{1}{Z} \\sum_h \\exp [x^TWh]\\] could learn to discretize data into topics. In this case, our topics are our hidden (or latent ) variables. Note Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are \"really there\", but hidden). Matrix Factorization Lets take a look at a specific example which uses matrix factorization for collaborative filtering . Part of the winning solution in the Netflix contest started with a Netflix dataset of 480,189 users, 17,770 movies and over 100 million ratings. The job was essentially to \"fill-in\" the missing information in a table that looked something like the following (hence the collaborative in collaborative filtering): After the modal was learned, it was clear that the latent representations it learned closley mapped what we might call genre : Multiple Kinds of Data in One Model My modeling the joint distribution of our data p(x, y) p(x, y) , we can incorporate multiple types of data under one model. In this example, our dataset consists of both images and text. Once the joint distribution is learned, we could provide a word and ask the model to sample from the learned distribution and return a picture (or vice versa!): Cite Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems 29 In fact, this is the key idea behind image captioning models: Latent Representations Once learned, latent representations of our data allow us to do some powerful things. For example, neural networks that are able to fill-in occuluded (or missing) portions of digital images: Cite Pixel Recurrent Neural Networks. Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu Because our latent space is really a vector space , we have access to all the mathematical operations that are defined on vectors, such as addition and subtraction . Furthermore, out latent representations (which themselves are just vector learned during model training) can be decoded into images (or words, or molecules, etc!). For example, if we were to learn latent representations of human faces, we could add and subtract these representations to create entirely new representations: Latent Representations of Structured Data Some data is structured in a way that is semantically meaningful. Put another way, there is a \"grammar\" to the data. Take the example of molecules representing pharmaceutical drugs. In this case, it is much more difficult to interpolate between two valid structures than it would be to interpolate between images of human faces, for example, because of the grammar of organic chemistry. Simplifying this point, two molecules that look extremely similar could in fact have wildly different behavior in the human body. The take-home point is that we need different methods for learning latent representations of structured data than unstructured data Course Themes This course will follow the broad theme of starting with a simple model and then adding to it. Example Linear regression and principal component analysis (PCA) are special cases of almost everything. A few \"lego bricks\" are enough to build most models (e.g. gaussians, categorical variables, linear transforms and neural networks). While the exact for of each distribution / function shouldn't mater much, your model should have a million parameters in it somewhere (the real world is messy!) Model checking is hard, but important. Learning algorithms are especially hard to debug. Computation Later assignments will involve a bit of programming. You can use whatever language you want, but python and numpy are recommended. For fitting and inference in high-dimensional models, gradient-based methods are basically the only game in town Lots of methods conflate model and fitting algorithm, we will try to separate these. ML as a Bag of Tricks Fast special cases Extensible family K-means Mixture of Gaussians Kernel Density Estimation Latent variable models SVMs Gaussian processes Boosting Deep neural nets Random Forests Bayesian neural nets K-Nearest Neighbours ?? Regularization as a Bag of Tricks Fast special cases Extensible family Early stopping Ensembling L2 Regularization Stochastic variational inference Gradient noise Dropout Expectation-Maximization A Language of Models Our goal will be to develop a language of models, a toolbox . For example, hidden Markov models, mixture of Gaussians, and logistic regression are all examples from a language of models. We will try to show a larger family, and point out common special cases. Using this language, you will be able to build you own custom models. In fact, we can talk about this family of models using very few ideas. Really, all we need are deep probabilistic latent-variable models and some decision theory . Russel and Norvig\u2019s parts of AI Extensible family Machine learning Natural language processing Knowledge representation Deep probabilistic latent-variable models + decision theory Automated reasoning Computer vision Robotics Advantages of probabilistic latent-variable models Data-efficient Learning Automatic regularization, can take advantage of more information. Model is aware what it doesn\u2019t know. E.g. after many cat images model updates get small. Showing a dog image for the first time would lead to large updates.\u0000 Compos-able models Models are built like lego blocks. E.g. you could incorporate a data corruption model. Handle Missing Corrupted Data Latent-variable models can easily handle missing and corrupted data, without the standard hacks of guessing missing values using averages. Predictive Uncertainty Necessary for decision-making. Bad models can confidently give us bad predictions. So can good modals! The key is to be able to express uncertainty. Conditional Predictions Be able to condition predictions can be powerful. E.g. if brexit happens, the value of the pound will fall. Active Learning What data would be expected to increase our confidence about a prediction. Disadvantages of Probabilistic Latent-variable Models Intractable integral over latent variables. Integrating over many dimensions is difficult and sometimes intractable. Probabilistic Graphical Models Vs. Neural Networks Imagine we had the following data we may try to model this data by fitting a mixture of Gaussians, as so which seems perfectly reasonable in this case. However, imagine instead we had the following data the use of a mixture model may not be appropriate in this case, as it fits the data poorly and reports too many clusters but a neural network who's job is to come up with a convincing distribution over where you can expect to see data does much better this brings us to a comparison of probabilistic graphical models and deep learning Probabilistic graphical models Deep learning \u2795 structured representations \u2796 neural net \"goo\" \u2795 priors and uncertainty \u2796 difficult parameterization \u2795 data and computational efficiency \u2796 can require lots of data \u2796 rigid assumptions may not fit \u2795 flexible \u2796 feature engineering \u2795 feature learning \u2796top-down inference \u2795 recognition networks \u2757 Left off on slide 35 The Unreasonable Easiness of Deep Learning The deep learning recipe involves defining an objective function (i.e. a probability of data given parameters) and optimizing the parameters to maximize the object. Gradients are computed automatically, you just need to define a model by some computation. Differentiable models Differentiable models, in general, follow these general principals 1. Model Distributions implicitly by a variable pushed through a deep net \\[y = f_{\\theta}(x) ; x \\sim \\mathcal N(0, I) \\] 2. Approximate intractable distribution by a tractable distribution parameterized by a deep net \\[p(y | x) = \\mathcal N(y | \\mu = f_{\\theta}(x), \\Sigma = g_{\\theta}(x))] ; x \\sim \\mathcal N(0, I) \\] 3. Optimize all parameters using stochastic gradient descent Modeling Idea Graphical models and neural networks have complimentary strengths, and can be combined. One such way to combine these models is by using structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks . By pushing these structured prior distributions through a neural network we can get a model which takes exploits the best of both worlds This idea can be extended to supervised or unsupervised learning Learning Outcomes Know standard algorithms (bag of tricks), when to use them, and their limitations. For basic applications and baselines. Know main elements of language of deep probabilistic models (bag of bricks: distributions, expectations, latent variables, neural networks) and how to combine them. For custom applications + research. Know standard computational tools (Monte Carlo, Stochastic optimization, regularization, automatic differentiation). For fitting models. Tentative List of Topics Linear methods for regression + classification Bayesian linear regression Probabilistic Generative and Discriminative models - Regularization methods Stochastic Optimization and Neural Networks Graphical model notation and exact inference Mixture Models, Bayesian Networks Model Comparison and marginal likelihood Stochastic Variational Inference Time series and recurrent models Gaussian processes Variational Autoencoders Machine-learning-centric History of Probabilistic Models 1940s - 1960s Motivating probability and Bayesian inference 1980s - 2000s Bayesian machine learning with MCMC 1990s - 2000s Graphical models with exact inference 1990s - present Bayesian Nonparametrics with MCMC (Indian Buffet process, Chinese restaurant process) 1990s - 2000s Bayesian ML with mean-field variational inference 2000s - present Probabilistic Programming 2000s - 2013 Deep undirected graphical models (RBMs, pretraining) 2010s - present Stan - Bayesian Data Analysis with HMC 2000s - 2013 Autoencoders, denoising autoencoders 2000s - present Invertible density estimation 2013 - present Stochastic variational inference, variational autoencoders 2014 - present Generative adversarial nets, Real NVP, Pixelnet 2016 - present Lego-style deep generative models (attend, infer, repeat) Appendix Useful Resources Blog + tutorial on matrix factorization for movie recommendation. Glow an interactive OpenAI blog on Generative Models. It appears that a few of these slides were taken straight from this video. Summary of notation in probability and statistics . Glossary of Terms","title":"Week 1"},{"location":"lectures/week_1/#week-1-introduction","text":"","title":"Week 1: Introduction"},{"location":"lectures/week_1/#assigned-reading","text":"Murphy: Chapters 1 and 2 Chapter 2 of David Mackay's textbook","title":"Assigned Reading"},{"location":"lectures/week_1/#overview","text":"Course information Overview of ML with examples Ungraded, anonymous background quiz","title":"Overview"},{"location":"lectures/week_1/#textbook-and-resources","text":"There is no required textbook, but optional reading will be assigned each week Kevin Murphy (2012), Machine Learning: A Probabilistic Perspective . David MacKay (2003) Information Theory, Inference, and Learning Algorithms . The David MacKay textbook is recommended, although 100% of tested material come from class. In this course, lecture slides are more a supplement then main content. The most important stuff will be done on the blackboard, so it is important to come to class with pen and paper. Tutorial slides are relevant, but probably won't be tested.","title":"Textbook and Resources"},{"location":"lectures/week_1/#assignments","text":"Assignments must be your own individual work. You can collaborate with up to 2 other students. You should name these people in your submission. Code should be readable . Make sure to put all plots and important results in your PDF submission.","title":"Assignments"},{"location":"lectures/week_1/#related-courses","text":"CSC411: List of methods, (K-NN, Decision trees), more focus on computation STA302: Linear regression and classical stats ECE521: Similar material, more focus on computation STA414: Mostly same material, slightly more emphasis on theory than coding CSC321: Neural networks - about 30% overlap","title":"Related Courses"},{"location":"lectures/week_1/#stats-vs-machine-learning","text":"Statisticians look at the data, consider the problem, and design a model we can understand. They Analyze methods to give guarantees Want to make few assumptions In machine learning , We only care about making good predictions! The basic idea is to learn a general procedure that works for lots of datasets. Often, there is no way around making assumptions, so we make our model large enough to hopefully learn something close to the truth. We can't use bounds in practice, so we evaluate and empirically choose model details. Sometimes, we end up with interpretable models anyways! In short, statistics starts with a model based on the data , machine learning aims to learn a model from the data.","title":"Stats vs Machine Learning"},{"location":"lectures/week_1/#types-of-learning","text":"Unsupervised Learning: Given unlabeled data instances x_1 x_1 , x_2 x_2 , x_3 x_3 ... build a statistical model of x x , which can be used for making predictions, decisions. Supervised Learning: Given input-output pairs (x,y) (x,y) the goal is to predict correct output given a new input. Semi-supervised Learning: We are given only a limited amount of (x, y) (x, y) pairs, but lots of unlabeled x x 's Active learning and RL: Also get to choose actions that influence (x, y) (x, y) pairs, but lots of unlabeled x x \u2019s. future information + reward. Can just use basic decision theory. Note that these are all just special cases of estimating distributions from data: p(y | x) p(y | x) , p(x) p(x) , p(x, y) p(x, y) !","title":"Types of Learning"},{"location":"lectures/week_1/#finding-structure-in-data","text":"With a big enough dataset, we can identify structure in the data. Take a large newswire corpus, for example. A simple model based on the word counts of webpages \\[P(x) = \\frac{1}{Z} \\sum_h \\exp [x^TWh]\\] could learn to discretize data into topics. In this case, our topics are our hidden (or latent ) variables. Note Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are \"really there\", but hidden).","title":"Finding Structure in Data"},{"location":"lectures/week_1/#matrix-factorization","text":"Lets take a look at a specific example which uses matrix factorization for collaborative filtering . Part of the winning solution in the Netflix contest started with a Netflix dataset of 480,189 users, 17,770 movies and over 100 million ratings. The job was essentially to \"fill-in\" the missing information in a table that looked something like the following (hence the collaborative in collaborative filtering): After the modal was learned, it was clear that the latent representations it learned closley mapped what we might call genre :","title":"Matrix Factorization"},{"location":"lectures/week_1/#multiple-kinds-of-data-in-one-model","text":"My modeling the joint distribution of our data p(x, y) p(x, y) , we can incorporate multiple types of data under one model. In this example, our dataset consists of both images and text. Once the joint distribution is learned, we could provide a word and ask the model to sample from the learned distribution and return a picture (or vice versa!): Cite Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems 29 In fact, this is the key idea behind image captioning models:","title":"Multiple Kinds of Data in One Model"},{"location":"lectures/week_1/#latent-representations","text":"Once learned, latent representations of our data allow us to do some powerful things. For example, neural networks that are able to fill-in occuluded (or missing) portions of digital images: Cite Pixel Recurrent Neural Networks. Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu Because our latent space is really a vector space , we have access to all the mathematical operations that are defined on vectors, such as addition and subtraction . Furthermore, out latent representations (which themselves are just vector learned during model training) can be decoded into images (or words, or molecules, etc!). For example, if we were to learn latent representations of human faces, we could add and subtract these representations to create entirely new representations:","title":"Latent Representations"},{"location":"lectures/week_1/#latent-representations-of-structured-data","text":"Some data is structured in a way that is semantically meaningful. Put another way, there is a \"grammar\" to the data. Take the example of molecules representing pharmaceutical drugs. In this case, it is much more difficult to interpolate between two valid structures than it would be to interpolate between images of human faces, for example, because of the grammar of organic chemistry. Simplifying this point, two molecules that look extremely similar could in fact have wildly different behavior in the human body. The take-home point is that we need different methods for learning latent representations of structured data than unstructured data","title":"Latent Representations of Structured Data"},{"location":"lectures/week_1/#course-themes","text":"This course will follow the broad theme of starting with a simple model and then adding to it. Example Linear regression and principal component analysis (PCA) are special cases of almost everything. A few \"lego bricks\" are enough to build most models (e.g. gaussians, categorical variables, linear transforms and neural networks). While the exact for of each distribution / function shouldn't mater much, your model should have a million parameters in it somewhere (the real world is messy!) Model checking is hard, but important. Learning algorithms are especially hard to debug.","title":"Course Themes"},{"location":"lectures/week_1/#computation","text":"Later assignments will involve a bit of programming. You can use whatever language you want, but python and numpy are recommended. For fitting and inference in high-dimensional models, gradient-based methods are basically the only game in town Lots of methods conflate model and fitting algorithm, we will try to separate these.","title":"Computation"},{"location":"lectures/week_1/#ml-as-a-bag-of-tricks","text":"Fast special cases Extensible family K-means Mixture of Gaussians Kernel Density Estimation Latent variable models SVMs Gaussian processes Boosting Deep neural nets Random Forests Bayesian neural nets K-Nearest Neighbours ??","title":"ML as a Bag of Tricks"},{"location":"lectures/week_1/#regularization-as-a-bag-of-tricks","text":"Fast special cases Extensible family Early stopping Ensembling L2 Regularization Stochastic variational inference Gradient noise Dropout Expectation-Maximization","title":"Regularization as a Bag of Tricks"},{"location":"lectures/week_1/#a-language-of-models","text":"Our goal will be to develop a language of models, a toolbox . For example, hidden Markov models, mixture of Gaussians, and logistic regression are all examples from a language of models. We will try to show a larger family, and point out common special cases. Using this language, you will be able to build you own custom models. In fact, we can talk about this family of models using very few ideas. Really, all we need are deep probabilistic latent-variable models and some decision theory . Russel and Norvig\u2019s parts of AI Extensible family Machine learning Natural language processing Knowledge representation Deep probabilistic latent-variable models + decision theory Automated reasoning Computer vision Robotics","title":"A Language of Models"},{"location":"lectures/week_1/#advantages-of-probabilistic-latent-variable-models","text":"Data-efficient Learning Automatic regularization, can take advantage of more information. Model is aware what it doesn\u2019t know. E.g. after many cat images model updates get small. Showing a dog image for the first time would lead to large updates.\u0000 Compos-able models Models are built like lego blocks. E.g. you could incorporate a data corruption model. Handle Missing Corrupted Data Latent-variable models can easily handle missing and corrupted data, without the standard hacks of guessing missing values using averages. Predictive Uncertainty Necessary for decision-making. Bad models can confidently give us bad predictions. So can good modals! The key is to be able to express uncertainty. Conditional Predictions Be able to condition predictions can be powerful. E.g. if brexit happens, the value of the pound will fall. Active Learning What data would be expected to increase our confidence about a prediction.","title":"Advantages of probabilistic latent-variable models"},{"location":"lectures/week_1/#disadvantages-of-probabilistic-latent-variable-models","text":"Intractable integral over latent variables. Integrating over many dimensions is difficult and sometimes intractable.","title":"Disadvantages of Probabilistic Latent-variable Models"},{"location":"lectures/week_1/#probabilistic-graphical-models-vs-neural-networks","text":"Imagine we had the following data we may try to model this data by fitting a mixture of Gaussians, as so which seems perfectly reasonable in this case. However, imagine instead we had the following data the use of a mixture model may not be appropriate in this case, as it fits the data poorly and reports too many clusters but a neural network who's job is to come up with a convincing distribution over where you can expect to see data does much better this brings us to a comparison of probabilistic graphical models and deep learning Probabilistic graphical models Deep learning \u2795 structured representations \u2796 neural net \"goo\" \u2795 priors and uncertainty \u2796 difficult parameterization \u2795 data and computational efficiency \u2796 can require lots of data \u2796 rigid assumptions may not fit \u2795 flexible \u2796 feature engineering \u2795 feature learning \u2796top-down inference \u2795 recognition networks \u2757 Left off on slide 35","title":"Probabilistic Graphical Models Vs. Neural Networks"},{"location":"lectures/week_1/#the-unreasonable-easiness-of-deep-learning","text":"The deep learning recipe involves defining an objective function (i.e. a probability of data given parameters) and optimizing the parameters to maximize the object. Gradients are computed automatically, you just need to define a model by some computation.","title":"The Unreasonable Easiness of Deep Learning"},{"location":"lectures/week_1/#differentiable-models","text":"Differentiable models, in general, follow these general principals 1. Model Distributions implicitly by a variable pushed through a deep net \\[y = f_{\\theta}(x) ; x \\sim \\mathcal N(0, I) \\] 2. Approximate intractable distribution by a tractable distribution parameterized by a deep net \\[p(y | x) = \\mathcal N(y | \\mu = f_{\\theta}(x), \\Sigma = g_{\\theta}(x))] ; x \\sim \\mathcal N(0, I) \\] 3. Optimize all parameters using stochastic gradient descent","title":"Differentiable models"},{"location":"lectures/week_1/#modeling-idea","text":"Graphical models and neural networks have complimentary strengths, and can be combined. One such way to combine these models is by using structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks . By pushing these structured prior distributions through a neural network we can get a model which takes exploits the best of both worlds This idea can be extended to supervised or unsupervised learning","title":"Modeling Idea"},{"location":"lectures/week_1/#learning-outcomes","text":"Know standard algorithms (bag of tricks), when to use them, and their limitations. For basic applications and baselines. Know main elements of language of deep probabilistic models (bag of bricks: distributions, expectations, latent variables, neural networks) and how to combine them. For custom applications + research. Know standard computational tools (Monte Carlo, Stochastic optimization, regularization, automatic differentiation). For fitting models.","title":"Learning Outcomes"},{"location":"lectures/week_1/#tentative-list-of-topics","text":"Linear methods for regression + classification Bayesian linear regression Probabilistic Generative and Discriminative models - Regularization methods Stochastic Optimization and Neural Networks Graphical model notation and exact inference Mixture Models, Bayesian Networks Model Comparison and marginal likelihood Stochastic Variational Inference Time series and recurrent models Gaussian processes Variational Autoencoders","title":"Tentative List of Topics"},{"location":"lectures/week_1/#machine-learning-centric-history-of-probabilistic-models","text":"1940s - 1960s Motivating probability and Bayesian inference 1980s - 2000s Bayesian machine learning with MCMC 1990s - 2000s Graphical models with exact inference 1990s - present Bayesian Nonparametrics with MCMC (Indian Buffet process, Chinese restaurant process) 1990s - 2000s Bayesian ML with mean-field variational inference 2000s - present Probabilistic Programming 2000s - 2013 Deep undirected graphical models (RBMs, pretraining) 2010s - present Stan - Bayesian Data Analysis with HMC 2000s - 2013 Autoencoders, denoising autoencoders 2000s - present Invertible density estimation 2013 - present Stochastic variational inference, variational autoencoders 2014 - present Generative adversarial nets, Real NVP, Pixelnet 2016 - present Lego-style deep generative models (attend, infer, repeat)","title":"Machine-learning-centric History of Probabilistic Models"},{"location":"lectures/week_1/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_1/#useful-resources","text":"Blog + tutorial on matrix factorization for movie recommendation. Glow an interactive OpenAI blog on Generative Models. It appears that a few of these slides were taken straight from this video. Summary of notation in probability and statistics .","title":"Useful Resources"},{"location":"lectures/week_1/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_10/","text":"Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI) Assigned Reading Murphy: Chapter 18 Overview Review Variational Inference Derive the variational objective ELBO intuition Stochastic optimization Posterior Inference for Latent Variable Models Imagine we had the following latent variable model which represents the probabilistic model p(x, z ; \\theta) p(x, z ; \\theta) where x_{1:N} x_{1:N} are the observations z_{1:N} z_{1:N} are the unobserved local latent variables \\theta \\theta are the global latent variables (i.e. the parameters) The conditional distribution of the unobserved variables given the observed variables (the posterior inference) is p(z, \\theta | x) = \\frac{p(x | z, \\theta)}{p(x)} = \\frac{p(x | z, \\theta)}{\\int\\int p(x, z, \\theta)d_zd_{\\theta}} p(z, \\theta | x) = \\frac{p(x | z, \\theta)}{p(x)} = \\frac{p(x | z, \\theta)}{\\int\\int p(x, z, \\theta)d_zd_{\\theta}} which we will denote as p_{\\theta}(z | x) p_{\\theta}(z | x) . Because the computation \\int\\int p(x, z, \\theta)d_zd_{\\theta} \\int\\int p(x, z, \\theta)d_zd_{\\theta} is intractable, making the computation of the conditional distribution itself intractable, we must turn to variational methods. Approximating the Posterior Inference with Variational Methods Approximation of the posterior inference with variational methods works as follows: Introduce a variational family, q_\\phi(z | x) q_\\phi(z | x) with parameters \\phi \\phi . Encode some notion of \"distance\" between p_\\theta p_\\theta and q_\\phi q_\\phi . Minimize this distance. This process effectively turns Bayesian Inference into an optimization problem (and we love optimization problems in machine learning). It is important to note that whatever function we choose for q_\\phi q_\\phi , it is unlikely that our variational family will have the true distribution p_\\theta p_\\theta in it. Kullback-Leibler Divergence We will measure the distance between q_\\phi q_\\phi and p_\\theta p_\\theta using the Kullback-Leibler divergence . Note Kullback\u2013Leibler divergence has lots of names, we will stick to \"KL divergence\" . We compute D_{KL} D_{KL} as follows: \\begin{align} D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) = \\int q_\\phi(z | x) \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)}dz \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\end{align} \\begin{align} D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) &= \\int q_\\phi(z | x) \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)}dz \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\end{align} Properties of the KL Divergence D_{KL}(q_\\phi || p_\\theta) \\ge 0 D_{KL}(q_\\phi || p_\\theta) \\ge 0 D_{KL}(q_\\phi || p_\\theta) = 0 \\Leftrightarrow q_\\phi = p_\\theta D_{KL}(q_\\phi || p_\\theta) = 0 \\Leftrightarrow q_\\phi = p_\\theta D_{KL}(q_\\phi || p_\\theta) \\not = D_{KL}(p_\\theta || q_\\phi) D_{KL}(q_\\phi || p_\\theta) \\not = D_{KL}(p_\\theta || q_\\phi) The significance of the last property is that D_{KL} D_{KL} is not a true distance measure. Variational Objective We want to approximate p_\\theta p_\\theta by finding a q_\\phi q_\\phi such that q_\\phi \\approx p_\\theta \\Rightarrow D_{KL}(q_\\phi || p_\\theta) = 0 q_\\phi \\approx p_\\theta \\Rightarrow D_{KL}(q_\\phi || p_\\theta) = 0 but the computation of D_{KL}(q_\\phi || p_\\theta) D_{KL}(q_\\phi || p_\\theta) is intractable (as discussed above). Note D_{KL}(q_\\phi || p_\\theta) D_{KL}(q_\\phi || p_\\theta) is intractable because it contains the term p_\\theta(z | x) p_\\theta(z | x) , which we have already established, is intractable. To circumvent this issue of intractability, we will derive the evidence lower bound (ELBO) , and show that maximizing the ELBO \\Rightarrow \\Rightarrow minimizing D_{KL}(q_\\phi || p_\\theta) D_{KL}(q_\\phi || p_\\theta) . \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log p_\\theta(x) \\\\ = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log p_\\theta(x) \\\\ &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} Where \\mathcal L(\\theta, \\phi ; x) \\mathcal L(\\theta, \\phi ; x) is the ELBO . Note Notice that \\log p_\\theta(x) \\log p_\\theta(x) is not dependent on z z . Rearranging, we get \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= \\log p_\\theta(x) \\\\ \\end{align} Because D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\therefore \\therefore maximizing the ELBO \\Rightarrow \\Rightarrow minimizing D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) . Alternative Derivation Starting with Jenson's inequality , f(E[X]) \\le E[f(x)] f(E[X]) \\le E[f(x)] if X X is a random variable and f f is a convex function. Given that \\log \\log is a concave function, we have \\begin{align} \\log p(x) = \\log \\int p_\\theta(x, z)dz \\\\ = \\log \\int p_\\theta(x, z) \\frac{q_\\phi(z | x)}{q_\\phi(z | x)} dz \\\\ = \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ \\Rightarrow \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\ge \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ = \\mathcal L(\\theta, \\phi ; x) \\end{align} \\begin{align} \\log p(x) &= \\log \\int p_\\theta(x, z)dz \\\\ &= \\log \\int p_\\theta(x, z) \\frac{q_\\phi(z | x)}{q_\\phi(z | x)} dz \\\\ &= \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ \\Rightarrow \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} & \\ge \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ &= - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ &= \\mathcal L(\\theta, \\phi ; x) \\end{align} Alternative Forms of ELBO and Intuitions We have that \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} 1) The most general interpretation of the ELBO is given by \\begin{align} \\mathcal L(\\theta, \\phi ; x) = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(z)p_\\theta(x | z)}{q_\\phi(z | x)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ]\\\\ \\end{align} \\begin{align} \\mathcal L(\\theta, \\phi ; x) &= - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(z)p_\\theta(x | z)}{q_\\phi(z | x)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ]\\\\ \\end{align} 2) We can also re-write 1) using entropy \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) \\Big ] \\mathbb{H} \\Big [ q_\\phi(z | x) \\Big ] \\\\ \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) \\Big ] \\mathbb{H} \\Big [ q_\\phi(z | x) \\Big ] \\\\ 3) Another re-write and we arrive at \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) Tip The instructor suggest that this would be useful for assignment 3. This frames the ELBO as a tradeoff. The first term can be thought of as a \"reconstruction likelihood\", i.e. how probable is x x given z z , which encourages the model to choose the distribution which best reconstructs the data. The second term acts as regularization, by enforcing the idea that our parameterization shouldn't move us too far from the true distribution. Note The instructor recommends we read \"sticking the landing\". Mean Field Variational Inference In mean field variational inference , we restrict ourselves to variational families, q q , that we can compute the gradient of, and assume the approximate distribution q q fully factorizes to q_\\phi(z) q_\\phi(z) (no x x !). I.e., we approximate p_\\theta(z|x) p_\\theta(z|x) with q_\\phi(z) q_\\phi(z) q_\\phi(z, \\theta | \\phi) = q_\\phi(\\theta | \\phi_{\\theta})\\prod_{i=1}^Nq(z_i | \\phi_i) q_\\phi(z, \\theta | \\phi) = q_\\phi(\\theta | \\phi_{\\theta})\\prod_{i=1}^Nq(z_i | \\phi_i) where \\phi = (\\phi_\\theta, \\phi_{1:N}) \\phi = (\\phi_\\theta, \\phi_{1:N}) . If q q s are in the same family as p p s, we can optimize via coordinate ascent . Traditional Variational Inference (ASIDE) Fix all other variables -- optimize local Aggregate local -- optimize global Repeat until KL divergence Warning I think this was meant to be an aside. Optimizing ELBO We have that \\begin{align} \\mathcal L(\\phi ; x) = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) - \\log {q_\\phi(z | x)} \\Big ] \\\\ \\end{align} \\begin{align} \\mathcal L(\\phi ; x) &= - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) - \\log {q_\\phi(z | x)} \\Big ] \\\\ \\end{align} If we want to optimize this with gradient methods, we will need to compute \\nabla_\\phi \\mathcal L(\\phi ; x) \\nabla_\\phi \\mathcal L(\\phi ; x) . Nowadays, we have automatic differentiation (AD) . We can optimize with gradient methods if: z z is continuous dependence on \\phi \\phi is exposed to AD If these are both true, then \\nabla_\\phi \\mathcal L(\\phi ; x) = \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\Big [ \\log p_\\theta(x, z) - \\log q_\\phi(z|x) \\Big ] \\nabla_\\phi \\mathcal L(\\phi ; x) = \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\Big [ \\log p_\\theta(x, z) - \\log q_\\phi(z|x) \\Big ] but, this is difficult because we are taking the gradient of an expectation and we are trying to compute this gradient from samples. This brings us to our big idea: instead of taking the gradient of an expectation , we compute the gradient as an expectation . Score Gradient Also called the likelihood ratio, or REINFORCE, was independently developed in 1990, 1992, 2013, and 2014 (twice). It is given by \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z)} f(z) = \\nabla_\\phi \\int f(z) q_\\phi (z) dz \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z)} f(z) = \\nabla_\\phi \\int f(z) q_\\phi (z) dz if we assume that q_\\phi(z) q_\\phi(z) is a continous function of \\phi \\phi , then \\begin{align} = \\int \\nabla_\\phi f(z) q_\\phi(z) dz \\\\ = \\int f(z) \\nabla_\\phi q_\\phi (z) dz \\end{align} \\begin{align} &= \\int \\nabla_\\phi f(z) q_\\phi(z) dz \\\\ &= \\int f(z) \\nabla_\\phi q_\\phi (z) dz \\end{align} using the log-derivative trick \\big ( \\nabla_\\phi \\log q_\\phi = \\frac{\\nabla_\\phi q_\\phi}{q_\\phi} \\big ) \\big ( \\nabla_\\phi \\log q_\\phi = \\frac{\\nabla_\\phi q_\\phi}{q_\\phi} \\big ) : \\begin{align} = \\int f(z) q_\\phi(z | x ) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] dz \\\\ = \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ f(z) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] \\\\ \\end{align} \\begin{align} &= \\int f(z) q_\\phi(z | x ) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] dz \\\\ &= \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ f(z) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] \\\\ \\end{align} where q_\\phi(z | x ) q_\\phi(z | x ) is the score function. Finally, we have \\nabla_\\phi \\mathcal L(\\phi ; x) = \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ \\big( \\log p_\\theta(x, z) - \\log q_\\phi(z | x) \\big) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] \\nabla_\\phi \\mathcal L(\\phi ; x) = \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ \\big( \\log p_\\theta(x, z) - \\log q_\\phi(z | x) \\big) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] which is unbiased , but high variance . Pathwise Gradient Appendix Useful Resources High level overview on variational inference. Glossary of Terms","title":"Week 10"},{"location":"lectures/week_10/#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi","text":"","title":"Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)"},{"location":"lectures/week_10/#assigned-reading","text":"Murphy: Chapter 18","title":"Assigned Reading"},{"location":"lectures/week_10/#overview","text":"Review Variational Inference Derive the variational objective ELBO intuition Stochastic optimization","title":"Overview"},{"location":"lectures/week_10/#posterior-inference-for-latent-variable-models","text":"Imagine we had the following latent variable model which represents the probabilistic model p(x, z ; \\theta) p(x, z ; \\theta) where x_{1:N} x_{1:N} are the observations z_{1:N} z_{1:N} are the unobserved local latent variables \\theta \\theta are the global latent variables (i.e. the parameters) The conditional distribution of the unobserved variables given the observed variables (the posterior inference) is p(z, \\theta | x) = \\frac{p(x | z, \\theta)}{p(x)} = \\frac{p(x | z, \\theta)}{\\int\\int p(x, z, \\theta)d_zd_{\\theta}} p(z, \\theta | x) = \\frac{p(x | z, \\theta)}{p(x)} = \\frac{p(x | z, \\theta)}{\\int\\int p(x, z, \\theta)d_zd_{\\theta}} which we will denote as p_{\\theta}(z | x) p_{\\theta}(z | x) . Because the computation \\int\\int p(x, z, \\theta)d_zd_{\\theta} \\int\\int p(x, z, \\theta)d_zd_{\\theta} is intractable, making the computation of the conditional distribution itself intractable, we must turn to variational methods.","title":"Posterior Inference for Latent Variable Models"},{"location":"lectures/week_10/#approximating-the-posterior-inference-with-variational-methods","text":"Approximation of the posterior inference with variational methods works as follows: Introduce a variational family, q_\\phi(z | x) q_\\phi(z | x) with parameters \\phi \\phi . Encode some notion of \"distance\" between p_\\theta p_\\theta and q_\\phi q_\\phi . Minimize this distance. This process effectively turns Bayesian Inference into an optimization problem (and we love optimization problems in machine learning). It is important to note that whatever function we choose for q_\\phi q_\\phi , it is unlikely that our variational family will have the true distribution p_\\theta p_\\theta in it.","title":"Approximating the Posterior Inference with Variational Methods"},{"location":"lectures/week_10/#kullback-leibler-divergence","text":"We will measure the distance between q_\\phi q_\\phi and p_\\theta p_\\theta using the Kullback-Leibler divergence . Note Kullback\u2013Leibler divergence has lots of names, we will stick to \"KL divergence\" . We compute D_{KL} D_{KL} as follows: \\begin{align} D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) = \\int q_\\phi(z | x) \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)}dz \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\end{align} \\begin{align} D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) &= \\int q_\\phi(z | x) \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)}dz \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\end{align}","title":"Kullback-Leibler Divergence"},{"location":"lectures/week_10/#properties-of-the-kl-divergence","text":"D_{KL}(q_\\phi || p_\\theta) \\ge 0 D_{KL}(q_\\phi || p_\\theta) \\ge 0 D_{KL}(q_\\phi || p_\\theta) = 0 \\Leftrightarrow q_\\phi = p_\\theta D_{KL}(q_\\phi || p_\\theta) = 0 \\Leftrightarrow q_\\phi = p_\\theta D_{KL}(q_\\phi || p_\\theta) \\not = D_{KL}(p_\\theta || q_\\phi) D_{KL}(q_\\phi || p_\\theta) \\not = D_{KL}(p_\\theta || q_\\phi) The significance of the last property is that D_{KL} D_{KL} is not a true distance measure.","title":"Properties of the KL Divergence"},{"location":"lectures/week_10/#variational-objective","text":"We want to approximate p_\\theta p_\\theta by finding a q_\\phi q_\\phi such that q_\\phi \\approx p_\\theta \\Rightarrow D_{KL}(q_\\phi || p_\\theta) = 0 q_\\phi \\approx p_\\theta \\Rightarrow D_{KL}(q_\\phi || p_\\theta) = 0 but the computation of D_{KL}(q_\\phi || p_\\theta) D_{KL}(q_\\phi || p_\\theta) is intractable (as discussed above). Note D_{KL}(q_\\phi || p_\\theta) D_{KL}(q_\\phi || p_\\theta) is intractable because it contains the term p_\\theta(z | x) p_\\theta(z | x) , which we have already established, is intractable. To circumvent this issue of intractability, we will derive the evidence lower bound (ELBO) , and show that maximizing the ELBO \\Rightarrow \\Rightarrow minimizing D_{KL}(q_\\phi || p_\\theta) D_{KL}(q_\\phi || p_\\theta) . \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log p_\\theta(x) \\\\ = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z | x)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Bigg [ \\log \\Bigg ( q_\\phi(z | x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(z, x)} \\Bigg ) \\Bigg ] \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(z, x)} + \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log p_\\theta(x) \\\\ &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\end{align} Where \\mathcal L(\\theta, \\phi ; x) \\mathcal L(\\theta, \\phi ; x) is the ELBO . Note Notice that \\log p_\\theta(x) \\log p_\\theta(x) is not dependent on z z . Rearranging, we get \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) = \\log p_\\theta(x) \\\\ \\end{align} \\begin{align} D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= -\\mathcal L(\\theta, \\phi ; x) + \\log p_\\theta(x) \\\\ \\Rightarrow \\mathcal L(\\theta, \\phi ; x) + D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) &= \\log p_\\theta(x) \\\\ \\end{align} Because D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) \\ge 0 \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\mathcal L(\\theta, \\phi ; x) \\le \\log p_\\theta(x) \\therefore \\therefore maximizing the ELBO \\Rightarrow \\Rightarrow minimizing D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) D_{KL}(q_\\phi (z | x) || p_\\theta (z | x)) .","title":"Variational Objective"},{"location":"lectures/week_10/#alternative-derivation","text":"Starting with Jenson's inequality , f(E[X]) \\le E[f(x)] f(E[X]) \\le E[f(x)] if X X is a random variable and f f is a convex function. Given that \\log \\log is a concave function, we have \\begin{align} \\log p(x) = \\log \\int p_\\theta(x, z)dz \\\\ = \\log \\int p_\\theta(x, z) \\frac{q_\\phi(z | x)}{q_\\phi(z | x)} dz \\\\ = \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ \\Rightarrow \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\ge \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ = \\mathcal L(\\theta, \\phi ; x) \\end{align} \\begin{align} \\log p(x) &= \\log \\int p_\\theta(x, z)dz \\\\ &= \\log \\int p_\\theta(x, z) \\frac{q_\\phi(z | x)}{q_\\phi(z | x)} dz \\\\ &= \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ \\Rightarrow \\log \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} & \\ge \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ &= - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ &= \\mathcal L(\\theta, \\phi ; x) \\end{align}","title":"Alternative Derivation"},{"location":"lectures/week_10/#alternative-forms-of-elbo-and-intuitions","text":"We have that \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} 1) The most general interpretation of the ELBO is given by \\begin{align} \\mathcal L(\\theta, \\phi ; x) = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(z)p_\\theta(x | z)}{q_\\phi(z | x)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ]\\\\ \\end{align} \\begin{align} \\mathcal L(\\theta, \\phi ; x) &= - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{p_\\theta(z)p_\\theta(x | z)}{q_\\phi(z | x)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ]\\\\ \\end{align} 2) We can also re-write 1) using entropy \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) \\Big ] \\mathbb{H} \\Big [ q_\\phi(z | x) \\Big ] \\\\ \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) \\Big ] \\mathbb{H} \\Big [ q_\\phi(z | x) \\Big ] \\\\ 3) Another re-write and we arrive at \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) Tip The instructor suggest that this would be useful for assignment 3. This frames the ELBO as a tradeoff. The first term can be thought of as a \"reconstruction likelihood\", i.e. how probable is x x given z z , which encourages the model to choose the distribution which best reconstructs the data. The second term acts as regularization, by enforcing the idea that our parameterization shouldn't move us too far from the true distribution. Note The instructor recommends we read \"sticking the landing\".","title":"Alternative Forms of ELBO and Intuitions"},{"location":"lectures/week_10/#mean-field-variational-inference","text":"In mean field variational inference , we restrict ourselves to variational families, q q , that we can compute the gradient of, and assume the approximate distribution q q fully factorizes to q_\\phi(z) q_\\phi(z) (no x x !). I.e., we approximate p_\\theta(z|x) p_\\theta(z|x) with q_\\phi(z) q_\\phi(z) q_\\phi(z, \\theta | \\phi) = q_\\phi(\\theta | \\phi_{\\theta})\\prod_{i=1}^Nq(z_i | \\phi_i) q_\\phi(z, \\theta | \\phi) = q_\\phi(\\theta | \\phi_{\\theta})\\prod_{i=1}^Nq(z_i | \\phi_i) where \\phi = (\\phi_\\theta, \\phi_{1:N}) \\phi = (\\phi_\\theta, \\phi_{1:N}) . If q q s are in the same family as p p s, we can optimize via coordinate ascent .","title":"Mean Field Variational Inference"},{"location":"lectures/week_10/#traditional-variational-inference-aside","text":"Fix all other variables -- optimize local Aggregate local -- optimize global Repeat until KL divergence Warning I think this was meant to be an aside.","title":"Traditional Variational Inference (ASIDE)"},{"location":"lectures/week_10/#optimizing-elbo","text":"We have that \\begin{align} \\mathcal L(\\phi ; x) = - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ = \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) - \\log {q_\\phi(z | x)} \\Big ] \\\\ \\end{align} \\begin{align} \\mathcal L(\\phi ; x) &= - \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\\\ &= \\underset{z \\sim q_\\phi}{\\operatorname{\\mathbb{E}}} \\Big [ \\log p_\\theta({x | z}) - \\log {q_\\phi(z | x)} \\Big ] \\\\ \\end{align} If we want to optimize this with gradient methods, we will need to compute \\nabla_\\phi \\mathcal L(\\phi ; x) \\nabla_\\phi \\mathcal L(\\phi ; x) . Nowadays, we have automatic differentiation (AD) . We can optimize with gradient methods if: z z is continuous dependence on \\phi \\phi is exposed to AD If these are both true, then \\nabla_\\phi \\mathcal L(\\phi ; x) = \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\Big [ \\log p_\\theta(x, z) - \\log q_\\phi(z|x) \\Big ] \\nabla_\\phi \\mathcal L(\\phi ; x) = \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\Big [ \\log p_\\theta(x, z) - \\log q_\\phi(z|x) \\Big ] but, this is difficult because we are taking the gradient of an expectation and we are trying to compute this gradient from samples. This brings us to our big idea: instead of taking the gradient of an expectation , we compute the gradient as an expectation .","title":"Optimizing ELBO"},{"location":"lectures/week_10/#score-gradient","text":"Also called the likelihood ratio, or REINFORCE, was independently developed in 1990, 1992, 2013, and 2014 (twice). It is given by \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z)} f(z) = \\nabla_\\phi \\int f(z) q_\\phi (z) dz \\nabla_\\phi \\mathbb{E}_{z \\sim q_\\phi(z)} f(z) = \\nabla_\\phi \\int f(z) q_\\phi (z) dz if we assume that q_\\phi(z) q_\\phi(z) is a continous function of \\phi \\phi , then \\begin{align} = \\int \\nabla_\\phi f(z) q_\\phi(z) dz \\\\ = \\int f(z) \\nabla_\\phi q_\\phi (z) dz \\end{align} \\begin{align} &= \\int \\nabla_\\phi f(z) q_\\phi(z) dz \\\\ &= \\int f(z) \\nabla_\\phi q_\\phi (z) dz \\end{align} using the log-derivative trick \\big ( \\nabla_\\phi \\log q_\\phi = \\frac{\\nabla_\\phi q_\\phi}{q_\\phi} \\big ) \\big ( \\nabla_\\phi \\log q_\\phi = \\frac{\\nabla_\\phi q_\\phi}{q_\\phi} \\big ) : \\begin{align} = \\int f(z) q_\\phi(z | x ) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] dz \\\\ = \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ f(z) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] \\\\ \\end{align} \\begin{align} &= \\int f(z) q_\\phi(z | x ) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] dz \\\\ &= \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ f(z) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] \\\\ \\end{align} where q_\\phi(z | x ) q_\\phi(z | x ) is the score function. Finally, we have \\nabla_\\phi \\mathcal L(\\phi ; x) = \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ \\big( \\log p_\\theta(x, z) - \\log q_\\phi(z | x) \\big) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] \\nabla_\\phi \\mathcal L(\\phi ; x) = \\mathbb{E}_{z \\sim q_\\phi(z)} \\Big [ \\big( \\log p_\\theta(x, z) - \\log q_\\phi(z | x) \\big) \\nabla_\\phi \\big [ \\log q_\\phi(z | x ) \\big ] \\Big ] which is unbiased , but high variance .","title":"Score Gradient"},{"location":"lectures/week_10/#pathwise-gradient","text":"","title":"Pathwise Gradient"},{"location":"lectures/week_10/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_10/#useful-resources","text":"High level overview on variational inference.","title":"Useful Resources"},{"location":"lectures/week_10/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_11/","text":"Week 11: Variational Auto Encoders Assigned Reading See course website . Motivation: Autoencoders (Deterministic) An autoencoders takes some data as input and learns some latent state representation of the data. More specifically, autoencoders reconstruct their own input using an encoder and a decoder . Encoder : g(x) = Z \\in F \\quad x \\in X g(x) = Z \\in F \\quad x \\in X Decoder : f(z) = \\tilde x f(z) = \\tilde x where X X is the data space, and F F is the feature space. The encoder , g(x) g(x) , takes in the input data (such as an image) and outputs a single value for each encoding dimension while the The decoder , f(z) f(z) takes this encoding and attempts to recreate the original input. Our goal is to learn g, f g, f from labeled data (which is nearly always done with a neural network ). Tip Much of this lecture comes from this paper , and forms the basis of all material on assignment 3. z z is the code the model attempts to compress a representation of the input, x x , into. It is important that this code is a bottleneck, i.e. that \\text{dim} \\ F \\lt \\text{dim} \\ X \\text{dim} \\ F \\lt \\text{dim} \\ X as this forces the autoencoder to engage in dimensionality reduction, for example by learning how to ignore noise (otherwise, we would just learn the identify function). The big idea is that the code contains only the most salient features of the input, such that we can reconstruct the input from the code reliably \\tilde x = f(g(x)) \\approx x \\tilde x = f(g(x)) \\approx x Problems with Deterministic Autoencoders There are two main problems with deterministic autoencoders. Problem 1: Proximity in data space does not mean proximity in feature space The embeddings (or codes) learned by the model are deterministic, i.e. g(x_1) = z_1 \\Rightarrow f(z_1) = \\tilde x_1 \\\\ g(x_2) = z_2 \\Rightarrow f(z_2) = \\tilde x_2 \\\\ g(x_1) = z_1 \\Rightarrow f(z_1) = \\tilde x_1 \\\\ g(x_2) = z_2 \\Rightarrow f(z_2) = \\tilde x_2 \\\\ but proximity in feature space is not enforced for inputs in close proximity in data space, i.e. z_1 \\approx z_2 \\not \\Rightarrow x_1 \\approx x_2 z_1 \\approx z_2 \\not \\Rightarrow x_1 \\approx x_2 The fundamental problem with autoencoders, for generation, is that the latent space they convert their inputs to and where their encoded vectors lie, may not be continuous , or allow easy interpolation. Indeed, given that these models are trained strictly to optimize reconstruction loss, the best strategy is often to encode each input into distinct clusters in the latent space to encode each type of data (class, etc) with discontinuities between clusters (as doing this will allow the decoder to easily reconstruct the input). But when you\u2019re building a generative model, you don\u2019t want to prepare to replicate the same image you put in. You want to randomly sample from the latent space, or generate variations on an input image, from a continuous latent space. If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space. During training, it never saw encoded vectors coming from that region of latent space. Problem 2: How to measure the goodness of a reconstruction? An important question is how to measure how well the model is able to reconstruct its inputs. Take the simple example of reconstructing handwritten digits In this case, the reconstruction looks quite good. However, if we chose a simple distance metric between inputs and reconstructions to measure the performance of our model, we would heavily penalize the left-shift in the reconstruction \\tilde x \\tilde x . The point of this example is that choosing an appropriate metric for evaluating model performance can be difficult, and that a miss-aligned objective can be disastrous. Note The loss function in this setting is referred to as the reconstruction loss . For autoencoders, the loss function is usually either the mean-squared error or cross-entropy between the output and the input. Variational Autoencoders (VAEs) The big idea behind variational autoencoders (VAEs) is to encode inputs with uncertainty. Unlike normal autoencoders, the encoder of a VAE (also called the recognition model ) outputs a probability distribution for each latent attribute, i.e., it encodes inputs x x to a distribution over latent codes, p(z | x) p(z | x) . With this modification, random sampling and interpolation become straightforward. VAEs learn a latent variable model for their input data. Instead of the encoder learning an encoding vector of size n n , it learns two vectors of size n n : vector of means, \\mu \\mu , and another vector of standard deviations, \\sigma \\sigma . Intuitively, the mean will control where the encoding of the input should be centered around while the standard deviation will control how much can the encoding vary from the mean. They form the parameters of a vector of random variables of length n n , with the i^{th} i^{th} element of \\mu \\mu and \\sigma \\sigma being the mean and standard deviation of the i^{th} i^{th} random variable, x_i x_i , from which we sample, to obtain the sampled encoding which we pass onward to the decoder. This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling. As encodings are generated at random from anywhere inside the \u201ccircle\u201d (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well. This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training. Note Much of this content came from this Medium post. Please give it some claps if you found it useful. The specifics Our model is represented by the joint distribution over the latent codes and the input data p_\\theta(x, z) p_\\theta(x, z) . Decomposing this distribution, we get p_\\theta(x, z) = \\text{prior} \\times \\text{likelihood} = p_\\theta(z)p_\\theta(x | z) p_\\theta(x, z) = \\text{prior} \\times \\text{likelihood} = p_\\theta(z)p_\\theta(x | z) assuming our model looks like Cite Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013) . then learning the model is just inference: p_\\theta(z | x) = \\frac{p_\\theta(z)p_\\theta(x | z)}{p_\\theta(x)} p_\\theta(z | x) = \\frac{p_\\theta(z)p_\\theta(x | z)}{p_\\theta(x)} however, as we said last lecture, learning p_\\theta(x) = \\int p_\\theta(x | z) p_\\theta(z) dz p_\\theta(x) = \\int p_\\theta(x | z) p_\\theta(z) dz is intractable. Our solution was to introduce an approximate distribution with its own set of parameters, q_\\phi q_\\phi , and learn these parameters such that q_\\phi (z | x) \\approx p_\\theta(z | x) q_\\phi (z | x) \\approx p_\\theta(z | x) which turned our inference problems into the optimization problem of minimizing the KL divergence between the true and approximate distributions D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) finally, we demonstrated that minimizing D_{KL} D_{KL} was equivalent to maximizing the ELBO, L L \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} We also talked about two other alternative forms or \"intuitions\" of the ELBO : \\begin{align} \\mathcal L(\\theta, \\phi ; x) = E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ] \\tag*{intuition (1)} \\\\ = E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\tag*{intuition (3)}\\\\ \\end{align} \\begin{align} \\mathcal L(\\theta, \\phi ; x) &= E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ] \\tag*{intuition (1)} \\\\ &= E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\tag*{intuition (3)}\\\\ \\end{align} The second of which (intuition 3) is the loss function we use for training VAEs. Notice now that the first term corresponds to the likelihood of our input under the distribution decoded from z z and the second term the divergence of the approximate distribution posterior from the prior of the true distribution . Note We talked last week about how the second terms acts a regularization, by enforcing the idea that our parameterization shouldn't move us too far from the true distribution. Also note that this term as a simple, closed form if the posterior and prior are Gaussians. The encoder and decoder in a VAE become: Encoder : g(x_i) = \\phi_i g(x_i) = \\phi_i Decoder : f(z_i) = \\theta_i f(z_i) = \\theta_i Where \\phi_i \\phi_i are the parameters of q_\\phi(z_i | x_i) q_\\phi(z_i | x_i) (we are encoding a distribution, which is exactly just its parameters) and \\theta_i \\theta_i are the parameters of p_\\theta(\\tilde x_i | z_i) p_\\theta(\\tilde x_i | z_i) , the reconstruction likelihood. Why does a VAE solve the problems of a deterministic autoencoder? Problem 1 Unlike the encoder of a normal autoencoder, which encodes each input as a distinct point and forms distinct clusters in the latent space to encode each type of data (class, etc) with discontinuities between clusters (as doing this will allow the decoder to easily reconstruct the input), the VAE generation model learns to reconstruct its inputs not only from the encoded points but also from the area around them . This allows the generation model to generate new data by sampling from an \u201carea\u201d instead of only being able to generate already seen data corresponding to the particular fixed encoded points. Problem 2 The first term in the ELBO is the reconstruction likelihood , i.e. the likelihood that we would have re-constructed our data under our model. This serves as our measure of \"goodness\" of the reconstructed inputs. VAE Pipeline Run a given input, x_i x_i through the encoder: g(x_i) = \\phi_i g(x_i) = \\phi_i to get the parameters of the approximate distribution q_{\\phi_i}(z | x) q_{\\phi_i}(z | x) . Sample z_i \\sim q_{\\phi_i}(z | x) z_i \\sim q_{\\phi_i}(z | x) . This is the code in our feature space F F . Run the sampled code through the decoder: f(z_i) = \\theta_i f(z_i) = \\theta_i to get the parameters of the true distribution p_{\\theta_i}(x | z) p_{\\theta_i}(x | z) . Compute the loss function: \\mathcal L(x ; \\theta, \\phi) = - E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] + D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\mathcal L(x ; \\theta, \\phi) = - E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] + D_{KL}(q_\\phi(z | x) || p_\\theta(z)) Use gradient-based optimization to backpropogate the loses \\nabla_\\theta L \\nabla_\\theta L , \\nabla_\\phi L \\nabla_\\phi L Once a VAE is trained, we can sample new inputs \\tilde x_i \\sim p_\\theta(x | z_i) \\tilde x_i \\sim p_\\theta(x | z_i) We can also interpolate between inputs, using simple vector arithmetic. Note This sampling is never performed during training. Gradient Example: MNIST Lets walk through an example of computing the gradients for a VAE on MNIST. We will choose our prior on z z to be the standard Gaussian with zero mean and unit variance \\mathcal{N}(0, I) \\mathcal{N}(0, I) our likelihood function to be p_\\theta(x | z) = \\Big \\{^{\\text{Bernoulli if binarized}}_{\\sigma (\\text{Gaussian}) \\text{ else}} p_\\theta(x | z) = \\Big \\{^{\\text{Bernoulli if binarized}}_{\\sigma (\\text{Gaussian}) \\text{ else}} and our approximate distribution to be q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) Note Notice that our mean and variance are functions of the input. Finally, we use neural networks as our encoder and decoder Encoder : g_\\phi(x_i) = \\phi_i = [u_i, \\log \\sigma_i] g_\\phi(x_i) = \\phi_i = [u_i, \\log \\sigma_i] Decoder : f_\\theta(z_i) = \\theta_i f_\\theta(z_i) = \\theta_i Where \\theta_i \\theta_i are the Bernoulli variables for each pixel in the input. To get our reconstructed input, we simply evaluate \\tilde x_i \\sim p_{\\theta_i}(x | z) \\tilde x_i \\sim p_{\\theta_i}(x | z) Note We log the variance in the encoder ( \\log \\sigma_i \\log \\sigma_i ) in order to force outputs to be positive. This allows the neural network to learn parameters in an unconstrained space. The entire model looks like: Cite Variational Autoencoder Explained . Where inputs x_i x_i are encoded to vectors \\mu \\mu and \\log \\sigma_i \\log \\sigma_i , which parameterize q_\\phi(z | x) q_\\phi(z | x) . Before decoding, we draw a sample z \\sim q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) z \\sim q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) and evaluate its likelihood under the model with p_\\theta(x | z) p_\\theta(x | z) . We compute the loss function \\mathcal L(\\theta, \\phi ; x) \\mathcal L(\\theta, \\phi ; x) and propagate its derivative with respect to \\theta \\theta and \\phi \\phi , \\nabla_\\theta L \\nabla_\\theta L , \\nabla_\\phi L \\nabla_\\phi L , through the network during training. The Reparameterization Trick The decoder generates a reconstruction by first sampling from the distribution q_\\phi(z | x) q_\\phi(z | x) . This sampling process introduces a major problem: gradients are blocked from flowing into the encoder, and hence it will not train. To solve this problem, the reparameterization trick is used. The trick goes as follows: Instead of sampling z z directly from its distribution (e.g. z \\sim \\mathcal{N}(\\mu(x), \\sigma(x)I) z \\sim \\mathcal{N}(\\mu(x), \\sigma(x)I) ) we express z z as a deterministic variable : z = g_\\phi(\\varepsilon, x) z = g_\\phi(\\varepsilon, x) where \\varepsilon \\varepsilon is an auxiliary independent random variable and g_\\phi g_\\phi converts \\varepsilon \\varepsilon to z z . In the case of where our approximate distribution is chosen to be the multivariate normal with diagonal covariance, the reparameterization trick gives: z = \\mu + \\sigma * \\varepsilon \\quad \\text{where } \\varepsilon \\sim \\mathcal N(0, I) z = \\mu + \\sigma * \\varepsilon \\quad \\text{where } \\varepsilon \\sim \\mathcal N(0, I) with this reparameterization, gradients can now flow through the entire network Appendix Useful Resources Keras Blog on autoencoders. Blog on VAEs. Blog on the intuitive understanding of VAEs. The original VAE paper (which assignment 3 is based on) and a video explanation. Blog on the reparameterization trick. Glossary of Terms","title":"Week 11"},{"location":"lectures/week_11/#week-11-variational-auto-encoders","text":"","title":"Week 11: Variational Auto Encoders"},{"location":"lectures/week_11/#assigned-reading","text":"See course website .","title":"Assigned Reading"},{"location":"lectures/week_11/#motivation-autoencoders-deterministic","text":"An autoencoders takes some data as input and learns some latent state representation of the data. More specifically, autoencoders reconstruct their own input using an encoder and a decoder . Encoder : g(x) = Z \\in F \\quad x \\in X g(x) = Z \\in F \\quad x \\in X Decoder : f(z) = \\tilde x f(z) = \\tilde x where X X is the data space, and F F is the feature space. The encoder , g(x) g(x) , takes in the input data (such as an image) and outputs a single value for each encoding dimension while the The decoder , f(z) f(z) takes this encoding and attempts to recreate the original input. Our goal is to learn g, f g, f from labeled data (which is nearly always done with a neural network ). Tip Much of this lecture comes from this paper , and forms the basis of all material on assignment 3. z z is the code the model attempts to compress a representation of the input, x x , into. It is important that this code is a bottleneck, i.e. that \\text{dim} \\ F \\lt \\text{dim} \\ X \\text{dim} \\ F \\lt \\text{dim} \\ X as this forces the autoencoder to engage in dimensionality reduction, for example by learning how to ignore noise (otherwise, we would just learn the identify function). The big idea is that the code contains only the most salient features of the input, such that we can reconstruct the input from the code reliably \\tilde x = f(g(x)) \\approx x \\tilde x = f(g(x)) \\approx x","title":"Motivation: Autoencoders (Deterministic)"},{"location":"lectures/week_11/#problems-with-deterministic-autoencoders","text":"There are two main problems with deterministic autoencoders.","title":"Problems with Deterministic Autoencoders"},{"location":"lectures/week_11/#problem-1-proximity-in-data-space-does-not-mean-proximity-in-feature-space","text":"The embeddings (or codes) learned by the model are deterministic, i.e. g(x_1) = z_1 \\Rightarrow f(z_1) = \\tilde x_1 \\\\ g(x_2) = z_2 \\Rightarrow f(z_2) = \\tilde x_2 \\\\ g(x_1) = z_1 \\Rightarrow f(z_1) = \\tilde x_1 \\\\ g(x_2) = z_2 \\Rightarrow f(z_2) = \\tilde x_2 \\\\ but proximity in feature space is not enforced for inputs in close proximity in data space, i.e. z_1 \\approx z_2 \\not \\Rightarrow x_1 \\approx x_2 z_1 \\approx z_2 \\not \\Rightarrow x_1 \\approx x_2 The fundamental problem with autoencoders, for generation, is that the latent space they convert their inputs to and where their encoded vectors lie, may not be continuous , or allow easy interpolation. Indeed, given that these models are trained strictly to optimize reconstruction loss, the best strategy is often to encode each input into distinct clusters in the latent space to encode each type of data (class, etc) with discontinuities between clusters (as doing this will allow the decoder to easily reconstruct the input). But when you\u2019re building a generative model, you don\u2019t want to prepare to replicate the same image you put in. You want to randomly sample from the latent space, or generate variations on an input image, from a continuous latent space. If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space. During training, it never saw encoded vectors coming from that region of latent space.","title":"Problem 1: Proximity in data space does not mean proximity in feature space"},{"location":"lectures/week_11/#problem-2-how-to-measure-the-goodness-of-a-reconstruction","text":"An important question is how to measure how well the model is able to reconstruct its inputs. Take the simple example of reconstructing handwritten digits In this case, the reconstruction looks quite good. However, if we chose a simple distance metric between inputs and reconstructions to measure the performance of our model, we would heavily penalize the left-shift in the reconstruction \\tilde x \\tilde x . The point of this example is that choosing an appropriate metric for evaluating model performance can be difficult, and that a miss-aligned objective can be disastrous. Note The loss function in this setting is referred to as the reconstruction loss . For autoencoders, the loss function is usually either the mean-squared error or cross-entropy between the output and the input.","title":"Problem 2: How to measure the goodness of a reconstruction?"},{"location":"lectures/week_11/#variational-autoencoders-vaes","text":"The big idea behind variational autoencoders (VAEs) is to encode inputs with uncertainty. Unlike normal autoencoders, the encoder of a VAE (also called the recognition model ) outputs a probability distribution for each latent attribute, i.e., it encodes inputs x x to a distribution over latent codes, p(z | x) p(z | x) . With this modification, random sampling and interpolation become straightforward. VAEs learn a latent variable model for their input data. Instead of the encoder learning an encoding vector of size n n , it learns two vectors of size n n : vector of means, \\mu \\mu , and another vector of standard deviations, \\sigma \\sigma . Intuitively, the mean will control where the encoding of the input should be centered around while the standard deviation will control how much can the encoding vary from the mean. They form the parameters of a vector of random variables of length n n , with the i^{th} i^{th} element of \\mu \\mu and \\sigma \\sigma being the mean and standard deviation of the i^{th} i^{th} random variable, x_i x_i , from which we sample, to obtain the sampled encoding which we pass onward to the decoder. This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling. As encodings are generated at random from anywhere inside the \u201ccircle\u201d (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well. This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training. Note Much of this content came from this Medium post. Please give it some claps if you found it useful.","title":"Variational Autoencoders (VAEs)"},{"location":"lectures/week_11/#the-specifics","text":"Our model is represented by the joint distribution over the latent codes and the input data p_\\theta(x, z) p_\\theta(x, z) . Decomposing this distribution, we get p_\\theta(x, z) = \\text{prior} \\times \\text{likelihood} = p_\\theta(z)p_\\theta(x | z) p_\\theta(x, z) = \\text{prior} \\times \\text{likelihood} = p_\\theta(z)p_\\theta(x | z) assuming our model looks like Cite Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013) . then learning the model is just inference: p_\\theta(z | x) = \\frac{p_\\theta(z)p_\\theta(x | z)}{p_\\theta(x)} p_\\theta(z | x) = \\frac{p_\\theta(z)p_\\theta(x | z)}{p_\\theta(x)} however, as we said last lecture, learning p_\\theta(x) = \\int p_\\theta(x | z) p_\\theta(z) dz p_\\theta(x) = \\int p_\\theta(x | z) p_\\theta(z) dz is intractable. Our solution was to introduce an approximate distribution with its own set of parameters, q_\\phi q_\\phi , and learn these parameters such that q_\\phi (z | x) \\approx p_\\theta(z | x) q_\\phi (z | x) \\approx p_\\theta(z | x) which turned our inference problems into the optimization problem of minimizing the KL divergence between the true and approximate distributions D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) D_{KL}(q_\\phi(z | x) || p_\\theta(z | x)) finally, we demonstrated that minimizing D_{KL} D_{KL} was equivalent to maximizing the ELBO, L L \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} \\mathcal L(\\theta, \\phi ; x) = \\text{ELBO} = - E_{z_\\phi \\sim q_\\phi} \\log \\frac{q_\\phi(z | x)}{p_\\theta(x, z)} We also talked about two other alternative forms or \"intuitions\" of the ELBO : \\begin{align} \\mathcal L(\\theta, \\phi ; x) = E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ] \\tag*{intuition (1)} \\\\ = E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\tag*{intuition (3)}\\\\ \\end{align} \\begin{align} \\mathcal L(\\theta, \\phi ; x) &= E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) + \\log p_\\theta({z}) - \\log {q_\\phi(z | x)} \\Big ] \\tag*{intuition (1)} \\\\ &= E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] - D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\tag*{intuition (3)}\\\\ \\end{align} The second of which (intuition 3) is the loss function we use for training VAEs. Notice now that the first term corresponds to the likelihood of our input under the distribution decoded from z z and the second term the divergence of the approximate distribution posterior from the prior of the true distribution . Note We talked last week about how the second terms acts a regularization, by enforcing the idea that our parameterization shouldn't move us too far from the true distribution. Also note that this term as a simple, closed form if the posterior and prior are Gaussians. The encoder and decoder in a VAE become: Encoder : g(x_i) = \\phi_i g(x_i) = \\phi_i Decoder : f(z_i) = \\theta_i f(z_i) = \\theta_i Where \\phi_i \\phi_i are the parameters of q_\\phi(z_i | x_i) q_\\phi(z_i | x_i) (we are encoding a distribution, which is exactly just its parameters) and \\theta_i \\theta_i are the parameters of p_\\theta(\\tilde x_i | z_i) p_\\theta(\\tilde x_i | z_i) , the reconstruction likelihood.","title":"The specifics"},{"location":"lectures/week_11/#why-does-a-vae-solve-the-problems-of-a-deterministic-autoencoder","text":"Problem 1 Unlike the encoder of a normal autoencoder, which encodes each input as a distinct point and forms distinct clusters in the latent space to encode each type of data (class, etc) with discontinuities between clusters (as doing this will allow the decoder to easily reconstruct the input), the VAE generation model learns to reconstruct its inputs not only from the encoded points but also from the area around them . This allows the generation model to generate new data by sampling from an \u201carea\u201d instead of only being able to generate already seen data corresponding to the particular fixed encoded points. Problem 2 The first term in the ELBO is the reconstruction likelihood , i.e. the likelihood that we would have re-constructed our data under our model. This serves as our measure of \"goodness\" of the reconstructed inputs.","title":"Why does a VAE solve the problems of a deterministic autoencoder?"},{"location":"lectures/week_11/#vae-pipeline","text":"Run a given input, x_i x_i through the encoder: g(x_i) = \\phi_i g(x_i) = \\phi_i to get the parameters of the approximate distribution q_{\\phi_i}(z | x) q_{\\phi_i}(z | x) . Sample z_i \\sim q_{\\phi_i}(z | x) z_i \\sim q_{\\phi_i}(z | x) . This is the code in our feature space F F . Run the sampled code through the decoder: f(z_i) = \\theta_i f(z_i) = \\theta_i to get the parameters of the true distribution p_{\\theta_i}(x | z) p_{\\theta_i}(x | z) . Compute the loss function: \\mathcal L(x ; \\theta, \\phi) = - E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] + D_{KL}(q_\\phi(z | x) || p_\\theta(z)) \\mathcal L(x ; \\theta, \\phi) = - E_{z_\\phi \\sim q_\\phi} \\Big [ \\log p_\\theta({x | z}) \\Big ] + D_{KL}(q_\\phi(z | x) || p_\\theta(z)) Use gradient-based optimization to backpropogate the loses \\nabla_\\theta L \\nabla_\\theta L , \\nabla_\\phi L \\nabla_\\phi L Once a VAE is trained, we can sample new inputs \\tilde x_i \\sim p_\\theta(x | z_i) \\tilde x_i \\sim p_\\theta(x | z_i) We can also interpolate between inputs, using simple vector arithmetic. Note This sampling is never performed during training.","title":"VAE Pipeline"},{"location":"lectures/week_11/#gradient-example-mnist","text":"Lets walk through an example of computing the gradients for a VAE on MNIST. We will choose our prior on z z to be the standard Gaussian with zero mean and unit variance \\mathcal{N}(0, I) \\mathcal{N}(0, I) our likelihood function to be p_\\theta(x | z) = \\Big \\{^{\\text{Bernoulli if binarized}}_{\\sigma (\\text{Gaussian}) \\text{ else}} p_\\theta(x | z) = \\Big \\{^{\\text{Bernoulli if binarized}}_{\\sigma (\\text{Gaussian}) \\text{ else}} and our approximate distribution to be q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) Note Notice that our mean and variance are functions of the input. Finally, we use neural networks as our encoder and decoder Encoder : g_\\phi(x_i) = \\phi_i = [u_i, \\log \\sigma_i] g_\\phi(x_i) = \\phi_i = [u_i, \\log \\sigma_i] Decoder : f_\\theta(z_i) = \\theta_i f_\\theta(z_i) = \\theta_i Where \\theta_i \\theta_i are the Bernoulli variables for each pixel in the input. To get our reconstructed input, we simply evaluate \\tilde x_i \\sim p_{\\theta_i}(x | z) \\tilde x_i \\sim p_{\\theta_i}(x | z) Note We log the variance in the encoder ( \\log \\sigma_i \\log \\sigma_i ) in order to force outputs to be positive. This allows the neural network to learn parameters in an unconstrained space. The entire model looks like: Cite Variational Autoencoder Explained . Where inputs x_i x_i are encoded to vectors \\mu \\mu and \\log \\sigma_i \\log \\sigma_i , which parameterize q_\\phi(z | x) q_\\phi(z | x) . Before decoding, we draw a sample z \\sim q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) z \\sim q_\\phi(z | x) = \\mathcal{N}(\\mu(x), \\sigma(x)I) and evaluate its likelihood under the model with p_\\theta(x | z) p_\\theta(x | z) . We compute the loss function \\mathcal L(\\theta, \\phi ; x) \\mathcal L(\\theta, \\phi ; x) and propagate its derivative with respect to \\theta \\theta and \\phi \\phi , \\nabla_\\theta L \\nabla_\\theta L , \\nabla_\\phi L \\nabla_\\phi L , through the network during training.","title":"Gradient Example: MNIST"},{"location":"lectures/week_11/#the-reparameterization-trick","text":"The decoder generates a reconstruction by first sampling from the distribution q_\\phi(z | x) q_\\phi(z | x) . This sampling process introduces a major problem: gradients are blocked from flowing into the encoder, and hence it will not train. To solve this problem, the reparameterization trick is used. The trick goes as follows: Instead of sampling z z directly from its distribution (e.g. z \\sim \\mathcal{N}(\\mu(x), \\sigma(x)I) z \\sim \\mathcal{N}(\\mu(x), \\sigma(x)I) ) we express z z as a deterministic variable : z = g_\\phi(\\varepsilon, x) z = g_\\phi(\\varepsilon, x) where \\varepsilon \\varepsilon is an auxiliary independent random variable and g_\\phi g_\\phi converts \\varepsilon \\varepsilon to z z . In the case of where our approximate distribution is chosen to be the multivariate normal with diagonal covariance, the reparameterization trick gives: z = \\mu + \\sigma * \\varepsilon \\quad \\text{where } \\varepsilon \\sim \\mathcal N(0, I) z = \\mu + \\sigma * \\varepsilon \\quad \\text{where } \\varepsilon \\sim \\mathcal N(0, I) with this reparameterization, gradients can now flow through the entire network","title":"The Reparameterization Trick"},{"location":"lectures/week_11/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_11/#useful-resources","text":"Keras Blog on autoencoders. Blog on VAEs. Blog on the intuitive understanding of VAEs. The original VAE paper (which assignment 3 is based on) and a video explanation. Blog on the reparameterization trick.","title":"Useful Resources"},{"location":"lectures/week_11/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_12/","text":"Week 12: Generative Adversarial Networks (GANs) Assigned Reading NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016) Generative adversarial networks; Goodfellow et al. (2014) In the first half of this lecture, we will review the generative models that we have discussed so far. Generative Models, a Review Generative models make the assumption that your data was generated from some distribution \\{x_i\\}_{i=1}^N \\sim p_{data} \\{x_i\\}_{i=1}^N \\sim p_{data} This is the distribution we will attempt to learn. More specifically, we want to learn a model, p_{model} p_{model} , that represents an estimate of p_{data} p_{data} . If we have done our job correctly, then samples from our model \\tilde x_i \\sim p_{model} \\tilde x_i \\sim p_{model} should look like samples from our data. The density p_{model} p_{model} can be explicitly defined approximated by random sampling from our model So far, we have only looked at models of the first type, sometimes referred to as explicit density models . As an example of the simplest possible model of the second type, sometimes referred to as implicit density models , imagine we had a sampler: j = \\text{randint}(1:N) \\\\ \\text{return} \\ x_j j = \\text{randint}(1:N) \\\\ \\text{return} \\ x_j This perhaps is the most trivial generative model we could define. Clearly, this model has no \"understanding\" of the distribution that generated our data. But what does it mean to \"understand\" a data distribution? the ability to generate new data the ability to smoothly interpolate between data points x_i \\rightarrow x_j x_i \\rightarrow x_j the ability to generate samples representing \"underlying features of variation\" Tip For the last point, think of the example of generating images. We might expect a sampled images from a good model to contain variation in the lighting, or camera angle. We have previously discussed auto-encoders , which achieve this \"understanding\" of our data via compression. Of course, this begs the question: Why do we want our models to be able to \"understand\" a data distribution in the first place? Samples from high dimensional distributions are useful and serve as an excellent test of our ability to represent and manipulate high-dimensional probability distributions (import objects in a wide variety of applied math and engineering domains). Model-based RL makes use of generative models. Generative models can be trained with missing data and can provide predictions on inputs that are missing data (e.g. semi-supervised learning) Generative modals enable machine learning to work with multi-modal outputs (for many tasks, a single input may correspond to many different correct answers, each of which is acceptable). Finally, many tasks intrinsically require realistic generation of samples from some distribution, e.g. single image super-resolution art creation image-to-image translation Prototypical Generative Model The prototypical generative model follows the following procedure: Sample some noise, z \\sim p(z) z \\sim p(z) Pass this noise through a model, G G Get a sample, x x from G(z) G(z) In VAEs, we saw that x \\sim p_\\theta(x | z) x \\sim p_\\theta(x | z) Maximum Likelihood Estimate Maximum likelihood estimate is how we have trained all the generative models we have seen thus far . The basic idea is to: Define a model which provides an estimate of a probability distribution parameterized by parameters \\theta \\theta Define a likelihood function that represents the probability of our data and train our parameters to maximize this likelihood e.g. for the i.i.d case: \\prod_{i=1}^N p_{model} (x^{(i)} ; \\theta) \\\\ \\Rightarrow \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) \\prod_{i=1}^N p_{model} (x^{(i)} ; \\theta) \\\\ \\Rightarrow \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) which is equivalent to the minimizing the D_{KL} D_{KL} between p_{data}(x) p_{data}(x) and p_{model}(x ; \\theta) p_{model}(x ; \\theta) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) If we were able to do this precisely, and if p_{data} p_{data} lies within the family of distributions p_{model}(x ; \\theta) p_{model}(x ; \\theta) , then the model would recover p_{data} p_{data} exactly. In practice, we do not have access to p_{data} p_{data} itself, but only to a training set consisting of m m samples from p_{data} p_{data} . We uses these to define \\hat p_{data} \\hat p_{data} , an empirical distribution that places mass only on exactly those m m points, approximating p_{data} p_{data} . Minimizing the D_{KL} D_{KL} between \\hat p_{data} \\hat p_{data} and p_{model} p_{model} is exactly equivalent to maximizing the log-likelihood of the training set. Explicit Density Models Explicit density models (i.e., the models we are discussing and have been discussing the entire course) define an explicit density function p_{model}(x ; \\theta) p_{model}(x ; \\theta) which is used to train the model, typically via maximum likelihood estimation. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill. The main difficulty present in explicit density models is designing a model that can capture all of the complexity of the data to be generated while still maintaining computational tractability. careful construction of models whose structure guarantees their tractability models that admit tractable approximations to the likelihood and its gradients Tractable examples include: Fully Visible Belief Nets (96, 98) WaveNet (2016) In the intractable case, we use variational (e.g. VAEs ) or MCMC approximations to get at p_{model}(x ; \\theta) p_{model}(x ; \\theta) . Generative Model Goal The goal of generative models, in short, is to produce samples similar to p_{data} p_{data} . But do we really need maximum likelihood estimations to achieve our goals? Tip Everything up until this point was considered review. Everything that follows is new content. Implicit Density Models In contrast, implicit density models are trained without explicitly defining a density function. The way we interact with p_{model} p_{model} is through samples. A taxonomic tree of generative models is shown below: Cite NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016) . But how do we train it? No density means no likelihood evaluation . We will look at a specific instance of implicit density models known as Generative Adversarial Networks. Generative Adversarial Network (GAN) Approach In the generative adversarial approach, we do not have likelihoods, only samples. The idea is based on an adversarial game and pulls heavily from game theory . The basic idea of GANs is to set up a game between two players: One of them is called the generator ( G_{\\theta_G} G_{\\theta_G} ). The generator creates samples that are intended to come from the same distribution as the training data. The other player is the discriminator ( D_{\\theta_D} D_{\\theta_D} ). The discriminator examines samples to determine whether they are real or fake. The discriminator learns using traditional supervised learning techniques, dividing inputs into two classes (real or fake). The generator is trained to fool the discriminator. Both are almost always modeled as neural networks (and are therefore differentiable w.r.t their outputs). Once training is complete, we throw away the discriminator . Tip We can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data. Formally, GANs are structured as a probabilistic model containing latent variables z z and observed variable x x Cite NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016) . to sample from the model: first sample z \\sim p(z) z \\sim p(z) ; where p(z) p(z) is some prior distribution then x = G(z) \\sim p_{model} x = G(z) \\sim p_{model} notice that we never explicitly define a distribution p_{model}(x; \\theta) p_{model}(x; \\theta) . Contrast this with VAEs, where to sample from the model: first sample z \\sim q_\\phi(z | x) z \\sim q_\\phi(z | x) for some input x x then using the encoder, compute \\theta = f(z) \\theta = f(z) finally, using the decoder, sample x \\sim p_\\theta(x | z) x \\sim p_\\theta(x | z) notice that we explicitly define a distribution ( p_\\theta(x | z) p_\\theta(x | z) ) and sample from it. Adversarial Game, the Specifics In the adversarial game, both players have cost functions that are defined in terms of both players\u2019 parameters The discriminator wishes to minimize J^{(D)}(\\theta_D, \\theta_G) J^{(D)}(\\theta_D, \\theta_G) and must do so while controlling only \\theta_D \\theta_D . The generator wishes to minimize J^{(G)}(\\theta_D, \\theta_G) J^{(G)}(\\theta_D, \\theta_G) and must do so while controlling only \\theta_G \\theta_G . Because each player\u2019s cost depends on the other player\u2019s parameters, but each player cannot control the other player\u2019s parameters, this scenario is most straightforward to describe as a game rather than as an optimization problem Training Procedure The training process consists of simultaneous SGD . On each step, two minibatches are sampled: a minibatch of x x values from the dataset and a minibatch of z z values drawn from the model\u2019s prior over latent variables, e.g., x \\sim p_{data} x \\sim p_{data} z \\sim p(z) \\Rightarrow x \\sim P_{G(z)} z \\sim p(z) \\Rightarrow x \\sim P_{G(z)} Then two gradient steps are made simultaneously: one updating \\theta_D \\theta_D to reduce J^{(D)} J^{(D)} and one updating \\theta_G \\theta_G to reduce J^{(G)} J^{(G)} . Cost Functions Several different cost functions may be used within the GANs framework. All of the different games designed for GANs so far use the same cost for the discriminator, J^{(D)} J^{(D)} . They differ only in terms of the cost used for the generator, J^{(G)} J^{(G)} . The Discriminator\u2019s Cost The cost used for the discriminator is (almost) always: J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] This is just the standard cross-entropy cost that is minimized when training a standard binary classifier with a sigmoid output. The only difference is that the classifier is trained on two mini-batches of data; one coming from the dataset, where the label is 1 for all examples, and one coming from the generator, where the label is 0 for all examples. Optimal Discriminator Strategy Our goal is to minimize J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] in function space, specifying D(x) D(x) directly. We begin by assuming that both p_{data} p_{data} and p_{model} p_{model} are nonzero everywhere. Note If we do not make this assumption, then some points are never visited during training, and have undefined behavior. To minimize J^{(D)} J^{(D)} with respect to D D , we can write down the functional derivatives with respect to a single entry D^{(x)} D^{(x)} , and set them equal to zero: \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 By solving this equation, we obtain D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} The discriminator (dashed blue line) estimates the ratio between the data density (black dots) and the sum of the data and model densities. Wherever the output of the discriminator is large, the model density is too low, and wherever the output of the discriminator is small, the model density is too high. Estimating this ratio is the key approximation mechanism used by GANs Note We can't actually compute D^\\star(x) D^\\star(x) directly but it illustrates how the discriminator achieves optimal behavior. Optimal Generator Strategy So far we have specified the cost function for only the discriminator. A complete specification of the game requires that we specify a cost function also for the generator. The simplest version of the game is a zero-sum game , in which the sum of all player\u2019s costs is always zero. In this version of the game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} Because J^{(G)} J^{(G)} is tied directly to J^{(D)} J^{(D)} , we can summarize the entire game with a value function specifying the discriminator\u2019s payoff: V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) Zero-sum games are also called minimax games because their solution involves minimization in an outer loop and maximization in an inner loop: \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) Where we want to maximize \\theta_D \\theta_D such that D_{\\theta_D}(x) = 1 D_{\\theta_D}(x) = 1 D_{\\theta_D}(G(z)) = 0 D_{\\theta_D}(G(z)) = 0 and minimize \\theta_G \\theta_G such that D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1 D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1 Heuristic, Non-saturating Game The cost used for the generator in the minimax game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} is useful for theoretical analysis, but does not perform especially well in practice. In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, e.g. D(G(z)) = 0 D(G(z)) = 0 , the generator\u2019s gradient vanishes (the cost function becomes flat!). This is known as the saturation problem . To solve this problem, we introduce the non-saturating game, in which we continue to use cross-entropy minimization for the generator, but instead of flipping the sign on the discriminator\u2019s cost to obtain a cost for the generator, we flip the target used to construct the cross-entropy cost J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) This leads to a strong gradient signal at D(G(z)) = 0 D(G(z)) = 0 . In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the log- probability of the discriminator being mistaken. Maximum Likelihood Game Warning Skipped for now to save time, not mentioned on the Study Guide . Summary In summary, Pros Cons Generate samples in parallel No good method of evaluating No Markov Chains needed for sampling Failings of supervised learning (underfitting /overfitting) No variational bound Hard to train Subjectively better samples Mode collapse (samples are very similar)","title":"Week 12"},{"location":"lectures/week_12/#week-12-generative-adversarial-networks-gans","text":"","title":"Week 12: Generative Adversarial Networks (GANs)"},{"location":"lectures/week_12/#assigned-reading","text":"NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016) Generative adversarial networks; Goodfellow et al. (2014) In the first half of this lecture, we will review the generative models that we have discussed so far.","title":"Assigned Reading"},{"location":"lectures/week_12/#generative-models-a-review","text":"Generative models make the assumption that your data was generated from some distribution \\{x_i\\}_{i=1}^N \\sim p_{data} \\{x_i\\}_{i=1}^N \\sim p_{data} This is the distribution we will attempt to learn. More specifically, we want to learn a model, p_{model} p_{model} , that represents an estimate of p_{data} p_{data} . If we have done our job correctly, then samples from our model \\tilde x_i \\sim p_{model} \\tilde x_i \\sim p_{model} should look like samples from our data. The density p_{model} p_{model} can be explicitly defined approximated by random sampling from our model So far, we have only looked at models of the first type, sometimes referred to as explicit density models . As an example of the simplest possible model of the second type, sometimes referred to as implicit density models , imagine we had a sampler: j = \\text{randint}(1:N) \\\\ \\text{return} \\ x_j j = \\text{randint}(1:N) \\\\ \\text{return} \\ x_j This perhaps is the most trivial generative model we could define. Clearly, this model has no \"understanding\" of the distribution that generated our data. But what does it mean to \"understand\" a data distribution? the ability to generate new data the ability to smoothly interpolate between data points x_i \\rightarrow x_j x_i \\rightarrow x_j the ability to generate samples representing \"underlying features of variation\" Tip For the last point, think of the example of generating images. We might expect a sampled images from a good model to contain variation in the lighting, or camera angle. We have previously discussed auto-encoders , which achieve this \"understanding\" of our data via compression. Of course, this begs the question: Why do we want our models to be able to \"understand\" a data distribution in the first place? Samples from high dimensional distributions are useful and serve as an excellent test of our ability to represent and manipulate high-dimensional probability distributions (import objects in a wide variety of applied math and engineering domains). Model-based RL makes use of generative models. Generative models can be trained with missing data and can provide predictions on inputs that are missing data (e.g. semi-supervised learning) Generative modals enable machine learning to work with multi-modal outputs (for many tasks, a single input may correspond to many different correct answers, each of which is acceptable). Finally, many tasks intrinsically require realistic generation of samples from some distribution, e.g. single image super-resolution art creation image-to-image translation","title":"Generative Models, a Review"},{"location":"lectures/week_12/#prototypical-generative-model","text":"The prototypical generative model follows the following procedure: Sample some noise, z \\sim p(z) z \\sim p(z) Pass this noise through a model, G G Get a sample, x x from G(z) G(z) In VAEs, we saw that x \\sim p_\\theta(x | z) x \\sim p_\\theta(x | z)","title":"Prototypical Generative Model"},{"location":"lectures/week_12/#maximum-likelihood-estimate","text":"Maximum likelihood estimate is how we have trained all the generative models we have seen thus far . The basic idea is to: Define a model which provides an estimate of a probability distribution parameterized by parameters \\theta \\theta Define a likelihood function that represents the probability of our data and train our parameters to maximize this likelihood e.g. for the i.i.d case: \\prod_{i=1}^N p_{model} (x^{(i)} ; \\theta) \\\\ \\Rightarrow \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) \\prod_{i=1}^N p_{model} (x^{(i)} ; \\theta) \\\\ \\Rightarrow \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\sum \\log p_{model} (x^{(i)} ; \\theta) which is equivalent to the minimizing the D_{KL} D_{KL} between p_{data}(x) p_{data}(x) and p_{model}(x ; \\theta) p_{model}(x ; \\theta) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) \\underset{\\theta}{\\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \\theta)) If we were able to do this precisely, and if p_{data} p_{data} lies within the family of distributions p_{model}(x ; \\theta) p_{model}(x ; \\theta) , then the model would recover p_{data} p_{data} exactly. In practice, we do not have access to p_{data} p_{data} itself, but only to a training set consisting of m m samples from p_{data} p_{data} . We uses these to define \\hat p_{data} \\hat p_{data} , an empirical distribution that places mass only on exactly those m m points, approximating p_{data} p_{data} . Minimizing the D_{KL} D_{KL} between \\hat p_{data} \\hat p_{data} and p_{model} p_{model} is exactly equivalent to maximizing the log-likelihood of the training set.","title":"Maximum Likelihood Estimate"},{"location":"lectures/week_12/#explicit-density-models","text":"Explicit density models (i.e., the models we are discussing and have been discussing the entire course) define an explicit density function p_{model}(x ; \\theta) p_{model}(x ; \\theta) which is used to train the model, typically via maximum likelihood estimation. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill. The main difficulty present in explicit density models is designing a model that can capture all of the complexity of the data to be generated while still maintaining computational tractability. careful construction of models whose structure guarantees their tractability models that admit tractable approximations to the likelihood and its gradients Tractable examples include: Fully Visible Belief Nets (96, 98) WaveNet (2016) In the intractable case, we use variational (e.g. VAEs ) or MCMC approximations to get at p_{model}(x ; \\theta) p_{model}(x ; \\theta) .","title":"Explicit Density Models"},{"location":"lectures/week_12/#generative-model-goal","text":"The goal of generative models, in short, is to produce samples similar to p_{data} p_{data} . But do we really need maximum likelihood estimations to achieve our goals? Tip Everything up until this point was considered review. Everything that follows is new content.","title":"Generative Model Goal"},{"location":"lectures/week_12/#implicit-density-models","text":"In contrast, implicit density models are trained without explicitly defining a density function. The way we interact with p_{model} p_{model} is through samples. A taxonomic tree of generative models is shown below: Cite NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016) . But how do we train it? No density means no likelihood evaluation . We will look at a specific instance of implicit density models known as Generative Adversarial Networks.","title":"Implicit Density Models"},{"location":"lectures/week_12/#generative-adversarial-network-gan-approach","text":"In the generative adversarial approach, we do not have likelihoods, only samples. The idea is based on an adversarial game and pulls heavily from game theory . The basic idea of GANs is to set up a game between two players: One of them is called the generator ( G_{\\theta_G} G_{\\theta_G} ). The generator creates samples that are intended to come from the same distribution as the training data. The other player is the discriminator ( D_{\\theta_D} D_{\\theta_D} ). The discriminator examines samples to determine whether they are real or fake. The discriminator learns using traditional supervised learning techniques, dividing inputs into two classes (real or fake). The generator is trained to fool the discriminator. Both are almost always modeled as neural networks (and are therefore differentiable w.r.t their outputs). Once training is complete, we throw away the discriminator . Tip We can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data. Formally, GANs are structured as a probabilistic model containing latent variables z z and observed variable x x Cite NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016) . to sample from the model: first sample z \\sim p(z) z \\sim p(z) ; where p(z) p(z) is some prior distribution then x = G(z) \\sim p_{model} x = G(z) \\sim p_{model} notice that we never explicitly define a distribution p_{model}(x; \\theta) p_{model}(x; \\theta) . Contrast this with VAEs, where to sample from the model: first sample z \\sim q_\\phi(z | x) z \\sim q_\\phi(z | x) for some input x x then using the encoder, compute \\theta = f(z) \\theta = f(z) finally, using the decoder, sample x \\sim p_\\theta(x | z) x \\sim p_\\theta(x | z) notice that we explicitly define a distribution ( p_\\theta(x | z) p_\\theta(x | z) ) and sample from it.","title":"Generative Adversarial Network (GAN) Approach"},{"location":"lectures/week_12/#adversarial-game-the-specifics","text":"In the adversarial game, both players have cost functions that are defined in terms of both players\u2019 parameters The discriminator wishes to minimize J^{(D)}(\\theta_D, \\theta_G) J^{(D)}(\\theta_D, \\theta_G) and must do so while controlling only \\theta_D \\theta_D . The generator wishes to minimize J^{(G)}(\\theta_D, \\theta_G) J^{(G)}(\\theta_D, \\theta_G) and must do so while controlling only \\theta_G \\theta_G . Because each player\u2019s cost depends on the other player\u2019s parameters, but each player cannot control the other player\u2019s parameters, this scenario is most straightforward to describe as a game rather than as an optimization problem","title":"Adversarial Game, the Specifics"},{"location":"lectures/week_12/#training-procedure","text":"The training process consists of simultaneous SGD . On each step, two minibatches are sampled: a minibatch of x x values from the dataset and a minibatch of z z values drawn from the model\u2019s prior over latent variables, e.g., x \\sim p_{data} x \\sim p_{data} z \\sim p(z) \\Rightarrow x \\sim P_{G(z)} z \\sim p(z) \\Rightarrow x \\sim P_{G(z)} Then two gradient steps are made simultaneously: one updating \\theta_D \\theta_D to reduce J^{(D)} J^{(D)} and one updating \\theta_G \\theta_G to reduce J^{(G)} J^{(G)} .","title":"Training Procedure"},{"location":"lectures/week_12/#cost-functions","text":"Several different cost functions may be used within the GANs framework. All of the different games designed for GANs so far use the same cost for the discriminator, J^{(D)} J^{(D)} . They differ only in terms of the cost used for the generator, J^{(G)} J^{(G)} .","title":"Cost Functions"},{"location":"lectures/week_12/#the-discriminators-cost","text":"The cost used for the discriminator is (almost) always: J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] This is just the standard cross-entropy cost that is minimized when training a standard binary classifier with a sigmoid output. The only difference is that the classifier is trained on two mini-batches of data; one coming from the dataset, where the label is 1 for all examples, and one coming from the generator, where the label is 0 for all examples.","title":"The Discriminator\u2019s Cost"},{"location":"lectures/week_12/#optimal-discriminator-strategy","text":"Our goal is to minimize J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] J^{(D)}(\\theta_D, \\theta_G) = -\\mathbb E_{x \\sim p_{data}}[\\log D(x)] - \\mathbb E_{z \\sim p(z)}[\\log(1 - D(G(z)))] in function space, specifying D(x) D(x) directly. We begin by assuming that both p_{data} p_{data} and p_{model} p_{model} are nonzero everywhere. Note If we do not make this assumption, then some points are never visited during training, and have undefined behavior. To minimize J^{(D)} J^{(D)} with respect to D D , we can write down the functional derivatives with respect to a single entry D^{(x)} D^{(x)} , and set them equal to zero: \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 \\frac{\\delta}{\\delta D(x)}J^{(D)} = 0 By solving this equation, we obtain D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} D^\\star(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)} The discriminator (dashed blue line) estimates the ratio between the data density (black dots) and the sum of the data and model densities. Wherever the output of the discriminator is large, the model density is too low, and wherever the output of the discriminator is small, the model density is too high. Estimating this ratio is the key approximation mechanism used by GANs Note We can't actually compute D^\\star(x) D^\\star(x) directly but it illustrates how the discriminator achieves optimal behavior.","title":"Optimal Discriminator Strategy"},{"location":"lectures/week_12/#optimal-generator-strategy","text":"So far we have specified the cost function for only the discriminator. A complete specification of the game requires that we specify a cost function also for the generator. The simplest version of the game is a zero-sum game , in which the sum of all player\u2019s costs is always zero. In this version of the game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} Because J^{(G)} J^{(G)} is tied directly to J^{(D)} J^{(D)} , we can summarize the entire game with a value function specifying the discriminator\u2019s payoff: V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) V(\\theta^{(D)}, \\theta^{(G)}) = -J^{(D)}(\\theta^{(D)}, \\theta^{(G)}) Zero-sum games are also called minimax games because their solution involves minimization in an outer loop and maximization in an inner loop: \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) \\theta^{(G)\\star} = \\underset{\\theta^{(G)}}{\\operatorname{argmin}} \\underset{\\theta^{(D)}}{\\operatorname{argmax}} V(\\theta^{(D)}, \\theta^{(G)}) Where we want to maximize \\theta_D \\theta_D such that D_{\\theta_D}(x) = 1 D_{\\theta_D}(x) = 1 D_{\\theta_D}(G(z)) = 0 D_{\\theta_D}(G(z)) = 0 and minimize \\theta_G \\theta_G such that D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1 D_{\\theta_D}(G_{\\theta_G}(z)) \\rightarrow 1","title":"Optimal Generator Strategy"},{"location":"lectures/week_12/#heuristic-non-saturating-game","text":"The cost used for the generator in the minimax game, J(^{(G)}) = -J^{(D)} J(^{(G)}) = -J^{(D)} is useful for theoretical analysis, but does not perform especially well in practice. In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, e.g. D(G(z)) = 0 D(G(z)) = 0 , the generator\u2019s gradient vanishes (the cost function becomes flat!). This is known as the saturation problem . To solve this problem, we introduce the non-saturating game, in which we continue to use cross-entropy minimization for the generator, but instead of flipping the sign on the discriminator\u2019s cost to obtain a cost for the generator, we flip the target used to construct the cross-entropy cost J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) J^{(G)} = -\\underset{z \\sim p(z)}{\\operatorname{\\mathbb E}} \\log(D(G(z))) This leads to a strong gradient signal at D(G(z)) = 0 D(G(z)) = 0 . In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the log- probability of the discriminator being mistaken.","title":"Heuristic, Non-saturating Game"},{"location":"lectures/week_12/#maximum-likelihood-game","text":"Warning Skipped for now to save time, not mentioned on the Study Guide .","title":"Maximum Likelihood Game"},{"location":"lectures/week_12/#summary","text":"In summary, Pros Cons Generate samples in parallel No good method of evaluating No Markov Chains needed for sampling Failings of supervised learning (underfitting /overfitting) No variational bound Hard to train Subjectively better samples Mode collapse (samples are very similar)","title":"Summary"},{"location":"lectures/week_13/","text":"Week 13: Flow-based Models This week is not on the final exam, so I won't spend time converting my and written lecture notes into nicely formatted notes here.","title":"Week 13"},{"location":"lectures/week_13/#week-13-flow-based-models","text":"This week is not on the final exam, so I won't spend time converting my and written lecture notes into nicely formatted notes here.","title":"Week 13: Flow-based Models"},{"location":"lectures/week_2/","text":"Week 2: Introduction to Probabilistic Models Assigned Reading Murphy: Chapters 3, 4, 7-9 (excluding * sections) Chapter 3 of David Mackay's textbook Overview Overview of probabilistic models Sufficient statistics Likelihood Maximum likelihood estimation (MLE) Classification Overview of probabilistic models In general, we have variables that can be observed or unobserved ( latent variables are always unobserved!). The job of a probabilistic model is to relate the variables (observed or unobserved). More specifically, a probabilistic learns a joint probability distribution over variables, e.g. p(x_1, x_2, ..., x_N) p(x_1, x_2, ..., x_N) . Because the distributions are parameterized, learning is essentially joint density estimation . In a generative model , we assume our data is generated by some distribution then try to learn that distribution . The joint distribution (or joint density function) is the central object we use to define our model. From this perspective, we can think about the basic tasks we care about in machine learning (ML) as operations on joint distributions. One such task is classification Model : p(X, C) p(X, C) Task : p(C=c | x) = \\frac{p(c, x)}{p(x)} p(C=c | x) = \\frac{p(c, x)}{p(x)} where X X are our inputs, C C our classes and p(x) p(x) is the probability of our data (sometimes called the evidence ). p(x) p(x) can be re-written as the marginal probability p(C=c | x) = \\frac{p(x, c)}{\\sum_C p(x, c_i)} p(C=c | x) = \\frac{p(x, c)}{\\sum_C p(x, c_i)} What happens if c c is never observed? Then we call this clustering . Clustering allows us to compute p(C=c|x) p(C=c|x) (\"the probability that the input belongs to some cluster\") even if c c is unobserved. p(C = c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} p(C = c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} If our inputs and classes are continuous, we call this regression p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int_Y p(x,y)dy} p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int_Y p(x,y)dy} Note The sum over our target/classes in the marginal distribution p(x) p(x) is replaced by an integral. In general, if a variable is always observed, we may not want to model its density (regression / classification) if a variable is never observed (always unobserved) then we call it a hidden or latent variable and we may want to model its density (clustering, density estimation) In fact, we can mostly classify (no pun intended) the problems we care about into four types: Classification : p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_C p(c, x)} p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_C p(c, x)} Clustering : p(c | x) = \\frac{p(c, x)}{p(x)} \\ ; \\ c \\text{ is unobserved} p(c | x) = \\frac{p(c, x)}{p(x)} \\ ; \\ c \\text{ is unobserved} Regression : p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int_Y p(x, y)dy} p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int_Y p(x, y)dy} Density Estimation : p(y | x) = \\frac{p(y, x)}{p(x)} \\ ; \\ y \\text{ is unobserved} p(y | x) = \\frac{p(y, x)}{p(x)} \\ ; \\ y \\text{ is unobserved} Operations on Probabilistic Models The fundamental operations we will perform on a probabilistic model are: Generate Data : For this you need to know how to sample from local models (directed) or how to do Gibbs or other sampling (undirected). Compute probabilities : When all nodes are either observed or marginalized the result is a single number which is the probability of the configuration. Inference : Compute expectations of some things given others which are observed or marginalized. Learning : Set the parameters of the joint distribution given some (partially) observed data to maximize the probability of seeing the data. Tip I believe \"configuration\" means the likelihood of all the variables of our model taking on some set of specific values. Goals of a Probabilistic Model We want to build prediction systems automatically based on data, and as little as possible on expert information. In this course, we\u2019ll use probability to combine evidence from data and make predictions. We\u2019ll use graphical models as a visual shorthand language to express and reason about families of model assumptions, structures, dependencies and information flow, without specifying exact distributional forms or parameters. In this case learning means setting parameters of distributions given a model structure . Note \"Structure learning\" is also possible but we won\u2019t consider it now. More specifically, we want two things of our probabilistic model: Compact representation : we don't want parameters to scale poorly with dimensionality of the data. Efficient computation : we need to be able to compute the marginal and conditional probability from the joint distribution efficiently. Multiple observations, Complete IID data A single observation of the data, X_i X_i is rarely useful on its own. Generally we have data including many observations, which creates a set of random variables: \\mathcal D = \\{X_1, ..., X_n\\} \\mathcal D = \\{X_1, ..., X_n\\} To achieve our above-listed goals, we will make assumptions. Often, we assume the following: 1. Observations are independently and identically distributed (i.i.d) according to the joint distribution of the model. This reduces the computation of the joint probability to the product of the individual probabilities of observations We don't always assume full independence. Sometimes, we assume only some level of independence between variables (e.g. var 3 depends on var 1 but not on var 2 ). 2. We observe all random variables in the domain on each observation (i.e. complete data, or fully observed model). Important We typically shade the nodes in a probabilistic graphical model to indicate they are observed . (Later we will work with unshaded nodes corresponding to missing data or latent variables.) Learning Parameters for a Distribution Lets take an example with discrete random variables . \\[ T: \\text{Temperature} \\; ; \\; h = \\text{\"hot\" or } c = \\text{\"cold\"} \\] \\[ W: \\text{Weather} \\; ; \\; s = \\text{\"sunny\" or } r = \\text{\"raining\"} \\] We know that \\[ P(T=h) = 0.40 \\] \\[ P(T=c) = 0.60 \\] \\[ P(W=s) = 0.70 \\] \\[ P(W=r) = 0.30 \\] and that these states define a valid probability distribution, so P(X) \\ge 0 \\; ; \\; \\sum_x P(X = x) = 1 P(X) \\ge 0 \\; ; \\; \\sum_x P(X = x) = 1 We could create a parameterized, probabilistic model, P(T, W) P(T, W) over the states \\[P(T | \\theta_T) \\ ; \\ \\theta_T = \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix}\\] \\[P(W | \\theta_W) \\ ;\\ \\theta_W = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}\\] Notice that \\theta_T \\theta_T and \\theta_W \\theta_W are the probability distributions of our random variables. Our parameters define the probability of the data explicitly and store it in a vector . We can represent the joint distribution P(T, W) P(T, W) , our model , as: T W P h s 0.28 c r 0.18 h r 0.12 c s 0.42 from the joint distribution (which is again, essentially our model) we can compute the marginals P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 we could also ask questions about conditional probabilities, like P(W = s | T = c) = \\frac{P(W=s,T=c)}{P(T=c)} = \\frac{P(W=s,T=c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64 P(W = s | T = c) = \\frac{P(W=s,T=c)}{P(T=c)} = \\frac{P(W=s,T=c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64 Why did we do this? The whole point of the above example was to show that from a probabilistic model, which itself is just a joint distribution represented as a matrix (or tensor), we can compute both the marginal and conditional probabilities. This will allow us to compute probabilities, generate data and perform inference. Joint Dimensionality Lets take our previous example and expand on it. Firstly, it is helpful to think of the joint distribution of some set of random variables as a grid with k^n k^n squares, where n n is our number of variables and k k our states. For our running example, this means our joint distribution is parameterized by a 4 dimensional vector, containing the probabilities of seeing any pair of states. We could of course, add more random variables to our model. Imagine we add B B , for whether or not we bike into work and H H , for overall health , each with two states. The dimensionality of our parameters (for the joint distribution over T, W, B, H T, W, B, H ) then becomes k^n = 2^4 k^n = 2^4 It is important to note that our joint distribution will be computed based on the assumptions we make about independence between variables. For example, we could assume that while T T and W W are independent from one another, H H is dependent on both T T and W W as well as B B . From the chain rule, we get P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B) P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B) Likelihood function So far, we have focused on the probability function p(x|\\theta) p(x|\\theta) which assigns a probability (density) to any joint configuration of variables x x given fixed parameters \\theta \\theta . But our goal is to learn \\theta \\theta , which we do not start with and which is not fixed . This is the opposite of how we want to think. Really, we have some fixed data and we want to find parameters \\theta \\theta which maximize the likelihood of that data. Note We are asking \"given x x , how do I choose \\theta \\theta ?\". To do this, we define some function of \\theta \\theta for a fixed x x \\ell(\\theta ; x) = \\log p(x|\\theta) \\ell(\\theta ; x) = \\log p(x|\\theta) which we call the log likelihood function . Note The likelihood function is essentially a notational trick in order to make it easy to talk about our data as a function of our parameters. The process of learning is choosing \\theta \\theta to minimize some cost or loss function, L(\\theta) L(\\theta) which includes \\ell (\\theta) \\ell (\\theta) . This can be done in a couple of ways, including: Maximum likelihood estimation (MLE) : L(\\theta) = \\ell (\\theta; \\mathcal D) L(\\theta) = \\ell (\\theta; \\mathcal D) Maximum a posteriori (MAP) : L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta) L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta) Maximum likelihood estimation The basic idea behind maximum likelihood estimation (MLE) is to pick values for our parameters which were most likely to have generated the data we saw \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; \\mathcal D) \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; \\mathcal D) Note MLE is commonly used in statistics, and often leads to \"intuitive\", \"appealing\" or \"natural\" estimators. For IID data \\[p(\\mathcal D | \\theta) = \\prod_m p(x^{(m)} | \\theta)\\] \\[\\ell (\\theta ; D) = \\sum_m \\log p(x^{(m)} | \\theta)\\] The IID assumption turns the log likelihood into a sum , making the derivative easy to compute term by term. Note The negative log likelihood, NLL(\\theta ; D) NLL(\\theta ; D) , simply introduces a negative sign so our optimization problem becomes a minimization, that is, maximizing \\ell (\\theta ; D) \\ell (\\theta ; D) is equivalent to minimizing NLL(\\theta ; D) NLL(\\theta ; D) . Sufficient statistics A statistic is a (possibly vector valued) deterministic function of a (set of) random variable(s). A sufficient statistic is a statistic that conveys exactly the same information about the data generating process that created that data as the entire data itself. In other words, once we know the sufficient statistic, T(x) T(x) , then our inferences are the same as would be obtained from our entire data. More formally, we say that T(X) T(X) is a sufficient statistic for X X if T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\ \\forall \\theta T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\ \\forall \\theta Put another way P(\\theta | T(X)) = P(\\theta | X) P(\\theta | T(X)) = P(\\theta | X) Note Why is this useful? Well, if we have a particular large data sample, a lot of the data may be redundant. If we knew the sufficient statistic for that sample, we could use it in place of the full data sample. Equivalently (by the Neyman factorization theorem) we can write P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) An example is the exponential family p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-g(\\eta)\\} p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-g(\\eta)\\} or, equivalently p(x | \\eta) = h(x)g(\\eta)\\exp\\{\\eta^TT(x)\\} p(x | \\eta) = h(x)g(\\eta)\\exp\\{\\eta^TT(x)\\} Sufficient statistics example: Bernoulli Trials Let us take the example of flipping a fair coin. This process that generates our data can be modeled as a Bernoulli distribution X \\backsim \\text{Ber}(\\theta) X \\backsim \\text{Ber}(\\theta) where X X is a random variable and x_i x_i represents the result of the ith coin flip \\[x_i = 0 \\text{ , if tails}\\] \\[x_i = 1 \\text{ , if heads}\\] the likelihood (assuming independence between flips of the coin) is \\[L = \\prod_{i=1}^N \\theta^{x_i}(1-\\theta)^{1-x_i}\\] \\[= \\theta^{\\sum_{i=1}^N x_i}(1-\\theta)^{N-\\sum_{i=1}^N x_i}\\] So we notice here that our likelihood depends on \\sum_{i=1}^N x_i \\sum_{i=1}^N x_i . In other words, our data only enters the likelihood in this particular form. This tells us that if we know this summary statistic, which we will call T(x) = \\sum_{i=1}^N x_i T(x) = \\sum_{i=1}^N x_i then essentially we know everything that is useful from our sample to do inference. To perform inference with T(x) T(x) , we define the log likelihood \\ell(\\theta ; X) = \\log p(X | \\theta) \\\\ = T(X) \\log \\theta + (N - T(X)) \\log(1-\\theta) \\ell(\\theta ; X) = \\log p(X | \\theta) \\\\ = T(X) \\log \\theta + (N - T(X)) \\log(1-\\theta) then we take the derivative and set it to 0 to find the maximum \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{T(X)}{\\theta} - \\frac{N - T(X)}{1-\\theta} \\\\ \\Rightarrow \\hat \\theta = \\frac{T(X)}{N} \\\\ \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{T(X)}{\\theta} - \\frac{N - T(X)}{1-\\theta} \\\\ \\Rightarrow \\hat \\theta = \\frac{T(X)}{N} \\\\ This is our maximum likelihood estimation of the parameters \\theta \\theta , \\theta^{\\star}_{MLE} \\theta^{\\star}_{MLE} . Note See Lecture 2 slides 10-13 for more examples. Summary of Probabilistic Models In general, learning the parameters of a probabilistic model depends on whether our variables are observed or partially observed, continuous or discrete Continuous Discrete Fully observed variables Bespoke estimates from calculus Normalized counts Partially observed variables Variational inference, recognition networks, MCMC Message passing, variable elimination, junction tree Appendix Useful Resources Helpful video on sufficient statistics. Glossary of Terms","title":"Week 2"},{"location":"lectures/week_2/#week-2-introduction-to-probabilistic-models","text":"","title":"Week 2: Introduction to Probabilistic Models"},{"location":"lectures/week_2/#assigned-reading","text":"Murphy: Chapters 3, 4, 7-9 (excluding * sections) Chapter 3 of David Mackay's textbook","title":"Assigned Reading"},{"location":"lectures/week_2/#overview","text":"Overview of probabilistic models Sufficient statistics Likelihood Maximum likelihood estimation (MLE) Classification","title":"Overview"},{"location":"lectures/week_2/#overview-of-probabilistic-models","text":"In general, we have variables that can be observed or unobserved ( latent variables are always unobserved!). The job of a probabilistic model is to relate the variables (observed or unobserved). More specifically, a probabilistic learns a joint probability distribution over variables, e.g. p(x_1, x_2, ..., x_N) p(x_1, x_2, ..., x_N) . Because the distributions are parameterized, learning is essentially joint density estimation . In a generative model , we assume our data is generated by some distribution then try to learn that distribution . The joint distribution (or joint density function) is the central object we use to define our model. From this perspective, we can think about the basic tasks we care about in machine learning (ML) as operations on joint distributions. One such task is classification Model : p(X, C) p(X, C) Task : p(C=c | x) = \\frac{p(c, x)}{p(x)} p(C=c | x) = \\frac{p(c, x)}{p(x)} where X X are our inputs, C C our classes and p(x) p(x) is the probability of our data (sometimes called the evidence ). p(x) p(x) can be re-written as the marginal probability p(C=c | x) = \\frac{p(x, c)}{\\sum_C p(x, c_i)} p(C=c | x) = \\frac{p(x, c)}{\\sum_C p(x, c_i)} What happens if c c is never observed? Then we call this clustering . Clustering allows us to compute p(C=c|x) p(C=c|x) (\"the probability that the input belongs to some cluster\") even if c c is unobserved. p(C = c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} p(C = c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} If our inputs and classes are continuous, we call this regression p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int_Y p(x,y)dy} p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int_Y p(x,y)dy} Note The sum over our target/classes in the marginal distribution p(x) p(x) is replaced by an integral. In general, if a variable is always observed, we may not want to model its density (regression / classification) if a variable is never observed (always unobserved) then we call it a hidden or latent variable and we may want to model its density (clustering, density estimation) In fact, we can mostly classify (no pun intended) the problems we care about into four types: Classification : p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_C p(c, x)} p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_C p(c, x)} Clustering : p(c | x) = \\frac{p(c, x)}{p(x)} \\ ; \\ c \\text{ is unobserved} p(c | x) = \\frac{p(c, x)}{p(x)} \\ ; \\ c \\text{ is unobserved} Regression : p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int_Y p(x, y)dy} p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int_Y p(x, y)dy} Density Estimation : p(y | x) = \\frac{p(y, x)}{p(x)} \\ ; \\ y \\text{ is unobserved} p(y | x) = \\frac{p(y, x)}{p(x)} \\ ; \\ y \\text{ is unobserved}","title":"Overview of probabilistic models"},{"location":"lectures/week_2/#operations-on-probabilistic-models","text":"The fundamental operations we will perform on a probabilistic model are: Generate Data : For this you need to know how to sample from local models (directed) or how to do Gibbs or other sampling (undirected). Compute probabilities : When all nodes are either observed or marginalized the result is a single number which is the probability of the configuration. Inference : Compute expectations of some things given others which are observed or marginalized. Learning : Set the parameters of the joint distribution given some (partially) observed data to maximize the probability of seeing the data. Tip I believe \"configuration\" means the likelihood of all the variables of our model taking on some set of specific values.","title":"Operations on Probabilistic Models"},{"location":"lectures/week_2/#goals-of-a-probabilistic-model","text":"We want to build prediction systems automatically based on data, and as little as possible on expert information. In this course, we\u2019ll use probability to combine evidence from data and make predictions. We\u2019ll use graphical models as a visual shorthand language to express and reason about families of model assumptions, structures, dependencies and information flow, without specifying exact distributional forms or parameters. In this case learning means setting parameters of distributions given a model structure . Note \"Structure learning\" is also possible but we won\u2019t consider it now. More specifically, we want two things of our probabilistic model: Compact representation : we don't want parameters to scale poorly with dimensionality of the data. Efficient computation : we need to be able to compute the marginal and conditional probability from the joint distribution efficiently.","title":"Goals of a Probabilistic Model"},{"location":"lectures/week_2/#multiple-observations-complete-iid-data","text":"A single observation of the data, X_i X_i is rarely useful on its own. Generally we have data including many observations, which creates a set of random variables: \\mathcal D = \\{X_1, ..., X_n\\} \\mathcal D = \\{X_1, ..., X_n\\} To achieve our above-listed goals, we will make assumptions. Often, we assume the following: 1. Observations are independently and identically distributed (i.i.d) according to the joint distribution of the model. This reduces the computation of the joint probability to the product of the individual probabilities of observations We don't always assume full independence. Sometimes, we assume only some level of independence between variables (e.g. var 3 depends on var 1 but not on var 2 ). 2. We observe all random variables in the domain on each observation (i.e. complete data, or fully observed model). Important We typically shade the nodes in a probabilistic graphical model to indicate they are observed . (Later we will work with unshaded nodes corresponding to missing data or latent variables.)","title":"Multiple observations, Complete IID data"},{"location":"lectures/week_2/#learning-parameters-for-a-distribution","text":"Lets take an example with discrete random variables . \\[ T: \\text{Temperature} \\; ; \\; h = \\text{\"hot\" or } c = \\text{\"cold\"} \\] \\[ W: \\text{Weather} \\; ; \\; s = \\text{\"sunny\" or } r = \\text{\"raining\"} \\] We know that \\[ P(T=h) = 0.40 \\] \\[ P(T=c) = 0.60 \\] \\[ P(W=s) = 0.70 \\] \\[ P(W=r) = 0.30 \\] and that these states define a valid probability distribution, so P(X) \\ge 0 \\; ; \\; \\sum_x P(X = x) = 1 P(X) \\ge 0 \\; ; \\; \\sum_x P(X = x) = 1 We could create a parameterized, probabilistic model, P(T, W) P(T, W) over the states \\[P(T | \\theta_T) \\ ; \\ \\theta_T = \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix}\\] \\[P(W | \\theta_W) \\ ;\\ \\theta_W = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}\\] Notice that \\theta_T \\theta_T and \\theta_W \\theta_W are the probability distributions of our random variables. Our parameters define the probability of the data explicitly and store it in a vector . We can represent the joint distribution P(T, W) P(T, W) , our model , as: T W P h s 0.28 c r 0.18 h r 0.12 c s 0.42 from the joint distribution (which is again, essentially our model) we can compute the marginals P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 we could also ask questions about conditional probabilities, like P(W = s | T = c) = \\frac{P(W=s,T=c)}{P(T=c)} = \\frac{P(W=s,T=c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64 P(W = s | T = c) = \\frac{P(W=s,T=c)}{P(T=c)} = \\frac{P(W=s,T=c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64","title":"Learning Parameters for a Distribution"},{"location":"lectures/week_2/#why-did-we-do-this","text":"The whole point of the above example was to show that from a probabilistic model, which itself is just a joint distribution represented as a matrix (or tensor), we can compute both the marginal and conditional probabilities. This will allow us to compute probabilities, generate data and perform inference.","title":"Why did we do this?"},{"location":"lectures/week_2/#joint-dimensionality","text":"Lets take our previous example and expand on it. Firstly, it is helpful to think of the joint distribution of some set of random variables as a grid with k^n k^n squares, where n n is our number of variables and k k our states. For our running example, this means our joint distribution is parameterized by a 4 dimensional vector, containing the probabilities of seeing any pair of states. We could of course, add more random variables to our model. Imagine we add B B , for whether or not we bike into work and H H , for overall health , each with two states. The dimensionality of our parameters (for the joint distribution over T, W, B, H T, W, B, H ) then becomes k^n = 2^4 k^n = 2^4 It is important to note that our joint distribution will be computed based on the assumptions we make about independence between variables. For example, we could assume that while T T and W W are independent from one another, H H is dependent on both T T and W W as well as B B . From the chain rule, we get P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B) P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B)","title":"Joint Dimensionality"},{"location":"lectures/week_2/#likelihood-function","text":"So far, we have focused on the probability function p(x|\\theta) p(x|\\theta) which assigns a probability (density) to any joint configuration of variables x x given fixed parameters \\theta \\theta . But our goal is to learn \\theta \\theta , which we do not start with and which is not fixed . This is the opposite of how we want to think. Really, we have some fixed data and we want to find parameters \\theta \\theta which maximize the likelihood of that data. Note We are asking \"given x x , how do I choose \\theta \\theta ?\". To do this, we define some function of \\theta \\theta for a fixed x x \\ell(\\theta ; x) = \\log p(x|\\theta) \\ell(\\theta ; x) = \\log p(x|\\theta) which we call the log likelihood function . Note The likelihood function is essentially a notational trick in order to make it easy to talk about our data as a function of our parameters. The process of learning is choosing \\theta \\theta to minimize some cost or loss function, L(\\theta) L(\\theta) which includes \\ell (\\theta) \\ell (\\theta) . This can be done in a couple of ways, including: Maximum likelihood estimation (MLE) : L(\\theta) = \\ell (\\theta; \\mathcal D) L(\\theta) = \\ell (\\theta; \\mathcal D) Maximum a posteriori (MAP) : L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta) L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta)","title":"Likelihood function"},{"location":"lectures/week_2/#maximum-likelihood-estimation","text":"The basic idea behind maximum likelihood estimation (MLE) is to pick values for our parameters which were most likely to have generated the data we saw \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; \\mathcal D) \\hat \\theta_{MLE} = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; \\mathcal D) Note MLE is commonly used in statistics, and often leads to \"intuitive\", \"appealing\" or \"natural\" estimators. For IID data \\[p(\\mathcal D | \\theta) = \\prod_m p(x^{(m)} | \\theta)\\] \\[\\ell (\\theta ; D) = \\sum_m \\log p(x^{(m)} | \\theta)\\] The IID assumption turns the log likelihood into a sum , making the derivative easy to compute term by term. Note The negative log likelihood, NLL(\\theta ; D) NLL(\\theta ; D) , simply introduces a negative sign so our optimization problem becomes a minimization, that is, maximizing \\ell (\\theta ; D) \\ell (\\theta ; D) is equivalent to minimizing NLL(\\theta ; D) NLL(\\theta ; D) .","title":"Maximum likelihood estimation"},{"location":"lectures/week_2/#sufficient-statistics","text":"A statistic is a (possibly vector valued) deterministic function of a (set of) random variable(s). A sufficient statistic is a statistic that conveys exactly the same information about the data generating process that created that data as the entire data itself. In other words, once we know the sufficient statistic, T(x) T(x) , then our inferences are the same as would be obtained from our entire data. More formally, we say that T(X) T(X) is a sufficient statistic for X X if T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\ \\forall \\theta T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\ \\forall \\theta Put another way P(\\theta | T(X)) = P(\\theta | X) P(\\theta | T(X)) = P(\\theta | X) Note Why is this useful? Well, if we have a particular large data sample, a lot of the data may be redundant. If we knew the sufficient statistic for that sample, we could use it in place of the full data sample. Equivalently (by the Neyman factorization theorem) we can write P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) An example is the exponential family p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-g(\\eta)\\} p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-g(\\eta)\\} or, equivalently p(x | \\eta) = h(x)g(\\eta)\\exp\\{\\eta^TT(x)\\} p(x | \\eta) = h(x)g(\\eta)\\exp\\{\\eta^TT(x)\\}","title":"Sufficient statistics"},{"location":"lectures/week_2/#sufficient-statistics-example-bernoulli-trials","text":"Let us take the example of flipping a fair coin. This process that generates our data can be modeled as a Bernoulli distribution X \\backsim \\text{Ber}(\\theta) X \\backsim \\text{Ber}(\\theta) where X X is a random variable and x_i x_i represents the result of the ith coin flip \\[x_i = 0 \\text{ , if tails}\\] \\[x_i = 1 \\text{ , if heads}\\] the likelihood (assuming independence between flips of the coin) is \\[L = \\prod_{i=1}^N \\theta^{x_i}(1-\\theta)^{1-x_i}\\] \\[= \\theta^{\\sum_{i=1}^N x_i}(1-\\theta)^{N-\\sum_{i=1}^N x_i}\\] So we notice here that our likelihood depends on \\sum_{i=1}^N x_i \\sum_{i=1}^N x_i . In other words, our data only enters the likelihood in this particular form. This tells us that if we know this summary statistic, which we will call T(x) = \\sum_{i=1}^N x_i T(x) = \\sum_{i=1}^N x_i then essentially we know everything that is useful from our sample to do inference. To perform inference with T(x) T(x) , we define the log likelihood \\ell(\\theta ; X) = \\log p(X | \\theta) \\\\ = T(X) \\log \\theta + (N - T(X)) \\log(1-\\theta) \\ell(\\theta ; X) = \\log p(X | \\theta) \\\\ = T(X) \\log \\theta + (N - T(X)) \\log(1-\\theta) then we take the derivative and set it to 0 to find the maximum \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{T(X)}{\\theta} - \\frac{N - T(X)}{1-\\theta} \\\\ \\Rightarrow \\hat \\theta = \\frac{T(X)}{N} \\\\ \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{T(X)}{\\theta} - \\frac{N - T(X)}{1-\\theta} \\\\ \\Rightarrow \\hat \\theta = \\frac{T(X)}{N} \\\\ This is our maximum likelihood estimation of the parameters \\theta \\theta , \\theta^{\\star}_{MLE} \\theta^{\\star}_{MLE} . Note See Lecture 2 slides 10-13 for more examples.","title":"Sufficient statistics example: Bernoulli Trials"},{"location":"lectures/week_2/#summary-of-probabilistic-models","text":"In general, learning the parameters of a probabilistic model depends on whether our variables are observed or partially observed, continuous or discrete Continuous Discrete Fully observed variables Bespoke estimates from calculus Normalized counts Partially observed variables Variational inference, recognition networks, MCMC Message passing, variable elimination, junction tree","title":"Summary of Probabilistic Models"},{"location":"lectures/week_2/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_2/#useful-resources","text":"Helpful video on sufficient statistics.","title":"Useful Resources"},{"location":"lectures/week_2/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_3/","text":"Week 3: Directed Graphical Models Assigned Reading Murphy: Chapters 10-12 (excluding * sections) Kevin Murphy's page on graphical models Roger Grosse's slides on backprop Overview Graphical notations Conditional independence Bayes Balls Latent variables Common motifs Graphical model notation The joint distribution of N N random variables can be computed by the chain rule p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots p(x_n | x_{n-1 : 1}) p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots p(x_n | x_{n-1 : 1}) this is true for any joint distribution over any random variables (assuming full dependence between variables). More formally, in probability the chain rule for two random variables is p(x, y) = p(x | y)p(y) p(x, y) = p(x | y)p(y) and for N N random variables p(\\cap^N_{i=1} x_i) = \\prod_{j=1}^N p(x_j | \\cap^{j-1}_{k=1} x_k) p(\\cap^N_{i=1} x_i) = \\prod_{j=1}^N p(x_j | \\cap^{j-1}_{k=1} x_k) Note Note that this is a bit of an abuse of notation, but p(x_k | \\cap^{k-1}_{j=1} x_j) p(x_k | \\cap^{k-1}_{j=1} x_j) will collpase to p(x_1) p(x_1) when k k = 1. Graphically, we might represent a model p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) as where nodes represent random variables arrows mean \"conditioned on\", e.g. \" x_i x_i is conditioned on x_{\\pi_1} x_{\\pi_1} \". For example, the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) is represented as This is what the model looks like with no assumptions on the conditional dependence between variables (said otherwise, we assume full conditional dependence of the joint distribution as per the chain rule). This model will scale poorly (exponential with the number of parameters, or k^n k^n where k k are states and n n are random variables, or nodes.). We can simplify the model by building in our assumptions about the conditional probabilities. More explicitly, a directed graphical model implies a restricted factorization of the joint distribution. Conditional Independence Let X X be the set of nodes in our graph (the random variables of our model), then two (sets of) variables X_A X_A , X_B X_B are conditionally independent given a third variable X_C X_C \\[(X_A \\perp X_B | X_C)\\] if \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) for all X_c X_c . Note \\star\\star \\star\\star is especially important, and we use this several times throughout the lecture. Only a subset of all distributions respect any given (nontrivial) conditional independence statement. The subset of distributions that respect all the CI assumptions we make is the family of distributions consistent with our assumptions. Probabilistic graphical models are a powerful, elegant and simple way to specify such a family. Directed acyclic graphical models (DAGM) A directed acyclic graphical model over N N random variables looks like p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_i x_i is a random variable (node in the graphical model) and x_{\\pi_i} x_{\\pi_i} are the parents of this node. In other words, the joint distribution of a DAGM factors into a product of local conditional distributions , where each node (a random variable) is conditionally dependent on its parent node(s), which could be empty. Tip The Wikipedia entry on Graphical models is helpful, particularly the section on Bayesian networks . Notice the difference between a DAGM and the chain rule for probability we introduced early on: we are conditioning on parent nodes as opposed to every node . Therefore, the model that represents this distribution is exponential in the fan-in of each node (the number of nodes in the parent set), instead of in N N . Independence assumptions on DAGMs Lets look again at the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) we introduced above. First, lets sort the DAGM topologically. The conditional independence of our random variables becomes x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} so random variables x_i x_i and x_{\\widetilde{\\pi_i}} x_{\\widetilde{\\pi_i}} are conditionally independent of each other but conditionally dependent on their parent nodes x_{\\pi_i} x_{\\pi_i} . Note To topological sort or order a DAGM means to sort all parents before their children. Lastly, lets place some assumptions on the conditional dependence of our random variables. Say our model looks like What have the assumptions done to our joint distribution represented by our model? p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) Cleary our assumptions on conditional independence have vastly simplified the model. Now Suppose each is x_i x_i is a binary random variable. Our assumptions on conditional independence also reduce the dimensionality of our model ~~where the model now has only 2^1 + 4(2^2) + 2^3 = 26 2^1 + 4(2^2) + 2^3 = 26 possible configurations~~ I don't think this is correct. Missing Edges Missing edges imply conditional independence . Recall that from the chain rule, we can get (for any joint distribution) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) If our joint distribution is represented by a DAGM, however, then some of the conditioned variables can be dropped. This is equivalent to enforcing conditional independence. D-Separation D-separation , or directed-separation is a notion of connectedness in DAGMs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s); where D-connection implies conditional dependence and d-separation implies conditional independence . In particular, we say that x_A \\bot x_B | x_C x_A \\bot x_B | x_C if every variable in A A is d-separated from every variable in B B conditioned on all the variables in C C . We will look at two methods for checking if an independence is true: A depth-first search algorithm and Bayes Balls . DFS Algorithm for checking independence To check if an independence is true, we can cycle through each node in A A , do a depth-first search to reach every node in B B , and examine the path between them. If all of the paths are d-separated, then we can assert x_A \\bot x_B | x_C x_A \\bot x_B | x_C Thus, it will be sufficient to consider triples of nodes. Note It is not totally clear to me why it is sufficient to consider triples of nodes. This is simply stated \"as is\" on the lecture slides. Lets go through some of the most common triples. Tip It was suggested in class that these types of examples make for really good midterm questions! 1. Chain Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(y|x)P(z|y) P(x, y, z) = P(x)P(y|x)P(z|y) which implies \\begin{align} P(z | x, y) = \\frac{P(x, y, z)}{P(x, y)} \\\\ = \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ = P(z | y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x, y, z)}{P(x, y)} \\\\ &= \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ &= P(z | y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) = P(z | y) P(z | x, y) = P(z | y) and so by \\star\\star \\star\\star , x \\bot z | y x \\bot z | y . Tip It is helpful to think about x x as the past, y y as the present and z z as the future when working with chains such as this one. 2. Common Cause Where we think of y y as the \"common cause\" of the two independent effects x x and z z . Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(y)P(x|y)P(z|y) P(x, y, z) = P(y)P(x|y)P(z|y) which implies \\begin{align} P(x, z | y) = \\frac{P(x, y, z)}{P(y)} \\\\ = \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ = P(x|y)P(z|y) \\\\ \\end{align} \\begin{align} P(x, z | y) &= \\frac{P(x, y, z)}{P(y)} \\\\ &= \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ &= P(x|y)P(z|y) \\\\ \\end{align} \\therefore \\therefore P(x, z| y) = P(x|y)P(z|y) P(x, z| y) = P(x|y)P(z|y) and so by \\star \\star , x \\bot z | y x \\bot z | y . 3. Explaining Away Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(z)P(y|x, z) P(x, y, z) = P(x)P(z)P(y|x, z) which implies \\begin{align} P(z | x, y) = \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ = \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ \\not = P(z|y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ &= \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ &\\not = P(z|y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) \\not = P(z|y) P(z | x, y) \\not = P(z|y) and so by \\star\\star \\star\\star , x \\not \\bot z | y x \\not \\bot z | y . In fact, x x and z z are marginally independent , but given y y they are conditionally dependent . This important effect is called explaining away ( Berkson\u2019s paradox ). Example Imaging flipping two coins independently, represented by events x x and z z . Furthermore, let y=1 y=1 if the coins come up the same and y=0 y=0 if they come up differently. Clearly, x x and z z are independent, but if I tell you y y , they become coupled! Bayes-Balls Algorithm An alternative algorithm for determining conditional independence in a DAGM is the Bayes Balls algorithm. To check if x_A \\bot x_B | x_C x_A \\bot x_B | x_C we need to check if every variable in A A is d-seperated from every variable in B B conditioned on all variables in C C . In other words, given that all the nodes in x_C x_C are \"clamped\", when we \"wiggle\" nodes x_A x_A can we change any of the nodes in x_B x_B ? In general, the algorithm works as follows: Shade all nodes x_C x_C Place \"balls\" at each node in x_A x_A (or x_B x_B ) Let the \"balls\" \"bounce\" around according to some rules If any of the balls reach any of the nodes in x_B x_B from x_A x_A (or x_A x_A from x_B x_B ) then x_A \\not \\bot x_B | x_C x_A \\not \\bot x_B | x_C Otherwise x_A \\bot x_B | x_C x_A \\bot x_B | x_C The rules are as follows : including the boundary rules : where arrows indicate paths the balls can travel, and arrows with bars indicate paths the balls cannot travel. Note Notice balls can travel opposite to edge directions! Here\u2019s a trick for the explaining away case: If y y or any of its descendants is shaded , the ball passes through. Tip See this video for an easy way to remember all 10 rules. Examples Question : In the following graph, is x_1 \\bot x_6 | \\{x_2, x_3\\} x_1 \\bot x_6 | \\{x_2, x_3\\} ? Answer : Yes, by the Bayes Balls algorithm. Question : In the following graph, is x_2 \\bot x_3 | \\{x_1, x_6\\} x_2 \\bot x_3 | \\{x_1, x_6\\} ? Answer : No, by the Bayes Balls algorithm. Plates Because Bayesian methods treat parameters as random variables, we would like to include them in the graphical model. One way to do this is to repeat all the iid observations explicitly and show the parameter only once. A better way is to use plates , in which repeated quantities that are iid are put in a box Plates are like \u201cmacros\u201d that allow you to draw a very complicated graphical model with a simpler notation. The rules of plates are simple : repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. N N ), updating the plate index variable (e.g. n n ) as you go. Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure. Nested Plates Plates can be nested, in which case their arrows get duplicated also, according to the rule: draw an arrow from every copy of the source node to every copy of the destination node. Plates can also cross (intersect), in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them. Example of a DAGM: Markov Chain Markov chains are a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, it is a model that satisfies the Markov property , i.e., conditional on the present state of the system, its future and past states are independent. Note The first images depicts a first -order Markov chain, the second a second -order Markov chain. Unobserved Variables Certain variables in our models may be unobserved ( Q Q in the example given below), either some of the time or always, at training time or at test time. Warning Jesse suggested that all the content between Unobserved Variables and Hidden Markov Models (HMMs) would not be tested. Note Graphically, we will use shading to indicate observation Partially Unobserved (Missing) Variables If variables are occasionally unobserved then they are missing data , e.g., undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we define a new cost function in which we sum out or marginalize the missing values at training or test time \\[\\ell(\\theta ; \\mathcal D) = \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log p(x^m | \\theta)\\] \\[= \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log \\sum_y p(x^m, y | \\theta)\\] Note Recall that p(x) = \\sum_q p(x, q) p(x) = \\sum_q p(x, q) . Latent variables What to do when a variable z z is always unobserved? Depends on where it appears in our model. If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out. E.g., given y y , x x fit the model p(z, y|x) = p(z|y)p(y|x, w)p(w) p(z, y|x) = p(z|y)p(y|x, w)p(w) . In other words if it is a leaf node. However, if z z is conditioned on, we need to model it. E.g. given y y , x x fit the model p(y|x) = \\sum_z p(y|x, z)p(z) p(y|x) = \\sum_z p(y|x, z)p(z) . Where do latent variables come from? Latent variables may appear naturally, from the structure of the problem (because something wasn\u2019t measured, because of faulty sensors, occlusion, privacy, etc.). But we also may want to intentionally introduce latent variables to model complex dependencies between variables without looking at the dependencies between them directly. This can actually simplify the model (e.g., mixtures). Mixture models Think about the following two sets of data, and notice how there is some underlying structure not dependent on x . The most basic latent variable model might introduce a single discrete node, z z , in order to better model the data. This allows different submodels (experts) to contribute to the (conditional) density model in different parts of the space (known as a mixture of experts ). Note The basic idea is to divide conquer: use simple parts to build complex models (e.g., multimodal densities, or piecewise-linear regressions). Mixture densities What if the class is unobserved ? Then we sum it out p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) where the mixing proportions , \\alpha_k \\alpha_k sum to 1, i.e. \\sum_k\\alpha_k = 1 \\sum_k\\alpha_k = 1 . We can use Bayes' rule to compute the posterior probability of the mixture component given some data: p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} these quantities are called responsibilities . Example: Gaussian Mixture Models Consider a mixture of K K Gaussian componentns p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ Density model: p(x | \\theta) p(x | \\theta) is a familiarity signal. Clustering: p(z | x, \\theta) p(z | x, \\theta) is the assignment rule, - \\ell(\\theta) - \\ell(\\theta) is the cost. Warning I didn't really understand this example. Example: Mixtures of Experts Mixtures of experts , also known as conditional mixtures are exactly like a class-conditional model, but the class is unobserved and so we sum it out: p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ where \\sum_k \\alpha_k (x) = 1 \\; \\forall x \\sum_k \\alpha_k (x) = 1 \\; \\forall x . This is a harder problem than the previous example, as we must learn \\alpha(x) \\alpha(x) , often called the gating function (unless we chose z z to be independent of x x ). However, we can still use Bayes' rule to compute the posterior probability of the mixture components given some data: p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)} p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)} Example: Mixtures of Linear Regression Experts In this model, each expert generates data according to a linear function of the input plus additive Gaussian noise p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) where the gating function can be a softmax classifier \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} Remember: we are not modeling the density of the inputs x x . Gradient learning with mixtures We can learn mixture densities using gradient descent on the likelihood as usual. \\ell(\\theta) = \\log p(x | \\theta) = \\log \\sum_k \\alpha_kp_k(x_k | \\theta_k) \\\\ \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{p(x | \\theta)} \\sum_k \\alpha_k \\frac{\\partial p_k(x | \\theta)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{1}{p(x | \\theta)}p_k(x | \\theta_k)\\frac{\\partial \\log p_k (x | \\theta_k)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{p_k(x | \\theta_k)}{p(x | \\theta)} \\frac{\\partial \\ell_k}{\\partial \\theta_k} \\\\ = \\sum_k \\alpha_k r_k \\frac{\\partial \\ell_k}{\\partial \\theta_k} \\ell(\\theta) = \\log p(x | \\theta) = \\log \\sum_k \\alpha_kp_k(x_k | \\theta_k) \\\\ \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{p(x | \\theta)} \\sum_k \\alpha_k \\frac{\\partial p_k(x | \\theta)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{1}{p(x | \\theta)}p_k(x | \\theta_k)\\frac{\\partial \\log p_k (x | \\theta_k)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{p_k(x | \\theta_k)}{p(x | \\theta)} \\frac{\\partial \\ell_k}{\\partial \\theta_k} \\\\ = \\sum_k \\alpha_k r_k \\frac{\\partial \\ell_k}{\\partial \\theta_k} In other words, the gradient is the responsibility weighted sum of the individual log likelihood gradients Tip We used two tricks here to derive the gradient, \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} = \\frac{1}{f(\\theta)} \\cdot \\frac{\\partial f(\\theta)}{\\partial \\theta} \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} = \\frac{1}{f(\\theta)} \\cdot \\frac{\\partial f(\\theta)}{\\partial \\theta} and \\frac{\\partial f(\\theta)}{\\partial \\theta} = f(\\theta) \\cdot \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} \\frac{\\partial f(\\theta)}{\\partial \\theta} = f(\\theta) \\cdot \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} Hidden Markov Models (HMMs) Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. It is a very popular type of latent variable model where Z_t Z_t are hidden states taking on one of K K discrete values X_t X_t are observed variables taking on values in any space the joint probability represented by the graph factorizes according to p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T} | Z_{1:T}) = p(Z_1) \\prod_{t=2}^T p(Z_t | Z_{t-1}) \\prod_{t=1}^T p(X_t | Z_t) p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T} | Z_{1:T}) = p(Z_1) \\prod_{t=2}^T p(Z_t | Z_{t-1}) \\prod_{t=1}^T p(X_t | Z_t) Appendix Useful Resources Metacademy lesson on Bayes Balls . In fact, that link will bring you to a short course on a couple important concepts for this course, including conditional probability, conditional independence, Bayesian networks and d-separation. A video on how to memorize the Bayes Balls rules (this is linked in the above course). Glossary of Terms","title":"Week 3"},{"location":"lectures/week_3/#week-3-directed-graphical-models","text":"","title":"Week 3: Directed Graphical Models"},{"location":"lectures/week_3/#assigned-reading","text":"Murphy: Chapters 10-12 (excluding * sections) Kevin Murphy's page on graphical models Roger Grosse's slides on backprop","title":"Assigned Reading"},{"location":"lectures/week_3/#overview","text":"Graphical notations Conditional independence Bayes Balls Latent variables Common motifs","title":"Overview"},{"location":"lectures/week_3/#graphical-model-notation","text":"The joint distribution of N N random variables can be computed by the chain rule p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots p(x_n | x_{n-1 : 1}) p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots p(x_n | x_{n-1 : 1}) this is true for any joint distribution over any random variables (assuming full dependence between variables). More formally, in probability the chain rule for two random variables is p(x, y) = p(x | y)p(y) p(x, y) = p(x | y)p(y) and for N N random variables p(\\cap^N_{i=1} x_i) = \\prod_{j=1}^N p(x_j | \\cap^{j-1}_{k=1} x_k) p(\\cap^N_{i=1} x_i) = \\prod_{j=1}^N p(x_j | \\cap^{j-1}_{k=1} x_k) Note Note that this is a bit of an abuse of notation, but p(x_k | \\cap^{k-1}_{j=1} x_j) p(x_k | \\cap^{k-1}_{j=1} x_j) will collpase to p(x_1) p(x_1) when k k = 1. Graphically, we might represent a model p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) as where nodes represent random variables arrows mean \"conditioned on\", e.g. \" x_i x_i is conditioned on x_{\\pi_1} x_{\\pi_1} \". For example, the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) is represented as This is what the model looks like with no assumptions on the conditional dependence between variables (said otherwise, we assume full conditional dependence of the joint distribution as per the chain rule). This model will scale poorly (exponential with the number of parameters, or k^n k^n where k k are states and n n are random variables, or nodes.). We can simplify the model by building in our assumptions about the conditional probabilities. More explicitly, a directed graphical model implies a restricted factorization of the joint distribution.","title":"Graphical model notation"},{"location":"lectures/week_3/#conditional-independence","text":"Let X X be the set of nodes in our graph (the random variables of our model), then two (sets of) variables X_A X_A , X_B X_B are conditionally independent given a third variable X_C X_C \\[(X_A \\perp X_B | X_C)\\] if \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) for all X_c X_c . Note \\star\\star \\star\\star is especially important, and we use this several times throughout the lecture. Only a subset of all distributions respect any given (nontrivial) conditional independence statement. The subset of distributions that respect all the CI assumptions we make is the family of distributions consistent with our assumptions. Probabilistic graphical models are a powerful, elegant and simple way to specify such a family.","title":"Conditional Independence"},{"location":"lectures/week_3/#directed-acyclic-graphical-models-dagm","text":"A directed acyclic graphical model over N N random variables looks like p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_i x_i is a random variable (node in the graphical model) and x_{\\pi_i} x_{\\pi_i} are the parents of this node. In other words, the joint distribution of a DAGM factors into a product of local conditional distributions , where each node (a random variable) is conditionally dependent on its parent node(s), which could be empty. Tip The Wikipedia entry on Graphical models is helpful, particularly the section on Bayesian networks . Notice the difference between a DAGM and the chain rule for probability we introduced early on: we are conditioning on parent nodes as opposed to every node . Therefore, the model that represents this distribution is exponential in the fan-in of each node (the number of nodes in the parent set), instead of in N N .","title":"Directed acyclic graphical models (DAGM)"},{"location":"lectures/week_3/#independence-assumptions-on-dagms","text":"Lets look again at the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) we introduced above. First, lets sort the DAGM topologically. The conditional independence of our random variables becomes x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} so random variables x_i x_i and x_{\\widetilde{\\pi_i}} x_{\\widetilde{\\pi_i}} are conditionally independent of each other but conditionally dependent on their parent nodes x_{\\pi_i} x_{\\pi_i} . Note To topological sort or order a DAGM means to sort all parents before their children. Lastly, lets place some assumptions on the conditional dependence of our random variables. Say our model looks like What have the assumptions done to our joint distribution represented by our model? p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) Cleary our assumptions on conditional independence have vastly simplified the model. Now Suppose each is x_i x_i is a binary random variable. Our assumptions on conditional independence also reduce the dimensionality of our model ~~where the model now has only 2^1 + 4(2^2) + 2^3 = 26 2^1 + 4(2^2) + 2^3 = 26 possible configurations~~ I don't think this is correct.","title":"Independence assumptions on DAGMs"},{"location":"lectures/week_3/#missing-edges","text":"Missing edges imply conditional independence . Recall that from the chain rule, we can get (for any joint distribution) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) If our joint distribution is represented by a DAGM, however, then some of the conditioned variables can be dropped. This is equivalent to enforcing conditional independence.","title":"Missing Edges"},{"location":"lectures/week_3/#d-separation","text":"D-separation , or directed-separation is a notion of connectedness in DAGMs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s); where D-connection implies conditional dependence and d-separation implies conditional independence . In particular, we say that x_A \\bot x_B | x_C x_A \\bot x_B | x_C if every variable in A A is d-separated from every variable in B B conditioned on all the variables in C C . We will look at two methods for checking if an independence is true: A depth-first search algorithm and Bayes Balls .","title":"D-Separation"},{"location":"lectures/week_3/#dfs-algorithm-for-checking-independence","text":"To check if an independence is true, we can cycle through each node in A A , do a depth-first search to reach every node in B B , and examine the path between them. If all of the paths are d-separated, then we can assert x_A \\bot x_B | x_C x_A \\bot x_B | x_C Thus, it will be sufficient to consider triples of nodes. Note It is not totally clear to me why it is sufficient to consider triples of nodes. This is simply stated \"as is\" on the lecture slides. Lets go through some of the most common triples. Tip It was suggested in class that these types of examples make for really good midterm questions! 1. Chain Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(y|x)P(z|y) P(x, y, z) = P(x)P(y|x)P(z|y) which implies \\begin{align} P(z | x, y) = \\frac{P(x, y, z)}{P(x, y)} \\\\ = \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ = P(z | y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x, y, z)}{P(x, y)} \\\\ &= \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ &= P(z | y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) = P(z | y) P(z | x, y) = P(z | y) and so by \\star\\star \\star\\star , x \\bot z | y x \\bot z | y . Tip It is helpful to think about x x as the past, y y as the present and z z as the future when working with chains such as this one. 2. Common Cause Where we think of y y as the \"common cause\" of the two independent effects x x and z z . Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(y)P(x|y)P(z|y) P(x, y, z) = P(y)P(x|y)P(z|y) which implies \\begin{align} P(x, z | y) = \\frac{P(x, y, z)}{P(y)} \\\\ = \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ = P(x|y)P(z|y) \\\\ \\end{align} \\begin{align} P(x, z | y) &= \\frac{P(x, y, z)}{P(y)} \\\\ &= \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ &= P(x|y)P(z|y) \\\\ \\end{align} \\therefore \\therefore P(x, z| y) = P(x|y)P(z|y) P(x, z| y) = P(x|y)P(z|y) and so by \\star \\star , x \\bot z | y x \\bot z | y . 3. Explaining Away Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(z)P(y|x, z) P(x, y, z) = P(x)P(z)P(y|x, z) which implies \\begin{align} P(z | x, y) = \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ = \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ \\not = P(z|y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ &= \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ &\\not = P(z|y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) \\not = P(z|y) P(z | x, y) \\not = P(z|y) and so by \\star\\star \\star\\star , x \\not \\bot z | y x \\not \\bot z | y . In fact, x x and z z are marginally independent , but given y y they are conditionally dependent . This important effect is called explaining away ( Berkson\u2019s paradox ). Example Imaging flipping two coins independently, represented by events x x and z z . Furthermore, let y=1 y=1 if the coins come up the same and y=0 y=0 if they come up differently. Clearly, x x and z z are independent, but if I tell you y y , they become coupled!","title":"DFS Algorithm for checking independence"},{"location":"lectures/week_3/#bayes-balls-algorithm","text":"An alternative algorithm for determining conditional independence in a DAGM is the Bayes Balls algorithm. To check if x_A \\bot x_B | x_C x_A \\bot x_B | x_C we need to check if every variable in A A is d-seperated from every variable in B B conditioned on all variables in C C . In other words, given that all the nodes in x_C x_C are \"clamped\", when we \"wiggle\" nodes x_A x_A can we change any of the nodes in x_B x_B ? In general, the algorithm works as follows: Shade all nodes x_C x_C Place \"balls\" at each node in x_A x_A (or x_B x_B ) Let the \"balls\" \"bounce\" around according to some rules If any of the balls reach any of the nodes in x_B x_B from x_A x_A (or x_A x_A from x_B x_B ) then x_A \\not \\bot x_B | x_C x_A \\not \\bot x_B | x_C Otherwise x_A \\bot x_B | x_C x_A \\bot x_B | x_C The rules are as follows : including the boundary rules : where arrows indicate paths the balls can travel, and arrows with bars indicate paths the balls cannot travel. Note Notice balls can travel opposite to edge directions! Here\u2019s a trick for the explaining away case: If y y or any of its descendants is shaded , the ball passes through. Tip See this video for an easy way to remember all 10 rules.","title":"Bayes-Balls Algorithm"},{"location":"lectures/week_3/#examples","text":"Question : In the following graph, is x_1 \\bot x_6 | \\{x_2, x_3\\} x_1 \\bot x_6 | \\{x_2, x_3\\} ? Answer : Yes, by the Bayes Balls algorithm. Question : In the following graph, is x_2 \\bot x_3 | \\{x_1, x_6\\} x_2 \\bot x_3 | \\{x_1, x_6\\} ? Answer : No, by the Bayes Balls algorithm.","title":"Examples"},{"location":"lectures/week_3/#plates","text":"Because Bayesian methods treat parameters as random variables, we would like to include them in the graphical model. One way to do this is to repeat all the iid observations explicitly and show the parameter only once. A better way is to use plates , in which repeated quantities that are iid are put in a box Plates are like \u201cmacros\u201d that allow you to draw a very complicated graphical model with a simpler notation. The rules of plates are simple : repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. N N ), updating the plate index variable (e.g. n n ) as you go. Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure.","title":"Plates"},{"location":"lectures/week_3/#nested-plates","text":"Plates can be nested, in which case their arrows get duplicated also, according to the rule: draw an arrow from every copy of the source node to every copy of the destination node. Plates can also cross (intersect), in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them.","title":"Nested Plates"},{"location":"lectures/week_3/#example-of-a-dagm-markov-chain","text":"Markov chains are a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, it is a model that satisfies the Markov property , i.e., conditional on the present state of the system, its future and past states are independent. Note The first images depicts a first -order Markov chain, the second a second -order Markov chain.","title":"Example of a DAGM: Markov Chain"},{"location":"lectures/week_3/#unobserved-variables","text":"Certain variables in our models may be unobserved ( Q Q in the example given below), either some of the time or always, at training time or at test time. Warning Jesse suggested that all the content between Unobserved Variables and Hidden Markov Models (HMMs) would not be tested. Note Graphically, we will use shading to indicate observation","title":"Unobserved Variables"},{"location":"lectures/week_3/#partially-unobserved-missing-variables","text":"If variables are occasionally unobserved then they are missing data , e.g., undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we define a new cost function in which we sum out or marginalize the missing values at training or test time \\[\\ell(\\theta ; \\mathcal D) = \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log p(x^m | \\theta)\\] \\[= \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log \\sum_y p(x^m, y | \\theta)\\] Note Recall that p(x) = \\sum_q p(x, q) p(x) = \\sum_q p(x, q) .","title":"Partially Unobserved (Missing) Variables"},{"location":"lectures/week_3/#latent-variables","text":"What to do when a variable z z is always unobserved? Depends on where it appears in our model. If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out. E.g., given y y , x x fit the model p(z, y|x) = p(z|y)p(y|x, w)p(w) p(z, y|x) = p(z|y)p(y|x, w)p(w) . In other words if it is a leaf node. However, if z z is conditioned on, we need to model it. E.g. given y y , x x fit the model p(y|x) = \\sum_z p(y|x, z)p(z) p(y|x) = \\sum_z p(y|x, z)p(z) .","title":"Latent variables"},{"location":"lectures/week_3/#where-do-latent-variables-come-from","text":"Latent variables may appear naturally, from the structure of the problem (because something wasn\u2019t measured, because of faulty sensors, occlusion, privacy, etc.). But we also may want to intentionally introduce latent variables to model complex dependencies between variables without looking at the dependencies between them directly. This can actually simplify the model (e.g., mixtures).","title":"Where do latent variables come from?"},{"location":"lectures/week_3/#mixture-models","text":"Think about the following two sets of data, and notice how there is some underlying structure not dependent on x . The most basic latent variable model might introduce a single discrete node, z z , in order to better model the data. This allows different submodels (experts) to contribute to the (conditional) density model in different parts of the space (known as a mixture of experts ). Note The basic idea is to divide conquer: use simple parts to build complex models (e.g., multimodal densities, or piecewise-linear regressions).","title":"Mixture models"},{"location":"lectures/week_3/#mixture-densities","text":"What if the class is unobserved ? Then we sum it out p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) where the mixing proportions , \\alpha_k \\alpha_k sum to 1, i.e. \\sum_k\\alpha_k = 1 \\sum_k\\alpha_k = 1 . We can use Bayes' rule to compute the posterior probability of the mixture component given some data: p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} these quantities are called responsibilities .","title":"Mixture densities"},{"location":"lectures/week_3/#example-gaussian-mixture-models","text":"Consider a mixture of K K Gaussian componentns p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ Density model: p(x | \\theta) p(x | \\theta) is a familiarity signal. Clustering: p(z | x, \\theta) p(z | x, \\theta) is the assignment rule, - \\ell(\\theta) - \\ell(\\theta) is the cost. Warning I didn't really understand this example.","title":"Example: Gaussian Mixture Models"},{"location":"lectures/week_3/#example-mixtures-of-experts","text":"Mixtures of experts , also known as conditional mixtures are exactly like a class-conditional model, but the class is unobserved and so we sum it out: p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ where \\sum_k \\alpha_k (x) = 1 \\; \\forall x \\sum_k \\alpha_k (x) = 1 \\; \\forall x . This is a harder problem than the previous example, as we must learn \\alpha(x) \\alpha(x) , often called the gating function (unless we chose z z to be independent of x x ). However, we can still use Bayes' rule to compute the posterior probability of the mixture components given some data: p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)} p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)}","title":"Example: Mixtures of Experts"},{"location":"lectures/week_3/#example-mixtures-of-linear-regression-experts","text":"In this model, each expert generates data according to a linear function of the input plus additive Gaussian noise p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) where the gating function can be a softmax classifier \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} Remember: we are not modeling the density of the inputs x x .","title":"Example: Mixtures of Linear Regression Experts"},{"location":"lectures/week_3/#gradient-learning-with-mixtures","text":"We can learn mixture densities using gradient descent on the likelihood as usual. \\ell(\\theta) = \\log p(x | \\theta) = \\log \\sum_k \\alpha_kp_k(x_k | \\theta_k) \\\\ \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{p(x | \\theta)} \\sum_k \\alpha_k \\frac{\\partial p_k(x | \\theta)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{1}{p(x | \\theta)}p_k(x | \\theta_k)\\frac{\\partial \\log p_k (x | \\theta_k)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{p_k(x | \\theta_k)}{p(x | \\theta)} \\frac{\\partial \\ell_k}{\\partial \\theta_k} \\\\ = \\sum_k \\alpha_k r_k \\frac{\\partial \\ell_k}{\\partial \\theta_k} \\ell(\\theta) = \\log p(x | \\theta) = \\log \\sum_k \\alpha_kp_k(x_k | \\theta_k) \\\\ \\Rightarrow \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{p(x | \\theta)} \\sum_k \\alpha_k \\frac{\\partial p_k(x | \\theta)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{1}{p(x | \\theta)}p_k(x | \\theta_k)\\frac{\\partial \\log p_k (x | \\theta_k)}{\\partial \\theta} \\\\ = \\sum_k \\alpha_k \\frac{p_k(x | \\theta_k)}{p(x | \\theta)} \\frac{\\partial \\ell_k}{\\partial \\theta_k} \\\\ = \\sum_k \\alpha_k r_k \\frac{\\partial \\ell_k}{\\partial \\theta_k} In other words, the gradient is the responsibility weighted sum of the individual log likelihood gradients Tip We used two tricks here to derive the gradient, \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} = \\frac{1}{f(\\theta)} \\cdot \\frac{\\partial f(\\theta)}{\\partial \\theta} \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} = \\frac{1}{f(\\theta)} \\cdot \\frac{\\partial f(\\theta)}{\\partial \\theta} and \\frac{\\partial f(\\theta)}{\\partial \\theta} = f(\\theta) \\cdot \\frac{\\partial \\log f(\\theta)}{\\partial \\theta} \\frac{\\partial f(\\theta)}{\\partial \\theta} = f(\\theta) \\cdot \\frac{\\partial \\log f(\\theta)}{\\partial \\theta}","title":"Gradient learning with mixtures"},{"location":"lectures/week_3/#hidden-markov-models-hmms","text":"Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. It is a very popular type of latent variable model where Z_t Z_t are hidden states taking on one of K K discrete values X_t X_t are observed variables taking on values in any space the joint probability represented by the graph factorizes according to p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T} | Z_{1:T}) = p(Z_1) \\prod_{t=2}^T p(Z_t | Z_{t-1}) \\prod_{t=1}^T p(X_t | Z_t) p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T} | Z_{1:T}) = p(Z_1) \\prod_{t=2}^T p(Z_t | Z_{t-1}) \\prod_{t=1}^T p(X_t | Z_t)","title":"Hidden Markov Models (HMMs)"},{"location":"lectures/week_3/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_3/#useful-resources","text":"Metacademy lesson on Bayes Balls . In fact, that link will bring you to a short course on a couple important concepts for this course, including conditional probability, conditional independence, Bayesian networks and d-separation. A video on how to memorize the Bayes Balls rules (this is linked in the above course).","title":"Useful Resources"},{"location":"lectures/week_3/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_4/","text":"Week 4: Undirected Graphical Models Assigned Reading Murphy: Chapters 19-19.5 Directed Graphical Models (a Review) So far, we have seen directed acyclic graphical models (DAGMs) . These models represent large joint distributions using local relationships specified by the graph, where each random variable is a node and the edges specify the conditional dependence between random variables (and therefore missing edges imply conditional independence). Graphically, these models looked like The graph factorizes according to the local conditional probabilities p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_{\\pi_i} x_{\\pi_i} are the parents of node x_i x_i . Each node is conditionally independent of its non-descendents given its parents \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i Note Recall that this is simply a topological ordering of the graph (i.e. parents have lower numbers than their children) For discrete variables, each node stores a conditional probability table (CPT) of size k^n k^n , where k k is the number of discrete states and n n the number of conditionally dependent nodes. Example The DAGM above has k^n = 2^{6} k^n = 2^{6} possible configurations. Are DAGMs always useful? For some problems, it is not always clear how to choose the direction for the edges in our DAGMs. Take the example of modeling dependencies in an image our assumptions lead to unatural conditional independence between random variables. Take for example, the Markov blanket of node X_8 X_8 mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} Note The Markov blanket contains the parents, children and co-parents of a node. More generally, it is the set of all variables that shield the node from the rest of the network. I think the point of this example is that one would expect X_2 X_2 and X_{14} X_{14} to be in the Markov blanket mb(8) mb(8) , especially given that X_4 X_4 and X_{12} X_{12} are. An alternative to DAGMs, is undirected graphical models (UGMs). Undirected Graphical Models Undirected graphical models (UDGMs), also called Markov random fields (MRFs) or Markov networks, are a set of random variables described by an undirected graph. As in DAGMs, the nodes in the graph represent random variables . However, in contrast to DAGMs, edges represent probabilistic interactions between neighboring variables (as opposed to conditional dependence). Dependencies in UGMs In DAGMs, we used conditional probabilities to represent the distribution of nodes given their parents. In UGMs, we use a more symmetric parameterization that captures the affinities between related variables. The following three properties are used to determine if nodes are conditionally independent: def . Global Markov Property (G): X_A \\bot X_B | X_C X_A \\bot X_B | X_C iff X_C X_C separates X_A X_A from X_B X_B Note That is, there is no path in the graph between A A and B B that doesn't go through X_C X_C . def . Local Markov Property (Markov Blanket) (L): The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph t \\bot \\mathcal V \\setminus cl(t) | mb(t) t \\bot \\mathcal V \\setminus cl(t) | mb(t) where cl(t) = mb(t) \\cup t cl(t) = mb(t) \\cup t is the closure of node t t . def . Pairwise (Markov) Property (P): The set of nodes that renders two nodes, s s and t t , conditionally independent of each other. s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 Note G_{st} G_{st} is a function that counts the number of edges between nodes s, t s, t where G \\Rightarrow L \\Rightarrow P \\Rightarrow G \\quad p(x) 0 G \\Rightarrow L \\Rightarrow P \\Rightarrow G \\quad p(x) > 0 Simple example Global: \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} Local: 1 \\bot \\text{rest} | \\{2, 3\\} 1 \\bot \\text{rest} | \\{2, 3\\} Pairwise: 1 \\bot 7 | \\text{rest} 1 \\bot 7 | \\text{rest} Image example Global: \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} Local: 1 \\bot \\text{rest} | \\{X_2, X_6\\} 1 \\bot \\text{rest} | \\{X_2, X_6\\} Pairwise: 1 \\bot 20 | \\text{rest} 1 \\bot 20 | \\text{rest} Not all UGMs can be represented as DGMs Take the following UGM for example (a) and our attempts at encoding this as a DGM (b, c). First, note the two conditional independencies of our UGM in (a): A \\bot C|D,B A \\bot C|D,B B \\bot D|A,C B \\bot D|A,C In (b), we are able to encode the first independence, but not the second (i.e., our DGM implies that B is dependent on D given A and C). In (c), we are again able to encode the first independence, but our model also implies that B and D are marginally independent. Not all DGMs can be represented as UGMs It is also true that not all DGMs can be represented as UGMs. One such example is the 'V-structure' that we saw in the explaining away case in lecture 3 . An undirected model is unable to capture the marginal independence, X \\bot Y X \\bot Y that holds at the same time as \\neg (X \\bot Y | Z ) \\neg (X \\bot Y | Z ) . Cliques A clique in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge (i.e., the subgraph induced by the clique is complete ). def . A maximal clique is a clique that cannot be extended by including one more adjacent vertex. def . A maximum clique is a clique of the largest possible size in a given graph. For example, in the following graph a maximal clique is show in blue, while a maximum clique is shown in green. Parameterization of an UGM Let x = (x_1, ..., x_m) x = (x_1, ..., x_m) be the set of all random variables in our graph. Unlike in DGMs, there is no topological ordering associated with an undirected graph, and so we cannot use the chain rule to represent the joint distribution p(x) p(x) . Instead of associating conditional probabilities with each node, we associate potential functions or factors with each maximal clique in the graph. For a given clique c c , we define the potential function or factor \\psi_c(x_c | \\theta_c) \\psi_c(x_c | \\theta_c) to be any non-negative function, where x_c x_c is some subset of variables in x x involved in a unique, maximal clique. The joint distribution is proportional to the product of clique potentials p(x) \\propto \\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) p(x) \\propto \\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) Note Any positive distribution whose conditional independencies are represented with an UGM can be represented this way. More formally , A positive distribution p(x) 0 p(x) > 0 satisfies the conditional independence properties of an undirected graph G G iff p p can be represented as a product of factors, one per maximal clique, i.e., p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) where \\mathcal C \\mathcal C is the set of all (maximal) cliques of G G , and Z(\\theta) Z(\\theta) the partition function , defined as Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) The factored structure of the distribution makes it possible to more efficiently do the sums/integrals needed to compute it. Lets see how to factorize the undirected graph of our running example: p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) Representing potentials Recall how we parameterized DAGMs of discrete random variables by using conditional probability tables to represent the possible configurations of each node. In this case, the parameters were a valid probability distribution (i.e. the probabilities of the possible configurations summed to 1). In UGMs, we do something similar. If the variables are discrete, we can represent the potential (or energy) functions as tables of (non-negative) numbers p(A, B, C, D) = \\frac{1}{Z} \\psi_{A, B}(A, B) \\psi_{B, C}(B, C) \\psi_{C, D}(C, D) \\psi_{A, D}(A, D) p(A, B, C, D) = \\frac{1}{Z} \\psi_{A, B}(A, B) \\psi_{B, C}(B, C) \\psi_{C, D}(C, D) \\psi_{A, D}(A, D) where Warning Why the switch from \\psi \\psi to \\phi \\phi here? It is important to note that these potentials are not probabilities, but instead encode relative affinities between the different assignments. For example, in the above table, a^0, b^0 a^0, b^0 is taken to be 30X more likely than a^1, a^0 a^1, a^0 . Factor product Given 3 disjoint sets of variables X, Y, Z X, Y, Z and factors \\psi_1(X, Y) \\psi_1(X, Y) , \\psi_2(Y, Z) \\psi_2(Y, Z) the factor product is defined as: \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\psi_{Y, Z}(Y, Z) \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\psi_{Y, Z}(Y, Z) From the factor product, we can make queries about the marginal probabilities , e.g. p(a^0, b^0, c^0, d^0) \\propto \\psi_{A, B, C, D}(a^0, b^0, c^0, d^0)\\\\ \\propto \\psi_{A, B}(a^0, b^0)\\psi_{B, C}(b^0, c^0)\\psi_{C, D}(c^0, d^0)\\psi_{A, D}(a^0, d^0) \\\\ \\propto (30)(100)(1)(100) = 300000 p(a^0, b^0, c^0, d^0) \\propto \\psi_{A, B, C, D}(a^0, b^0, c^0, d^0)\\\\ \\propto \\psi_{A, B}(a^0, b^0)\\psi_{B, C}(b^0, c^0)\\psi_{C, D}(c^0, d^0)\\psi_{A, D}(a^0, d^0) \\\\ \\propto (30)(100)(1)(100) = 300000 enumerating all marginal probabilities in a table for our running example, we get Tip To get the normalized marginal probability, divide by the partition function Z(\\theta) = \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c | \\theta_c) Z(\\theta) = \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c | \\theta_c) to compute the marginal probability of a single variable in our graph, e.g. p(b_0) p(b_0) , marginalize over the other variables p(b^0) \\propto \\sum_{a, c, d} p(a, b^o, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B, C, D}(a, b^0, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B}(a, b^0)\\psi_{B, C}(b^0, c)\\psi_{C, D}(c, d)\\psi_{A, D}(a, d) p(b^0) \\propto \\sum_{a, c, d} p(a, b^o, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B, C, D}(a, b^0, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B}(a, b^0)\\psi_{B, C}(b^0, c)\\psi_{C, D}(c, d)\\psi_{A, D}(a, d) we can also make queries about the conditional probability . Conditioning on a assignment u u to a subset of variables U U can be done by Eliminating all entries that are inconsistent with the assignment Re-normalizing the remaining entries so that they sum to 1 For example, conditioning on c_1 c_1 p(c^1 | a, b, d) \\propto \\sum_{a, b, d} p(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B, C, D}(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B}(a, b)\\psi_{B, C}(b, c^1)\\psi_{C, D}(c^1, d)\\psi_{A, D}(a, d) p(c^1 | a, b, d) \\propto \\sum_{a, b, d} p(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B, C, D}(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B}(a, b)\\psi_{B, C}(b, c^1)\\psi_{C, D}(c^1, d)\\psi_{A, D}(a, d) from this sum, take only factors consistent with the assignment c^1 c^1 , re-normalize remaining entries and then sum. Error I don't fully understand this section. Seems to be a disconnect between lecture notes and lecture slides. Re-visit, clean-up, and if I still don't understand go to office hours.","title":"Week 4"},{"location":"lectures/week_4/#week-4-undirected-graphical-models","text":"","title":"Week 4: Undirected Graphical Models"},{"location":"lectures/week_4/#assigned-reading","text":"Murphy: Chapters 19-19.5","title":"Assigned Reading"},{"location":"lectures/week_4/#directed-graphical-models-a-review","text":"So far, we have seen directed acyclic graphical models (DAGMs) . These models represent large joint distributions using local relationships specified by the graph, where each random variable is a node and the edges specify the conditional dependence between random variables (and therefore missing edges imply conditional independence). Graphically, these models looked like The graph factorizes according to the local conditional probabilities p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_{\\pi_i} x_{\\pi_i} are the parents of node x_i x_i . Each node is conditionally independent of its non-descendents given its parents \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i Note Recall that this is simply a topological ordering of the graph (i.e. parents have lower numbers than their children) For discrete variables, each node stores a conditional probability table (CPT) of size k^n k^n , where k k is the number of discrete states and n n the number of conditionally dependent nodes. Example The DAGM above has k^n = 2^{6} k^n = 2^{6} possible configurations.","title":"Directed Graphical Models (a Review)"},{"location":"lectures/week_4/#are-dagms-always-useful","text":"For some problems, it is not always clear how to choose the direction for the edges in our DAGMs. Take the example of modeling dependencies in an image our assumptions lead to unatural conditional independence between random variables. Take for example, the Markov blanket of node X_8 X_8 mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} Note The Markov blanket contains the parents, children and co-parents of a node. More generally, it is the set of all variables that shield the node from the rest of the network. I think the point of this example is that one would expect X_2 X_2 and X_{14} X_{14} to be in the Markov blanket mb(8) mb(8) , especially given that X_4 X_4 and X_{12} X_{12} are. An alternative to DAGMs, is undirected graphical models (UGMs).","title":"Are DAGMs always useful?"},{"location":"lectures/week_4/#undirected-graphical-models","text":"Undirected graphical models (UDGMs), also called Markov random fields (MRFs) or Markov networks, are a set of random variables described by an undirected graph. As in DAGMs, the nodes in the graph represent random variables . However, in contrast to DAGMs, edges represent probabilistic interactions between neighboring variables (as opposed to conditional dependence).","title":"Undirected Graphical Models"},{"location":"lectures/week_4/#dependencies-in-ugms","text":"In DAGMs, we used conditional probabilities to represent the distribution of nodes given their parents. In UGMs, we use a more symmetric parameterization that captures the affinities between related variables. The following three properties are used to determine if nodes are conditionally independent: def . Global Markov Property (G): X_A \\bot X_B | X_C X_A \\bot X_B | X_C iff X_C X_C separates X_A X_A from X_B X_B Note That is, there is no path in the graph between A A and B B that doesn't go through X_C X_C . def . Local Markov Property (Markov Blanket) (L): The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph t \\bot \\mathcal V \\setminus cl(t) | mb(t) t \\bot \\mathcal V \\setminus cl(t) | mb(t) where cl(t) = mb(t) \\cup t cl(t) = mb(t) \\cup t is the closure of node t t . def . Pairwise (Markov) Property (P): The set of nodes that renders two nodes, s s and t t , conditionally independent of each other. s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 Note G_{st} G_{st} is a function that counts the number of edges between nodes s, t s, t where G \\Rightarrow L \\Rightarrow P \\Rightarrow G \\quad p(x) 0 G \\Rightarrow L \\Rightarrow P \\Rightarrow G \\quad p(x) > 0","title":"Dependencies in UGMs"},{"location":"lectures/week_4/#simple-example","text":"Global: \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} Local: 1 \\bot \\text{rest} | \\{2, 3\\} 1 \\bot \\text{rest} | \\{2, 3\\} Pairwise: 1 \\bot 7 | \\text{rest} 1 \\bot 7 | \\text{rest}","title":"Simple example"},{"location":"lectures/week_4/#image-example","text":"Global: \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} \\{X_1, X_2\\} \\bot \\{X_{15}, X_{20}\\} | \\{X_3, X_6, X_7\\} Local: 1 \\bot \\text{rest} | \\{X_2, X_6\\} 1 \\bot \\text{rest} | \\{X_2, X_6\\} Pairwise: 1 \\bot 20 | \\text{rest} 1 \\bot 20 | \\text{rest}","title":"Image example"},{"location":"lectures/week_4/#not-all-ugms-can-be-represented-as-dgms","text":"Take the following UGM for example (a) and our attempts at encoding this as a DGM (b, c). First, note the two conditional independencies of our UGM in (a): A \\bot C|D,B A \\bot C|D,B B \\bot D|A,C B \\bot D|A,C In (b), we are able to encode the first independence, but not the second (i.e., our DGM implies that B is dependent on D given A and C). In (c), we are again able to encode the first independence, but our model also implies that B and D are marginally independent.","title":"Not all UGMs can be represented as DGMs"},{"location":"lectures/week_4/#not-all-dgms-can-be-represented-as-ugms","text":"It is also true that not all DGMs can be represented as UGMs. One such example is the 'V-structure' that we saw in the explaining away case in lecture 3 . An undirected model is unable to capture the marginal independence, X \\bot Y X \\bot Y that holds at the same time as \\neg (X \\bot Y | Z ) \\neg (X \\bot Y | Z ) .","title":"Not all DGMs can be represented as UGMs"},{"location":"lectures/week_4/#cliques","text":"A clique in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge (i.e., the subgraph induced by the clique is complete ). def . A maximal clique is a clique that cannot be extended by including one more adjacent vertex. def . A maximum clique is a clique of the largest possible size in a given graph. For example, in the following graph a maximal clique is show in blue, while a maximum clique is shown in green.","title":"Cliques"},{"location":"lectures/week_4/#parameterization-of-an-ugm","text":"Let x = (x_1, ..., x_m) x = (x_1, ..., x_m) be the set of all random variables in our graph. Unlike in DGMs, there is no topological ordering associated with an undirected graph, and so we cannot use the chain rule to represent the joint distribution p(x) p(x) . Instead of associating conditional probabilities with each node, we associate potential functions or factors with each maximal clique in the graph. For a given clique c c , we define the potential function or factor \\psi_c(x_c | \\theta_c) \\psi_c(x_c | \\theta_c) to be any non-negative function, where x_c x_c is some subset of variables in x x involved in a unique, maximal clique. The joint distribution is proportional to the product of clique potentials p(x) \\propto \\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) p(x) \\propto \\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) Note Any positive distribution whose conditional independencies are represented with an UGM can be represented this way. More formally , A positive distribution p(x) 0 p(x) > 0 satisfies the conditional independence properties of an undirected graph G G iff p p can be represented as a product of factors, one per maximal clique, i.e., p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta_c) where \\mathcal C \\mathcal C is the set of all (maximal) cliques of G G , and Z(\\theta) Z(\\theta) the partition function , defined as Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) The factored structure of the distribution makes it possible to more efficiently do the sums/integrals needed to compute it. Lets see how to factorize the undirected graph of our running example: p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7)","title":"Parameterization of an UGM"},{"location":"lectures/week_4/#representing-potentials","text":"Recall how we parameterized DAGMs of discrete random variables by using conditional probability tables to represent the possible configurations of each node. In this case, the parameters were a valid probability distribution (i.e. the probabilities of the possible configurations summed to 1). In UGMs, we do something similar. If the variables are discrete, we can represent the potential (or energy) functions as tables of (non-negative) numbers p(A, B, C, D) = \\frac{1}{Z} \\psi_{A, B}(A, B) \\psi_{B, C}(B, C) \\psi_{C, D}(C, D) \\psi_{A, D}(A, D) p(A, B, C, D) = \\frac{1}{Z} \\psi_{A, B}(A, B) \\psi_{B, C}(B, C) \\psi_{C, D}(C, D) \\psi_{A, D}(A, D) where Warning Why the switch from \\psi \\psi to \\phi \\phi here? It is important to note that these potentials are not probabilities, but instead encode relative affinities between the different assignments. For example, in the above table, a^0, b^0 a^0, b^0 is taken to be 30X more likely than a^1, a^0 a^1, a^0 .","title":"Representing potentials"},{"location":"lectures/week_4/#factor-product","text":"Given 3 disjoint sets of variables X, Y, Z X, Y, Z and factors \\psi_1(X, Y) \\psi_1(X, Y) , \\psi_2(Y, Z) \\psi_2(Y, Z) the factor product is defined as: \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\psi_{Y, Z}(Y, Z) \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\psi_{Y, Z}(Y, Z) From the factor product, we can make queries about the marginal probabilities , e.g. p(a^0, b^0, c^0, d^0) \\propto \\psi_{A, B, C, D}(a^0, b^0, c^0, d^0)\\\\ \\propto \\psi_{A, B}(a^0, b^0)\\psi_{B, C}(b^0, c^0)\\psi_{C, D}(c^0, d^0)\\psi_{A, D}(a^0, d^0) \\\\ \\propto (30)(100)(1)(100) = 300000 p(a^0, b^0, c^0, d^0) \\propto \\psi_{A, B, C, D}(a^0, b^0, c^0, d^0)\\\\ \\propto \\psi_{A, B}(a^0, b^0)\\psi_{B, C}(b^0, c^0)\\psi_{C, D}(c^0, d^0)\\psi_{A, D}(a^0, d^0) \\\\ \\propto (30)(100)(1)(100) = 300000 enumerating all marginal probabilities in a table for our running example, we get Tip To get the normalized marginal probability, divide by the partition function Z(\\theta) = \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c | \\theta_c) Z(\\theta) = \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c | \\theta_c) to compute the marginal probability of a single variable in our graph, e.g. p(b_0) p(b_0) , marginalize over the other variables p(b^0) \\propto \\sum_{a, c, d} p(a, b^o, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B, C, D}(a, b^0, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B}(a, b^0)\\psi_{B, C}(b^0, c)\\psi_{C, D}(c, d)\\psi_{A, D}(a, d) p(b^0) \\propto \\sum_{a, c, d} p(a, b^o, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B, C, D}(a, b^0, c, d) \\\\ \\propto \\sum_{a, c, d} \\psi_{A, B}(a, b^0)\\psi_{B, C}(b^0, c)\\psi_{C, D}(c, d)\\psi_{A, D}(a, d) we can also make queries about the conditional probability . Conditioning on a assignment u u to a subset of variables U U can be done by Eliminating all entries that are inconsistent with the assignment Re-normalizing the remaining entries so that they sum to 1 For example, conditioning on c_1 c_1 p(c^1 | a, b, d) \\propto \\sum_{a, b, d} p(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B, C, D}(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B}(a, b)\\psi_{B, C}(b, c^1)\\psi_{C, D}(c^1, d)\\psi_{A, D}(a, d) p(c^1 | a, b, d) \\propto \\sum_{a, b, d} p(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B, C, D}(a, b, c^1, d) \\\\ \\propto \\sum_{a, b, d} \\psi_{A, B}(a, b)\\psi_{B, C}(b, c^1)\\psi_{C, D}(c^1, d)\\psi_{A, D}(a, d) from this sum, take only factors consistent with the assignment c^1 c^1 , re-normalize remaining entries and then sum. Error I don't fully understand this section. Seems to be a disconnect between lecture notes and lecture slides. Re-visit, clean-up, and if I still don't understand go to office hours.","title":"Factor product"},{"location":"lectures/week_5/","text":"Week 5: Exact Inference Assigned Reading Murphy: Chapter 20 MacKay: Chapter 21.1 (worked example with numbers) MacKay: Chapter 16 (Message Passing, including soldier intuition) MacKay: Chapter 26 Exact Inference on Factor Graphs Overview Variable elimination (VE) Complexity of VE Inference in PGM In this lecture, we will explore inference in probabilistic graphical models (PGM). Let X_F = \\text{The unobserved variable we want to infer} \\\\ X_E = \\text{The observed evidence} \\\\ X_R = X - \\{X_F, X_E\\} X_F = \\text{The unobserved variable we want to infer} \\\\ X_E = \\text{The observed evidence} \\\\ X_R = X - \\{X_F, X_E\\} where X_R X_R is the set of random variables in our model that are neither part of the query nor the evidence. We will focus computing the conditional probability distribution p(X_F | X_E) = \\frac{p(X_F, X_E)}{p(X_E)} p(X_F | X_E) = \\frac{p(X_F, X_E)}{p(X_E)} each of the distributions we need to compute can be computed by marginalizing over the other variables. p(X_F, X_E) = \\sum_{X_R}p(X_F, X_E, X_R) \\\\ p(X_E) = \\sum_{X_F, X_R}p(X_F, X_E, X_R) p(X_F, X_E) = \\sum_{X_R}p(X_F, X_E, X_R) \\\\ p(X_E) = \\sum_{X_F, X_R}p(X_F, X_E, X_R) However, naively marginalizing over all unobserved variables requires a number of computations exponential in the number of random variables, N N , in our model. Variable elimination Variable elimination is a simple and general exact inference algorithm in any probabilistic graphical model. The running time will depend on the graph structure of the model, but , by using dynamic programming we can avoid enumerating all assignments. Simple Example: Chain Lets start with the example of a simple chain A \\rightarrow B \\rightarrow C \\rightarrow D A \\rightarrow B \\rightarrow C \\rightarrow D where we want to compute P(D) P(D) . We have X_F = \\{D\\}, \\ X_E = \\{\\}, \\ X_R = \\{A, B, C\\} X_F = \\{D\\}, \\ X_E = \\{\\}, \\ X_R = \\{A, B, C\\} and \\begin{align} P(X_F) = \\sum_{X_R} p(X_F, X_R) \\\\ \\Rightarrow P(D) = \\sum_{A, B, C}p(A, B, C, D) \\\\ = \\sum_C \\sum_B \\sum_A p(A)p(B | A) p(C | B) p(D | C) \\end{align} \\begin{align} P(X_F) &= \\sum_{X_R} p(X_F, X_R) \\\\ \\Rightarrow P(D) &= \\sum_{A, B, C}p(A, B, C, D) \\\\ & = \\sum_C \\sum_B \\sum_A p(A)p(B | A) p(C | B) p(D | C) \\end{align} clearly, this is exponential in the number of variables ( \\mathcal O(k^n) \\mathcal O(k^n) ). But, reordering the joint distribution P(D) = \\sum_C p(D | C) \\sum_B p(C | B) \\sum_A p(A)p(B | A) P(D) = \\sum_C p(D | C) \\sum_B p(C | B) \\sum_A p(A)p(B | A) we can begin to simplify the summation P(D) = \\sum_C p(D | C) \\sum_b p(C | B) \\sum_A p(A)p(B | A) \\\\ = \\sum_C p(D | C) \\sum_B p(C | B) P(B) \\\\ = \\sum_C p(D | C) P(C) \\\\ P(D) = \\sum_C p(D | C) \\sum_b p(C | B) \\sum_A p(A)p(B | A) \\\\ = \\sum_C p(D | C) \\sum_B p(C | B) P(B) \\\\ = \\sum_C p(D | C) P(C) \\\\ So, by using dynamic programming to do the computation inside out instead of outside in , we have done inference over the joint distribution represented by the chain without generating it explicitly . The cost of performing inference on the chain in this manner is \\mathcal O(nk^2) \\mathcal O(nk^2) . In comparison, generating the full joint distribution and marginalizing over it has complexity \\mathcal O(k^n) \\mathcal O(k^n) ! Tip See slide 7-13 for a full derivation of this example, including the runtime \\mathcal O(nk^2) \\mathcal O(nk^2) . Simple Example: DGM Lets take the DGM we saw in lecture 3 What is p(x_1 | \\bar x_6) p(x_1 | \\bar x_6) ? We have Note The \\bar x \\bar x means that the variable is observed. \\ X_F = \\{x_1\\}, X_E = \\{x_6\\}, \\ X_R = \\{x_2, x_3, x_4, x_5\\} \\ X_F = \\{x_1\\}, X_E = \\{x_6\\}, \\ X_R = \\{x_2, x_3, x_4, x_5\\} and \\begin{align} p(X_F | X_E) = \\frac{\\sum_{X_R} p(X_F, X_E, X_R)}{\\sum_{X_F, X_R} p(X_F, X_E, X_R)} \\\\ \\Rightarrow p(x_1 | \\bar x_6) = \\frac{p(x_1, \\bar x_6)}{p(\\bar x_6)} \\\\ = \\frac{p(x_1, \\bar x_6)}{\\sum_{x \\in {X_F, X_R}}p(x, \\bar x_6)} \\\\ \\end{align} \\begin{align} p(X_F | X_E) &= \\frac{\\sum_{X_R} p(X_F, X_E, X_R)}{\\sum_{X_F, X_R} p(X_F, X_E, X_R)} \\\\ \\Rightarrow p(x_1 | \\bar x_6) &= \\frac{p(x_1, \\bar x_6)}{p(\\bar x_6)} \\\\ &= \\frac{p(x_1, \\bar x_6)}{\\sum_{x \\in {X_F, X_R}}p(x, \\bar x_6)} \\\\ \\end{align} to compute p(x_1, \\bar x_6) p(x_1, \\bar x_6) , we use variable elimination p(x_1, \\bar x_6) = p(x_1) \\sum_{x_2} \\sum_{x_3} \\sum_{x_4} \\sum_{x_5} p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(\\bar x_6 | x_2, x_5)\\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) \\sum_{x_5} p(x_5 | x_3)p(\\bar x_6 | x_2, x_5) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\sum_{x_4} p(x_4 | x_2) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) p(\\bar x_6 | x_1, x_2) \\\\ = p(x_1) p(\\bar x_6 | x_1) \\\\ p(x_1, \\bar x_6) = p(x_1) \\sum_{x_2} \\sum_{x_3} \\sum_{x_4} \\sum_{x_5} p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(\\bar x_6 | x_2, x_5)\\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) \\sum_{x_5} p(x_5 | x_3)p(\\bar x_6 | x_2, x_5) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\sum_{x_4} p(x_4 | x_2) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) p(\\bar x_6 | x_1, x_2) \\\\ = p(x_1) p(\\bar x_6 | x_1) \\\\ Summary so far A worst case analysis says that computing the joint distribution is NP-hard. In practice, some subexpressions in the joint depend only on a subset of variables due to the structure of the Bayesian network. We can therefore use dynamic programming to reduce the number of computations, however, this depends on choosing a good variable elimination ordering . Sum-product inference We want an algorithm to compute P(Y) P(Y) for directed and undirected models. This can be reduced to the following sum-product inference task \\tau(Y) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, y_{Scope[\\psi] \\cap Y}) \\quad \\forall Y \\tau(Y) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, y_{Scope[\\psi] \\cap Y}) \\quad \\forall Y where \\Psi \\Psi is a set of potentials or factors. For directed models , \\Psi \\Psi is given by the conditional probability distributions for all variables \\Psi = \\{\\psi_{x_i}\\}^N_{i=1} = \\{p(x_i | x_{\\pi_i})\\}^N_{i=1} \\Psi = \\{\\psi_{x_i}\\}^N_{i=1} = \\{p(x_i | x_{\\pi_i})\\}^N_{i=1} where the sum is over the set Z = X - X_F Z = X - X_F . For undirected models , \\Psi \\Psi is given by the set of potentials. Because the sum product returns unnormalized \\Phi \\Phi distributions, we must normalize by \\sum_Y\\tau(y) \\sum_Y\\tau(y) . Example: Directed Graph Take the following directed graph as example The joint distribution is given by p(C, D, I, G, S, L, H, J) = p(C)p(D|C)p(I)p(G | D, I)p(L | G)P(S | I)p(J | S, L)p(H | J, G) p(C, D, I, G, S, L, H, J) = p(C)p(D|C)p(I)p(G | D, I)p(L | G)P(S | I)p(J | S, L)p(H | J, G) with factors \\Psi = \\{\\psi_C(C), \\psi_D(C, D), \\psi_I(I), \\psi_G(G, D, I), \\psi_L(L, G), \\psi_S(S, I), \\psi_J(J, S, L), \\psi_H(H, J, H)\\} \\Psi = \\{\\psi_C(C), \\psi_D(C, D), \\psi_I(I), \\psi_G(G, D, I), \\psi_L(L, G), \\psi_S(S, I), \\psi_J(J, S, L), \\psi_H(H, J, H)\\} Let's do variable elimination with ordering \\prec \\{C, D, I, H, G, S, L\\} \\prec \\{C, D, I, H, G, S, L\\}","title":"Week 5"},{"location":"lectures/week_5/#week-5-exact-inference","text":"","title":"Week 5: Exact Inference"},{"location":"lectures/week_5/#assigned-reading","text":"Murphy: Chapter 20 MacKay: Chapter 21.1 (worked example with numbers) MacKay: Chapter 16 (Message Passing, including soldier intuition) MacKay: Chapter 26 Exact Inference on Factor Graphs","title":"Assigned Reading"},{"location":"lectures/week_5/#overview","text":"Variable elimination (VE) Complexity of VE","title":"Overview"},{"location":"lectures/week_5/#inference-in-pgm","text":"In this lecture, we will explore inference in probabilistic graphical models (PGM). Let X_F = \\text{The unobserved variable we want to infer} \\\\ X_E = \\text{The observed evidence} \\\\ X_R = X - \\{X_F, X_E\\} X_F = \\text{The unobserved variable we want to infer} \\\\ X_E = \\text{The observed evidence} \\\\ X_R = X - \\{X_F, X_E\\} where X_R X_R is the set of random variables in our model that are neither part of the query nor the evidence. We will focus computing the conditional probability distribution p(X_F | X_E) = \\frac{p(X_F, X_E)}{p(X_E)} p(X_F | X_E) = \\frac{p(X_F, X_E)}{p(X_E)} each of the distributions we need to compute can be computed by marginalizing over the other variables. p(X_F, X_E) = \\sum_{X_R}p(X_F, X_E, X_R) \\\\ p(X_E) = \\sum_{X_F, X_R}p(X_F, X_E, X_R) p(X_F, X_E) = \\sum_{X_R}p(X_F, X_E, X_R) \\\\ p(X_E) = \\sum_{X_F, X_R}p(X_F, X_E, X_R) However, naively marginalizing over all unobserved variables requires a number of computations exponential in the number of random variables, N N , in our model.","title":"Inference in PGM"},{"location":"lectures/week_5/#variable-elimination","text":"Variable elimination is a simple and general exact inference algorithm in any probabilistic graphical model. The running time will depend on the graph structure of the model, but , by using dynamic programming we can avoid enumerating all assignments.","title":"Variable elimination"},{"location":"lectures/week_5/#simple-example-chain","text":"Lets start with the example of a simple chain A \\rightarrow B \\rightarrow C \\rightarrow D A \\rightarrow B \\rightarrow C \\rightarrow D where we want to compute P(D) P(D) . We have X_F = \\{D\\}, \\ X_E = \\{\\}, \\ X_R = \\{A, B, C\\} X_F = \\{D\\}, \\ X_E = \\{\\}, \\ X_R = \\{A, B, C\\} and \\begin{align} P(X_F) = \\sum_{X_R} p(X_F, X_R) \\\\ \\Rightarrow P(D) = \\sum_{A, B, C}p(A, B, C, D) \\\\ = \\sum_C \\sum_B \\sum_A p(A)p(B | A) p(C | B) p(D | C) \\end{align} \\begin{align} P(X_F) &= \\sum_{X_R} p(X_F, X_R) \\\\ \\Rightarrow P(D) &= \\sum_{A, B, C}p(A, B, C, D) \\\\ & = \\sum_C \\sum_B \\sum_A p(A)p(B | A) p(C | B) p(D | C) \\end{align} clearly, this is exponential in the number of variables ( \\mathcal O(k^n) \\mathcal O(k^n) ). But, reordering the joint distribution P(D) = \\sum_C p(D | C) \\sum_B p(C | B) \\sum_A p(A)p(B | A) P(D) = \\sum_C p(D | C) \\sum_B p(C | B) \\sum_A p(A)p(B | A) we can begin to simplify the summation P(D) = \\sum_C p(D | C) \\sum_b p(C | B) \\sum_A p(A)p(B | A) \\\\ = \\sum_C p(D | C) \\sum_B p(C | B) P(B) \\\\ = \\sum_C p(D | C) P(C) \\\\ P(D) = \\sum_C p(D | C) \\sum_b p(C | B) \\sum_A p(A)p(B | A) \\\\ = \\sum_C p(D | C) \\sum_B p(C | B) P(B) \\\\ = \\sum_C p(D | C) P(C) \\\\ So, by using dynamic programming to do the computation inside out instead of outside in , we have done inference over the joint distribution represented by the chain without generating it explicitly . The cost of performing inference on the chain in this manner is \\mathcal O(nk^2) \\mathcal O(nk^2) . In comparison, generating the full joint distribution and marginalizing over it has complexity \\mathcal O(k^n) \\mathcal O(k^n) ! Tip See slide 7-13 for a full derivation of this example, including the runtime \\mathcal O(nk^2) \\mathcal O(nk^2) .","title":"Simple Example: Chain"},{"location":"lectures/week_5/#simple-example-dgm","text":"Lets take the DGM we saw in lecture 3 What is p(x_1 | \\bar x_6) p(x_1 | \\bar x_6) ? We have Note The \\bar x \\bar x means that the variable is observed. \\ X_F = \\{x_1\\}, X_E = \\{x_6\\}, \\ X_R = \\{x_2, x_3, x_4, x_5\\} \\ X_F = \\{x_1\\}, X_E = \\{x_6\\}, \\ X_R = \\{x_2, x_3, x_4, x_5\\} and \\begin{align} p(X_F | X_E) = \\frac{\\sum_{X_R} p(X_F, X_E, X_R)}{\\sum_{X_F, X_R} p(X_F, X_E, X_R)} \\\\ \\Rightarrow p(x_1 | \\bar x_6) = \\frac{p(x_1, \\bar x_6)}{p(\\bar x_6)} \\\\ = \\frac{p(x_1, \\bar x_6)}{\\sum_{x \\in {X_F, X_R}}p(x, \\bar x_6)} \\\\ \\end{align} \\begin{align} p(X_F | X_E) &= \\frac{\\sum_{X_R} p(X_F, X_E, X_R)}{\\sum_{X_F, X_R} p(X_F, X_E, X_R)} \\\\ \\Rightarrow p(x_1 | \\bar x_6) &= \\frac{p(x_1, \\bar x_6)}{p(\\bar x_6)} \\\\ &= \\frac{p(x_1, \\bar x_6)}{\\sum_{x \\in {X_F, X_R}}p(x, \\bar x_6)} \\\\ \\end{align} to compute p(x_1, \\bar x_6) p(x_1, \\bar x_6) , we use variable elimination p(x_1, \\bar x_6) = p(x_1) \\sum_{x_2} \\sum_{x_3} \\sum_{x_4} \\sum_{x_5} p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(\\bar x_6 | x_2, x_5)\\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) \\sum_{x_5} p(x_5 | x_3)p(\\bar x_6 | x_2, x_5) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\sum_{x_4} p(x_4 | x_2) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) p(\\bar x_6 | x_1, x_2) \\\\ = p(x_1) p(\\bar x_6 | x_1) \\\\ p(x_1, \\bar x_6) = p(x_1) \\sum_{x_2} \\sum_{x_3} \\sum_{x_4} \\sum_{x_5} p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(\\bar x_6 | x_2, x_5)\\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) \\sum_{x_5} p(x_5 | x_3)p(\\bar x_6 | x_2, x_5) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) \\sum_{x_4} p(x_4 | x_2) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\sum_{x_4} p(x_4 | x_2) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) \\sum_{x_3} p(x_3 | x_1) p(\\bar x_6 | x_2, x_3) \\\\ = p(x_1) \\sum_{x_2} p(x_2 | x_1) p(\\bar x_6 | x_1, x_2) \\\\ = p(x_1) p(\\bar x_6 | x_1) \\\\","title":"Simple Example: DGM"},{"location":"lectures/week_5/#summary-so-far","text":"A worst case analysis says that computing the joint distribution is NP-hard. In practice, some subexpressions in the joint depend only on a subset of variables due to the structure of the Bayesian network. We can therefore use dynamic programming to reduce the number of computations, however, this depends on choosing a good variable elimination ordering .","title":"Summary so far"},{"location":"lectures/week_5/#sum-product-inference","text":"We want an algorithm to compute P(Y) P(Y) for directed and undirected models. This can be reduced to the following sum-product inference task \\tau(Y) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, y_{Scope[\\psi] \\cap Y}) \\quad \\forall Y \\tau(Y) = \\sum_z \\prod_{\\psi \\in \\Psi} \\psi(z_{Scope[\\psi] \\cap Z}, y_{Scope[\\psi] \\cap Y}) \\quad \\forall Y where \\Psi \\Psi is a set of potentials or factors. For directed models , \\Psi \\Psi is given by the conditional probability distributions for all variables \\Psi = \\{\\psi_{x_i}\\}^N_{i=1} = \\{p(x_i | x_{\\pi_i})\\}^N_{i=1} \\Psi = \\{\\psi_{x_i}\\}^N_{i=1} = \\{p(x_i | x_{\\pi_i})\\}^N_{i=1} where the sum is over the set Z = X - X_F Z = X - X_F . For undirected models , \\Psi \\Psi is given by the set of potentials. Because the sum product returns unnormalized \\Phi \\Phi distributions, we must normalize by \\sum_Y\\tau(y) \\sum_Y\\tau(y) .","title":"Sum-product inference"},{"location":"lectures/week_5/#example-directed-graph","text":"Take the following directed graph as example The joint distribution is given by p(C, D, I, G, S, L, H, J) = p(C)p(D|C)p(I)p(G | D, I)p(L | G)P(S | I)p(J | S, L)p(H | J, G) p(C, D, I, G, S, L, H, J) = p(C)p(D|C)p(I)p(G | D, I)p(L | G)P(S | I)p(J | S, L)p(H | J, G) with factors \\Psi = \\{\\psi_C(C), \\psi_D(C, D), \\psi_I(I), \\psi_G(G, D, I), \\psi_L(L, G), \\psi_S(S, I), \\psi_J(J, S, L), \\psi_H(H, J, H)\\} \\Psi = \\{\\psi_C(C), \\psi_D(C, D), \\psi_I(I), \\psi_G(G, D, I), \\psi_L(L, G), \\psi_S(S, I), \\psi_J(J, S, L), \\psi_H(H, J, H)\\} Let's do variable elimination with ordering \\prec \\{C, D, I, H, G, S, L\\} \\prec \\{C, D, I, H, G, S, L\\}","title":"Example: Directed Graph"},{"location":"lectures/week_6/","text":"Choosing an Elimination Ordering To choose an elimination ordering, we use a set of heuristics: Min-fill : the cost of a vertex is the number of edges that need to be added to the graph due to its elimination. Weighted-Min-Fill : the cost of a vertex is the sum of weights of the edges that need to be added to the graph due to its elimination. Weight of an edge is the product of weights of its constituent vertices. Min-neighbors : the cost of a vertex is the number of neighbors it has in the current graph. Min-weight : the cost of a vertex is the product of weights (domain cardinality) of its neighbors. None of these criteria are better than the others. You often just have to try several. Error Skipped the section from lecture that came between Choosing an Elimination Ordering and Belief Propogation . Belief Propagation What if we want p(x_i) \\ \\forall x_i \\in X p(x_i) \\ \\forall x_i \\in X ? We could run variable elimination for each variable x_i x_i , but this is computationally expensive. Can we do something more efficient? Consider a tree: P(X_{1:n}) = \\frac{1}{z} \\prod \\phi(x_i) \\prod_{(i, j) \\in T} \\phi_{i, j}(x_i, x_j) P(X_{1:n}) = \\frac{1}{z} \\prod \\phi(x_i) \\prod_{(i, j) \\in T} \\phi_{i, j}(x_i, x_j) We can compute the sum product belief propagation in order to compute all marginals with just two passes. Belief propagation is based on message-passing of \"messages\" between neighboring vertices of the graph. The message sent from variable j j to i \\in N(j) i \\in N(j) is m_{j \\rightarrow i}(x_i) = \\sum_{x_j}\\phi_j(x_j)\\phi_{ij}(x_i, x_j)\\prod_{k \\in N(j) \\not = i} m_{k \\rightarrow j}(x_j) m_{j \\rightarrow i}(x_i) = \\sum_{x_j}\\phi_j(x_j)\\phi_{ij}(x_i, x_j)\\prod_{k \\in N(j) \\not = i} m_{k \\rightarrow j}(x_j) where each message m_{j \\rightarrow i}(x_i) m_{j \\rightarrow i}(x_i) is a vector with one value for each state of x_i x_i .","title":"Week 6"},{"location":"lectures/week_6/#choosing-an-elimination-ordering","text":"To choose an elimination ordering, we use a set of heuristics: Min-fill : the cost of a vertex is the number of edges that need to be added to the graph due to its elimination. Weighted-Min-Fill : the cost of a vertex is the sum of weights of the edges that need to be added to the graph due to its elimination. Weight of an edge is the product of weights of its constituent vertices. Min-neighbors : the cost of a vertex is the number of neighbors it has in the current graph. Min-weight : the cost of a vertex is the product of weights (domain cardinality) of its neighbors. None of these criteria are better than the others. You often just have to try several. Error Skipped the section from lecture that came between Choosing an Elimination Ordering and Belief Propogation .","title":"Choosing an Elimination Ordering"},{"location":"lectures/week_6/#belief-propagation","text":"What if we want p(x_i) \\ \\forall x_i \\in X p(x_i) \\ \\forall x_i \\in X ? We could run variable elimination for each variable x_i x_i , but this is computationally expensive. Can we do something more efficient? Consider a tree: P(X_{1:n}) = \\frac{1}{z} \\prod \\phi(x_i) \\prod_{(i, j) \\in T} \\phi_{i, j}(x_i, x_j) P(X_{1:n}) = \\frac{1}{z} \\prod \\phi(x_i) \\prod_{(i, j) \\in T} \\phi_{i, j}(x_i, x_j) We can compute the sum product belief propagation in order to compute all marginals with just two passes. Belief propagation is based on message-passing of \"messages\" between neighboring vertices of the graph. The message sent from variable j j to i \\in N(j) i \\in N(j) is m_{j \\rightarrow i}(x_i) = \\sum_{x_j}\\phi_j(x_j)\\phi_{ij}(x_i, x_j)\\prod_{k \\in N(j) \\not = i} m_{k \\rightarrow j}(x_j) m_{j \\rightarrow i}(x_i) = \\sum_{x_j}\\phi_j(x_j)\\phi_{ij}(x_i, x_j)\\prod_{k \\in N(j) \\not = i} m_{k \\rightarrow j}(x_j) where each message m_{j \\rightarrow i}(x_i) m_{j \\rightarrow i}(x_i) is a vector with one value for each state of x_i x_i .","title":"Belief Propagation"},{"location":"lectures/week_8/","text":"Week 8: Sampling and Monte Carlo Methods Assigned Reading MacKay: Chapter 29 Overview Simple Monte Carlo Importance Sampling Rejection Sampling Metropolis Hastings Gibbs Properties of Markov Chains We are going to put a pause on variational methods and return to them in a few weeks. Today we will talk about sampling : ways to compute the joint probability in a tractable way. Sampling We will use the word \" sample \" in the following sense: a sample from a distribution p(x) p(x) is a single realization x x whose probability distribution is p(x) p(x) . This contrasts with the alternative usage in statistics, where sample refers to a collection of realizations {x} {x} . Recall from last week that we assume the density from which we wish to draw samples, p(x) p(x) , can be evaluated to within a multiplicative constant. That is, we can evaluate a function \\tilde p(x) \\tilde p(x) such that p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} The problems to be solved Monte Carlo methods are computational techniques that make use of random numbers. The aims of Monte Carlo methods are to solve one or both of the following problems. Problem 1 : To generate samples \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} from a given probability distribution p(x) p(x) . Problem 2 : To estimate expectations of functions, \\phi(x) \\phi(x) , under this distribution, p(x) p(x) \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] = \\int \\phi(x)p(x)dx \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] = \\int \\phi(x)p(x)dx Warning \\phi \\phi is unrelated to the factors we talked about in week 6; here it is just some function we want to compute the expectation of. Simple example Simple examples of functions \\phi(x) \\phi(x) whose expectations we might be interested in include the first and second moments of quantities that we wish to predict, from which we can compute means and variances; for example if some quantity t t depends on x x , we can find the mean and variance of t t under p(x) p(x) by finding the expectations of the functions \\phi_1(x) = t(x) \\phi_1(x) = t(x) and \\phi_2(x) = (t(x))^2 \\phi_2(x) = (t(x))^2 \\phi_1(x) = t(x) \\Rightarrow \\Phi_1 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_1(x)] \\Rightarrow \\text{mean}(t) = \\Phi_1 \\\\ \\phi_2(x) = (t(x))^2 \\Rightarrow \\Phi_2 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_2(x)] \\Rightarrow \\text{var}(t) = \\Phi_2 - (\\Phi_1)^2 \\\\ \\phi_1(x) = t(x) \\Rightarrow \\Phi_1 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_1(x)] \\Rightarrow \\text{mean}(t) = \\Phi_1 \\\\ \\phi_2(x) = (t(x))^2 \\Rightarrow \\Phi_2 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_2(x)] \\Rightarrow \\text{var}(t) = \\Phi_2 - (\\Phi_1)^2 \\\\ Simple Monte Carlo We will concentrate on the first problem (sampling), because if we have solved it, then we can solve the second problem by using the random samples \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} to give an estimator. This brings us to simple Monte Carlo : def . Simple Monte Carlo : Given \\{x^{(r)}\\}^R_{r=1} \\sim p(x) \\{x^{(r)}\\}^R_{r=1} \\sim p(x) we estimate the expectation \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] to be the estimator \\hat \\Phi \\hat \\Phi \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi Properties of MC If the vectors \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} are generated from p(x) p(x) then the expectation of \\hat \\Phi \\hat \\Phi is \\Phi \\Phi . E.g. \\hat \\Phi \\hat \\Phi is an unbiased estimator of \\Phi \\Phi . Proof \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square As the number of samples of R R increases, the variance of \\hat \\Phi \\hat \\Phi will decrease proportional to \\frac{1}{R} \\frac{1}{R} Proof \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square Important The accuracy of the Monte Carlo estimate depends only on the variance of \\phi \\phi , not on the dimensionality of the x x . So regardless of the dimensionality of x x , it may be that as few as a dozen independent samples \\{x^{(r)}\\} \\{x^{(r)}\\} suffice to estimate \\Phi \\Phi satisfactorily. Sampling p(x) is hard Earlier we said that we will assume we can sample from the density p(x) p(x) to within a multiplicative constant p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} If we can evaluate \\tilde p(x) \\tilde p(x) , then why can't we solve problem 1? There are two difficulties We do not typically know the normalizing constant, Z Z Even if we did know Z Z , the problem of drawing samples from p(x) p(x) is still a challenging one, especially in high-dimensional spaces, because there is no obvious way to sample from p p without enumerating most or all of the possible states. Note Correct samples from p p will by definition tend to come from places in x x -space where p(x) p(x) is big; how can we identify those places where p(x) p(x) is big, without evaluating p(x) p(x) everywhere? There are only a few high-dimensional densities from which it is easy to draw samples, for example the Gaussian distribution. First, we will build our intuition for why sampling from p p is difficult. In the next section, will discuss clever Monte Carlo methods can solve them. Bad Idea: Lattice Discretization Imagine that we wish to draw samples from the density p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} given in figure (a). Just because we can plot this distribution, that does not mean we can draw samples from it. To start, we don't know the normalizing constant, Z Z . Warning Note the some of these images come from MacKay, who uses a different notation. We use lowercase p p and q q in place of his upper case P P and Q Q , and we use \\tilde p \\tilde p / \\tilde q \\tilde q in place of his p^* p^* / q^* q^* . To simplify the problem, we could discretize the variable x x and sample from the discrete probability distribution over a finite set of uniformly spaced points \\{x_i\\} \\{x_i\\} (figure (d)). If we evaluate \\tilde p(x_i) \\tilde p(x_i) at each point x_i x_i , we could compute Z = \\sum_i \\tilde p_i(x_i) Z = \\sum_i \\tilde p_i(x_i) and p_i = \\frac{\\tilde p_i}{Z} p_i = \\frac{\\tilde p_i}{Z} and could sample from the probability distribution \\{p_i\\}_{i=1}^R \\{p_i\\}_{i=1}^R using various methods based on a source of random bits. Unfortunately, the cost of this procedure is intractable. To evaluate Z Z , we must visit every point in the space. In figure (b) there are 50 50 uniformly spaced points in one dimension. If our system had, N=1000 N=1000 dimensions say, then the corresponding number of points would be 50^{N} = 50^{1000} 50^{N} = 50^{1000} . Even if each component x_n x_n took only two discrete values, the number of evaluations of \\tilde p \\tilde p needed to evaluate Z Z would take many times longer than the age of the universe. Tip TL;DR The cost of this lattice discretization method of sampling from p(x) p(x) is exponential in the dimension of our data (e.g. D^N D^N where D D is the number of data points and N N their dimension). A useful analogy Imagine the tasks of drawing random water samples from a lake and finding the average plankton concentration (figure 29.2). Let \\tilde p({\\bf x}) \\tilde p({\\bf x}) = the depth of the lake at {\\bf x} = (x, y) {\\bf x} = (x, y) \\phi({\\bf x}) \\phi({\\bf x}) = the plankton concentration as a function of {\\bf x} {\\bf x} Z Z = the volume of the lake = \\int \\tilde p({\\bf x }) d{\\bf x } \\int \\tilde p({\\bf x }) d{\\bf x } The average concentration of plankton is therefore \\Phi = \\frac{1}{Z} \\int \\phi({\\bf x }) \\tilde p({\\bf x }) d{\\bf x } \\Phi = \\frac{1}{Z} \\int \\phi({\\bf x }) \\tilde p({\\bf x }) d{\\bf x } Say you can take the boat to any desired location {\\bf x } {\\bf x } on the lake, and can measure the depth, \\tilde p({\\bf x }) \\tilde p({\\bf x }) , and plankton concentration, \\phi({\\bf x}) \\phi({\\bf x}) , at that point. Therefore, Problem 1 is to draw water samples at random from the lake, in such a way that each sample is equally likely to come from any point within the lake. Problem 2 is to find the average plankton concentration. These are difficult problems to solve because at the outset we know nothing about the depth \\tilde p({\\bf x }) \\tilde p({\\bf x }) . Perhaps much of the volume of the lake is contained in narrow, deep underwater canyons (figure 29.3), in which case, to correctly sample from the lake and correctly estimate \\Phi \\Phi our method must implicitly discover the canyons and find their volume relative to the rest of the lake. Monte Carlo Methods Importance Sampling Importance sampling is not a method for generating samples from p(x) p(x) ( Problem 1 ); it is just a method for estimating the expectation of a function \\phi(x) \\phi(x) ( Problem 2 ). Tip Importance sampling can be viewed as a generalization of the uniform sampling method, something we did not discuss in class but is discussed in MacKay: Chapter 29. We begin with the same assumption we have made earlier; the density from which we wish to draw samples, p(x) p(x) , can be evaluated to within a multiplicative constant. That is, we can evaluate a function \\tilde p(x) \\tilde p(x) such that p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} We further assume we have a simpler density, q(x) q(x) from which it is easy to sample from (i.e. x \\sim q(x) x \\sim q(x) ) and easy to evaluate (i.e. \\tilde q(x) \\tilde q(x) ) q(x) = \\frac{\\tilde q(x)}{Z_q} q(x) = \\frac{\\tilde q(x)}{Z_q} we call such a density q(x) q(x) the sampler density . An example of the functions \\tilde p, \\tilde q \\tilde p, \\tilde q and \\phi \\phi is given in in figure 29.5. In importance sampling, we generate R R samples from q(x) q(x) \\{x^{(r)}\\}^R_{r=1} \\sim q(x) \\{x^{(r)}\\}^R_{r=1} \\sim q(x) If these points were samples from p(x) p(x) then we could estimate \\Phi \\Phi by \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi That is, we could use a simple Monte Carlo estimator . But when we generate samples from q q , values of x x where q(x) q(x) is greater than p(x) p(x) will be over-represented in this estimator, and points where q(x) q(x) is less than p(x) p(x) will be under-represented . To take into account the fact that we have sampled from the wrong distribution, we introduce weights . \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} Finally, we rewrite our estimator under q q \\Phi = \\int \\phi(x)p(x)dx\\\\ = \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} \\Phi = \\int \\phi(x)p(x)dx\\\\ = \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} however, the estimator as written still relies on p(x) p(x) , we want an estimator that relies on \\tilde p(x) \\tilde p(x) \\begin{align} = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ = \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ = \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ = \\hat \\Phi_{iw} \\end{align} \\begin{align} &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ &= \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ &= \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ &= \\hat \\Phi_{iw} \\end{align} Warning I don't know where the q(x) q(x) went from line 1 to 2? where \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r , w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} and \\hat \\Phi_{iw} \\hat \\Phi_{iw} is our importance weighted estimator. Note \\hat \\Phi_{iw} \\hat \\Phi_{iw} is biased, by consistent. Rejection Sampling In rejection sampling we assume again a one-dimensional density p(x) = \\tilde p(x)/Z p(x) = \\tilde p(x)/Z that is too complicated a function for us to be able to sample from it directly. We assume that we have a simpler proposal density q(x) q(x) which we can evaluate (within a multiplicative factor Z_q Z_q , as before), and from which we can generate samples. We further assume that we know the value of a constant c c such that c \\tilde q(x) \\tilde p(x) \\quad \\forall x c \\tilde q(x) > \\tilde p(x) \\quad \\forall x A schematic picture of two such functions is shown below The procedure is as follows: Generate two random numbers. The first, x x , is generated from the proposal density q(x) q(x) . The second, u u is generated uniformly from the interval [0, c \\tilde q(x)] [0, c \\tilde q(x)] (see figure (b) above). Evaluate \\tilde p(x) \\tilde p(x) and accept or reject the sample x x by comparing the value of u u with the value of \\tilde p(x) \\tilde p(x) If u \\tilde p(x) u > \\tilde p(x) , then x x is rejected Otherwise x x is accepted; x x is added to our set of samples \\{x^{(r)}\\} \\{x^{(r)}\\} and the value of u u discarded. Why does this procedure generate samples from p(x) p(x) ? The proposed point (x, u) (x, u) comes with uniform probability from the lightly shaded area underneath the curve c \\tilde q(x) c \\tilde q(x) as shown in figure (b) above. The rejection rule rejects all the points that lie above the curve \\tilde p(x) \\tilde p(x) . So the points (x,u) (x,u) that are accepted are uniformly distributed in the heavily shaded area under \\tilde p(x) \\tilde p(x) . This implies that the probability density of the x-coordinates of the accepted points must be proportional to \\tilde p(x) \\tilde p(x) , so the samples must be independent samples from p(x) p(x) . Note Rejection sampling will work best if q q is a good approximation to p p . If q q is very different from p p then, for cq cq to exceed p p everywhere, c c will necessarily have to be large and the frequency of rejection will be large. Rejection sampling in many dimensions In a high-dimensional problem it is very likely that the requirement that c \\tilde q c \\tilde q be an upper bound for \\tilde p \\tilde p will force c c to be so huge that acceptances will be very rare indeed. Finding such a value of c c may be difficult too, since in many problems we know neither where the modes of \\tilde p \\tilde p are located nor how high they are. In general c c grows exponentially with the dimensionality N N , so the acceptance rate is expected to be exponentially small in N N \\text{acceptance rate} = \\frac{\\text{area under } \\tilde p}{\\text{area under } c\\tilde q} = \\frac{1}{Z} \\text{acceptance rate} = \\frac{\\text{area under } \\tilde p}{\\text{area under } c\\tilde q} = \\frac{1}{Z} Metropolis-Hastings method Importance sampling and rejection sampling work well only if the proposal density q(x) q(x) is similar to p(x) p(x) . In high dimensions, it is hard to find one such q q . The Metropolis\u2013Hastings algorithm instead makes use of a proposal density q q which depends on the current state x^{(t)} x^{(t)} . The density q(x' | x^{(t)}) q(x' | x^{(t)}) might be a simple distribution such as a Gaussian centered on the current x^{(t)} x^{(t)} , but in general can be any fixed density from which we can draw samples. In contrast to importance sampling and rejection sampling, it is not necessary q(x' | x^{(t)}) q(x' | x^{(t)}) look at all similar to p(x) p(x) in order for the algorithm to be practically useful. An example of a proposal density with two different states ( x^{(1)}, x^{(2)} x^{(1)}, x^{(2)} ) is shown in figure 29.10. As before, we assume we can evaluate \\tilde p(x) \\tilde p(x) for any x x . The procedure is as follows: A tentative new state x' x' is generated from the proposal density q(x' | x^{(t)}) q(x' | x^{(t)}) . To decide whether to accept the new state, we compute a = \\frac{\\tilde p(x')q(x^{(t)} | x')}{\\tilde p(x^{(t)}) q(x' | x^{(t)})} a = \\frac{\\tilde p(x')q(x^{(t)} | x')}{\\tilde p(x^{(t)}) q(x' | x^{(t)})} If a \\ge 1 a \\ge 1 then the new state is accepted. Otherwise, the new state is accepted with probability a a . If accepted, set x^{(t + 1)} = x' x^{(t + 1)} = x' . Otherwise, set x^{(t + 1)} = x^{(t)} x^{(t + 1)} = x^{(t)} . Note Note the difference from rejection sampling: in rejection sampling, rejected points are discarded and have no influence on the list of samples \\{x^{(r)}\\} \\{x^{(r)}\\} that we collected. Here, a rejection causes the current state to be written again onto the list. Metropolis\u2013Hastings converges to p(x) p(x) for any q(x' | x^{(t)}) \\ge 0 \\quad \\forall x', x^{(t)} q(x' | x^{(t)}) \\ge 0 \\quad \\forall x', x^{(t)} as t \\rightarrow \\infty t \\rightarrow \\infty . That is, our list of samples converges towards the true distribution \\{x^{(r)}\\}_{r=1}^R \\rightarrow p(x) \\{x^{(r)}\\}_{r=1}^R \\rightarrow p(x) . There are however, no guarantees on convergence. The Metropolis method is an example of a Markov chain Monte Carlo method (abbreviated MCMC). In contrast to rejection sampling, where the accepted points \\{x^{(t)}\\} \\{x^{(t)}\\} are independent samples from the desired distribution, Markov chain Monte Carlo methods involve a Markov process in which a sequence of states \\{x^{(t)}\\} \\{x^{(t)}\\} is generated, each sample x^{(t)} x^{(t)} having a probability distribution that depends on the previous value, x^{(t-1)} x^{(t-1)} . Since successive samples are dependent, the Markov chain may have to be run for a considerable time in order to generate samples that are effectively independent samples from p p . Just as it was difficult to estimate the variance of an importance sampling estimator, so it is difficult to assess whether a Markov chain Monte Carlo method has \u2018converged\u2019, and to quantify how long one has to wait to obtain samples that are effectively independent samples from p p . Appendix Useful Resources Interactive Metropolis Hastings Online Demo Glossary of Terms","title":"Week 8"},{"location":"lectures/week_8/#week-8-sampling-and-monte-carlo-methods","text":"","title":"Week 8: Sampling and Monte Carlo Methods"},{"location":"lectures/week_8/#assigned-reading","text":"MacKay: Chapter 29","title":"Assigned Reading"},{"location":"lectures/week_8/#overview","text":"Simple Monte Carlo Importance Sampling Rejection Sampling Metropolis Hastings Gibbs Properties of Markov Chains We are going to put a pause on variational methods and return to them in a few weeks. Today we will talk about sampling : ways to compute the joint probability in a tractable way.","title":"Overview"},{"location":"lectures/week_8/#sampling","text":"We will use the word \" sample \" in the following sense: a sample from a distribution p(x) p(x) is a single realization x x whose probability distribution is p(x) p(x) . This contrasts with the alternative usage in statistics, where sample refers to a collection of realizations {x} {x} . Recall from last week that we assume the density from which we wish to draw samples, p(x) p(x) , can be evaluated to within a multiplicative constant. That is, we can evaluate a function \\tilde p(x) \\tilde p(x) such that p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z}","title":"Sampling"},{"location":"lectures/week_8/#the-problems-to-be-solved","text":"Monte Carlo methods are computational techniques that make use of random numbers. The aims of Monte Carlo methods are to solve one or both of the following problems. Problem 1 : To generate samples \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} from a given probability distribution p(x) p(x) . Problem 2 : To estimate expectations of functions, \\phi(x) \\phi(x) , under this distribution, p(x) p(x) \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] = \\int \\phi(x)p(x)dx \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] = \\int \\phi(x)p(x)dx Warning \\phi \\phi is unrelated to the factors we talked about in week 6; here it is just some function we want to compute the expectation of.","title":"The problems to be solved"},{"location":"lectures/week_8/#simple-example","text":"Simple examples of functions \\phi(x) \\phi(x) whose expectations we might be interested in include the first and second moments of quantities that we wish to predict, from which we can compute means and variances; for example if some quantity t t depends on x x , we can find the mean and variance of t t under p(x) p(x) by finding the expectations of the functions \\phi_1(x) = t(x) \\phi_1(x) = t(x) and \\phi_2(x) = (t(x))^2 \\phi_2(x) = (t(x))^2 \\phi_1(x) = t(x) \\Rightarrow \\Phi_1 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_1(x)] \\Rightarrow \\text{mean}(t) = \\Phi_1 \\\\ \\phi_2(x) = (t(x))^2 \\Rightarrow \\Phi_2 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_2(x)] \\Rightarrow \\text{var}(t) = \\Phi_2 - (\\Phi_1)^2 \\\\ \\phi_1(x) = t(x) \\Rightarrow \\Phi_1 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_1(x)] \\Rightarrow \\text{mean}(t) = \\Phi_1 \\\\ \\phi_2(x) = (t(x))^2 \\Rightarrow \\Phi_2 = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi_2(x)] \\Rightarrow \\text{var}(t) = \\Phi_2 - (\\Phi_1)^2 \\\\","title":"Simple example"},{"location":"lectures/week_8/#simple-monte-carlo","text":"We will concentrate on the first problem (sampling), because if we have solved it, then we can solve the second problem by using the random samples \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} to give an estimator. This brings us to simple Monte Carlo : def . Simple Monte Carlo : Given \\{x^{(r)}\\}^R_{r=1} \\sim p(x) \\{x^{(r)}\\}^R_{r=1} \\sim p(x) we estimate the expectation \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] to be the estimator \\hat \\Phi \\hat \\Phi \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi","title":"Simple Monte Carlo"},{"location":"lectures/week_8/#properties-of-mc","text":"If the vectors \\{x^{(r)}\\}^R_{r=1} \\{x^{(r)}\\}^R_{r=1} are generated from p(x) p(x) then the expectation of \\hat \\Phi \\hat \\Phi is \\Phi \\Phi . E.g. \\hat \\Phi \\hat \\Phi is an unbiased estimator of \\Phi \\Phi . Proof \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square \\mathbb E [\\hat \\Phi]_{x \\sim p(\\{x^{(r)}\\}^R_{r=1})} = \\mathbb E \\bigg [ \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\mathbb E \\big [ \\phi(x^{(r)}) \\big ] \\\\ = \\frac{1}{R} \\sum_{r=1}^R \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\frac{R}{R} \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} \\big [ \\phi(x) \\big ] \\\\ = \\Phi \\quad \\square As the number of samples of R R increases, the variance of \\hat \\Phi \\hat \\Phi will decrease proportional to \\frac{1}{R} \\frac{1}{R} Proof \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square \\text{var}[\\hat \\Phi] = \\text{var} \\bigg [ \\frac{1}{R}\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{1}{R^2} \\text{var} \\bigg [\\sum^R_{r=1}\\phi(x^{(r)}) \\bigg ]\\\\ = \\frac{1}{R^2} \\sum^R_{r=1} \\text{var} \\bigg [\\phi(x^{(r)}) \\bigg ] \\\\ = \\frac{R}{R^2} \\text{var} [\\phi(x) ] \\\\ = \\frac{1}{R} \\text{var} [\\phi(x) ] \\quad \\square Important The accuracy of the Monte Carlo estimate depends only on the variance of \\phi \\phi , not on the dimensionality of the x x . So regardless of the dimensionality of x x , it may be that as few as a dozen independent samples \\{x^{(r)}\\} \\{x^{(r)}\\} suffice to estimate \\Phi \\Phi satisfactorily.","title":"Properties of MC"},{"location":"lectures/week_8/#sampling-px-is-hard","text":"Earlier we said that we will assume we can sample from the density p(x) p(x) to within a multiplicative constant p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} If we can evaluate \\tilde p(x) \\tilde p(x) , then why can't we solve problem 1? There are two difficulties We do not typically know the normalizing constant, Z Z Even if we did know Z Z , the problem of drawing samples from p(x) p(x) is still a challenging one, especially in high-dimensional spaces, because there is no obvious way to sample from p p without enumerating most or all of the possible states. Note Correct samples from p p will by definition tend to come from places in x x -space where p(x) p(x) is big; how can we identify those places where p(x) p(x) is big, without evaluating p(x) p(x) everywhere? There are only a few high-dimensional densities from which it is easy to draw samples, for example the Gaussian distribution. First, we will build our intuition for why sampling from p p is difficult. In the next section, will discuss clever Monte Carlo methods can solve them.","title":"Sampling p(x) is hard"},{"location":"lectures/week_8/#bad-idea-lattice-discretization","text":"Imagine that we wish to draw samples from the density p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} given in figure (a). Just because we can plot this distribution, that does not mean we can draw samples from it. To start, we don't know the normalizing constant, Z Z . Warning Note the some of these images come from MacKay, who uses a different notation. We use lowercase p p and q q in place of his upper case P P and Q Q , and we use \\tilde p \\tilde p / \\tilde q \\tilde q in place of his p^* p^* / q^* q^* . To simplify the problem, we could discretize the variable x x and sample from the discrete probability distribution over a finite set of uniformly spaced points \\{x_i\\} \\{x_i\\} (figure (d)). If we evaluate \\tilde p(x_i) \\tilde p(x_i) at each point x_i x_i , we could compute Z = \\sum_i \\tilde p_i(x_i) Z = \\sum_i \\tilde p_i(x_i) and p_i = \\frac{\\tilde p_i}{Z} p_i = \\frac{\\tilde p_i}{Z} and could sample from the probability distribution \\{p_i\\}_{i=1}^R \\{p_i\\}_{i=1}^R using various methods based on a source of random bits. Unfortunately, the cost of this procedure is intractable. To evaluate Z Z , we must visit every point in the space. In figure (b) there are 50 50 uniformly spaced points in one dimension. If our system had, N=1000 N=1000 dimensions say, then the corresponding number of points would be 50^{N} = 50^{1000} 50^{N} = 50^{1000} . Even if each component x_n x_n took only two discrete values, the number of evaluations of \\tilde p \\tilde p needed to evaluate Z Z would take many times longer than the age of the universe. Tip TL;DR The cost of this lattice discretization method of sampling from p(x) p(x) is exponential in the dimension of our data (e.g. D^N D^N where D D is the number of data points and N N their dimension).","title":"Bad Idea: Lattice Discretization"},{"location":"lectures/week_8/#a-useful-analogy","text":"Imagine the tasks of drawing random water samples from a lake and finding the average plankton concentration (figure 29.2). Let \\tilde p({\\bf x}) \\tilde p({\\bf x}) = the depth of the lake at {\\bf x} = (x, y) {\\bf x} = (x, y) \\phi({\\bf x}) \\phi({\\bf x}) = the plankton concentration as a function of {\\bf x} {\\bf x} Z Z = the volume of the lake = \\int \\tilde p({\\bf x }) d{\\bf x } \\int \\tilde p({\\bf x }) d{\\bf x } The average concentration of plankton is therefore \\Phi = \\frac{1}{Z} \\int \\phi({\\bf x }) \\tilde p({\\bf x }) d{\\bf x } \\Phi = \\frac{1}{Z} \\int \\phi({\\bf x }) \\tilde p({\\bf x }) d{\\bf x } Say you can take the boat to any desired location {\\bf x } {\\bf x } on the lake, and can measure the depth, \\tilde p({\\bf x }) \\tilde p({\\bf x }) , and plankton concentration, \\phi({\\bf x}) \\phi({\\bf x}) , at that point. Therefore, Problem 1 is to draw water samples at random from the lake, in such a way that each sample is equally likely to come from any point within the lake. Problem 2 is to find the average plankton concentration. These are difficult problems to solve because at the outset we know nothing about the depth \\tilde p({\\bf x }) \\tilde p({\\bf x }) . Perhaps much of the volume of the lake is contained in narrow, deep underwater canyons (figure 29.3), in which case, to correctly sample from the lake and correctly estimate \\Phi \\Phi our method must implicitly discover the canyons and find their volume relative to the rest of the lake.","title":"A useful analogy"},{"location":"lectures/week_8/#monte-carlo-methods","text":"","title":"Monte Carlo Methods"},{"location":"lectures/week_8/#importance-sampling","text":"Importance sampling is not a method for generating samples from p(x) p(x) ( Problem 1 ); it is just a method for estimating the expectation of a function \\phi(x) \\phi(x) ( Problem 2 ). Tip Importance sampling can be viewed as a generalization of the uniform sampling method, something we did not discuss in class but is discussed in MacKay: Chapter 29. We begin with the same assumption we have made earlier; the density from which we wish to draw samples, p(x) p(x) , can be evaluated to within a multiplicative constant. That is, we can evaluate a function \\tilde p(x) \\tilde p(x) such that p(x) = \\frac{\\tilde p(x)}{Z} p(x) = \\frac{\\tilde p(x)}{Z} We further assume we have a simpler density, q(x) q(x) from which it is easy to sample from (i.e. x \\sim q(x) x \\sim q(x) ) and easy to evaluate (i.e. \\tilde q(x) \\tilde q(x) ) q(x) = \\frac{\\tilde q(x)}{Z_q} q(x) = \\frac{\\tilde q(x)}{Z_q} we call such a density q(x) q(x) the sampler density . An example of the functions \\tilde p, \\tilde q \\tilde p, \\tilde q and \\phi \\phi is given in in figure 29.5. In importance sampling, we generate R R samples from q(x) q(x) \\{x^{(r)}\\}^R_{r=1} \\sim q(x) \\{x^{(r)}\\}^R_{r=1} \\sim q(x) If these points were samples from p(x) p(x) then we could estimate \\Phi \\Phi by \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi \\Phi = \\underset{x \\sim p(x)}{\\operatorname{\\mathbb E}} [\\phi(x)] \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) = \\hat \\Phi That is, we could use a simple Monte Carlo estimator . But when we generate samples from q q , values of x x where q(x) q(x) is greater than p(x) p(x) will be over-represented in this estimator, and points where q(x) q(x) is less than p(x) p(x) will be under-represented . To take into account the fact that we have sampled from the wrong distribution, we introduce weights . \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\tilde w_r = \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} Finally, we rewrite our estimator under q q \\Phi = \\int \\phi(x)p(x)dx\\\\ = \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} \\Phi = \\int \\phi(x)p(x)dx\\\\ = \\int \\phi(x) \\cdot \\frac{p(x)}{q(x)} \\cdot q(x)dx \\\\ \\approx \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)})\\frac{p(x^{(r)})}{q(x^{(r)})} however, the estimator as written still relies on p(x) p(x) , we want an estimator that relies on \\tilde p(x) \\tilde p(x) \\begin{align} = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ = \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ = \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ = \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ = \\hat \\Phi_{iw} \\end{align} \\begin{align} &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\frac{\\tilde p(x^{(r)})}{\\tilde q(x^{(r)})} \\\\ &= \\frac{Z_q}{Z_p} \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r \\\\ &= \\frac{\\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot \\tilde w_r}{\\frac{1}{R}\\sum_{r=1}^R \\tilde w_r} \\\\ &= \\frac{1}{R}\\sum_{r=1}^R \\phi(x^{(r)}) \\cdot w_r \\\\ &= \\hat \\Phi_{iw} \\end{align} Warning I don't know where the q(x) q(x) went from line 1 to 2? where \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r \\frac{Z_p}{Z_q} = \\frac{1}{R}\\sum_{r=1}^R \\tilde w_r , w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} w_r = \\frac{\\tilde w_r}{\\sum_{r=1}^R \\tilde w_r} and \\hat \\Phi_{iw} \\hat \\Phi_{iw} is our importance weighted estimator. Note \\hat \\Phi_{iw} \\hat \\Phi_{iw} is biased, by consistent.","title":"Importance Sampling"},{"location":"lectures/week_8/#rejection-sampling","text":"In rejection sampling we assume again a one-dimensional density p(x) = \\tilde p(x)/Z p(x) = \\tilde p(x)/Z that is too complicated a function for us to be able to sample from it directly. We assume that we have a simpler proposal density q(x) q(x) which we can evaluate (within a multiplicative factor Z_q Z_q , as before), and from which we can generate samples. We further assume that we know the value of a constant c c such that c \\tilde q(x) \\tilde p(x) \\quad \\forall x c \\tilde q(x) > \\tilde p(x) \\quad \\forall x A schematic picture of two such functions is shown below The procedure is as follows: Generate two random numbers. The first, x x , is generated from the proposal density q(x) q(x) . The second, u u is generated uniformly from the interval [0, c \\tilde q(x)] [0, c \\tilde q(x)] (see figure (b) above). Evaluate \\tilde p(x) \\tilde p(x) and accept or reject the sample x x by comparing the value of u u with the value of \\tilde p(x) \\tilde p(x) If u \\tilde p(x) u > \\tilde p(x) , then x x is rejected Otherwise x x is accepted; x x is added to our set of samples \\{x^{(r)}\\} \\{x^{(r)}\\} and the value of u u discarded. Why does this procedure generate samples from p(x) p(x) ? The proposed point (x, u) (x, u) comes with uniform probability from the lightly shaded area underneath the curve c \\tilde q(x) c \\tilde q(x) as shown in figure (b) above. The rejection rule rejects all the points that lie above the curve \\tilde p(x) \\tilde p(x) . So the points (x,u) (x,u) that are accepted are uniformly distributed in the heavily shaded area under \\tilde p(x) \\tilde p(x) . This implies that the probability density of the x-coordinates of the accepted points must be proportional to \\tilde p(x) \\tilde p(x) , so the samples must be independent samples from p(x) p(x) . Note Rejection sampling will work best if q q is a good approximation to p p . If q q is very different from p p then, for cq cq to exceed p p everywhere, c c will necessarily have to be large and the frequency of rejection will be large.","title":"Rejection Sampling"},{"location":"lectures/week_8/#rejection-sampling-in-many-dimensions","text":"In a high-dimensional problem it is very likely that the requirement that c \\tilde q c \\tilde q be an upper bound for \\tilde p \\tilde p will force c c to be so huge that acceptances will be very rare indeed. Finding such a value of c c may be difficult too, since in many problems we know neither where the modes of \\tilde p \\tilde p are located nor how high they are. In general c c grows exponentially with the dimensionality N N , so the acceptance rate is expected to be exponentially small in N N \\text{acceptance rate} = \\frac{\\text{area under } \\tilde p}{\\text{area under } c\\tilde q} = \\frac{1}{Z} \\text{acceptance rate} = \\frac{\\text{area under } \\tilde p}{\\text{area under } c\\tilde q} = \\frac{1}{Z}","title":"Rejection sampling in many dimensions"},{"location":"lectures/week_8/#metropolis-hastings-method","text":"Importance sampling and rejection sampling work well only if the proposal density q(x) q(x) is similar to p(x) p(x) . In high dimensions, it is hard to find one such q q . The Metropolis\u2013Hastings algorithm instead makes use of a proposal density q q which depends on the current state x^{(t)} x^{(t)} . The density q(x' | x^{(t)}) q(x' | x^{(t)}) might be a simple distribution such as a Gaussian centered on the current x^{(t)} x^{(t)} , but in general can be any fixed density from which we can draw samples. In contrast to importance sampling and rejection sampling, it is not necessary q(x' | x^{(t)}) q(x' | x^{(t)}) look at all similar to p(x) p(x) in order for the algorithm to be practically useful. An example of a proposal density with two different states ( x^{(1)}, x^{(2)} x^{(1)}, x^{(2)} ) is shown in figure 29.10. As before, we assume we can evaluate \\tilde p(x) \\tilde p(x) for any x x . The procedure is as follows: A tentative new state x' x' is generated from the proposal density q(x' | x^{(t)}) q(x' | x^{(t)}) . To decide whether to accept the new state, we compute a = \\frac{\\tilde p(x')q(x^{(t)} | x')}{\\tilde p(x^{(t)}) q(x' | x^{(t)})} a = \\frac{\\tilde p(x')q(x^{(t)} | x')}{\\tilde p(x^{(t)}) q(x' | x^{(t)})} If a \\ge 1 a \\ge 1 then the new state is accepted. Otherwise, the new state is accepted with probability a a . If accepted, set x^{(t + 1)} = x' x^{(t + 1)} = x' . Otherwise, set x^{(t + 1)} = x^{(t)} x^{(t + 1)} = x^{(t)} . Note Note the difference from rejection sampling: in rejection sampling, rejected points are discarded and have no influence on the list of samples \\{x^{(r)}\\} \\{x^{(r)}\\} that we collected. Here, a rejection causes the current state to be written again onto the list. Metropolis\u2013Hastings converges to p(x) p(x) for any q(x' | x^{(t)}) \\ge 0 \\quad \\forall x', x^{(t)} q(x' | x^{(t)}) \\ge 0 \\quad \\forall x', x^{(t)} as t \\rightarrow \\infty t \\rightarrow \\infty . That is, our list of samples converges towards the true distribution \\{x^{(r)}\\}_{r=1}^R \\rightarrow p(x) \\{x^{(r)}\\}_{r=1}^R \\rightarrow p(x) . There are however, no guarantees on convergence. The Metropolis method is an example of a Markov chain Monte Carlo method (abbreviated MCMC). In contrast to rejection sampling, where the accepted points \\{x^{(t)}\\} \\{x^{(t)}\\} are independent samples from the desired distribution, Markov chain Monte Carlo methods involve a Markov process in which a sequence of states \\{x^{(t)}\\} \\{x^{(t)}\\} is generated, each sample x^{(t)} x^{(t)} having a probability distribution that depends on the previous value, x^{(t-1)} x^{(t-1)} . Since successive samples are dependent, the Markov chain may have to be run for a considerable time in order to generate samples that are effectively independent samples from p p . Just as it was difficult to estimate the variance of an importance sampling estimator, so it is difficult to assess whether a Markov chain Monte Carlo method has \u2018converged\u2019, and to quantify how long one has to wait to obtain samples that are effectively independent samples from p p .","title":"Metropolis-Hastings method"},{"location":"lectures/week_8/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_8/#useful-resources","text":"Interactive Metropolis Hastings Online Demo","title":"Useful Resources"},{"location":"lectures/week_8/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_9/","text":"Week 9: Hidden Markov Models Assigned Reading Murphy: Chapter 18 Overview Markov Chains Hidden Markov Models Forward / Backward Algorithm Viterbi Algorithm Sequential data Let us turn our attention to sequential data x_{1:T} = \\{x_1, ..., x_T\\} x_{1:T} = \\{x_1, ..., x_T\\} Broadly speaking, sequential data can be time-series (e.g. stock prices, speech, video analysis) or ordered (e.g. textual data, gene sequences). So far, we have assumed our data to be i.i.d, however this is a poor assumption for sequential data, as there is often clear dependencies between data points. Recall the general joint factorization via the chain rule p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1) p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1) But this quickly becomes intractable for high-dimensional data, which is why we made the i.i.d in the first place. We want temporal dependence without the intractability of computing the chain rule, so we make the simplifying assumption that our data can be modeled as a first-order Markov chain p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}) p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}) Tip In plain english: each observation is assumed to be independent of all previous observations except most recent. This assumption greatly simplifies our joint distribution p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}) p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}) Aside Last week we talked about the Markov chain Monte carlo sampling method Metropolis-Hastings. It is called this because the way you draw samples is a Markov chain. A useful distinction to make at this point is between stationary and non-stationary distributions that generate our data Stationary distribution : the distribution generating the data does not change through time Non-stationary distribution : the distribution generating the data is a function of time We are only going to consider the case of a stationary distribution (because it greatly simplifies the math). In this case, we use the same distribution at every timestep, sometimes called a homogenous Markov chain . Therefore, p(x_t | x_{t-1}) \\quad \\forall t p(x_t | x_{t-1}) \\quad \\forall t notice how the process generating the data is independent of t t . Generalizing to high-order chains The first-order assumption is still very restrictive. There are many examples where this would be a poor modeling choice (such as when modeling natural language, where long-term dependencies occur often). We can generalize to high-order dependence trivially second-order p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}, x_{t-2}) p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}, x_{t-2}) m m -order p(x_t | x_{1:t-1}) = p(x_t | x_{t-m:t-1}) p(x_t | x_{1:t-1}) = p(x_t | x_{t-m:t-1}) Parameterization How does the order of temporal dependence affect the number of parameters in our model? Assume x x is a discrete random variable with k k states. How many parameters are needed to parameterize x_t x_t : k-1 k-1 , as the last state is implicit. first-order chain : k(k-1) k(k-1) , as we need k k number of parameters for each parameter of x_t x_t m m -order chain : k^m(k-1) k^m(k-1) , as we need k^m k^m number of parameters for each parameter of x_t x_t Tip He hinted that this would be useful for assignment 2. Seems like it might be the answer to question 4a. Aside So far, we have been thinking about models that operate in discrete space and discrete time but we could also think about models that operate in continuous space or continuous time . Hidden Markov Models [ Markov chains ], make another restrictive assumption: the state of our variables is fully observed . Hence, we introduce Hidden Markov Models (https://duckduckgo.com/?q=hidden+markov+models t=ffab ia=web) HMMs hide the temporal dependence by keeping it in the unobserved state. No assumptions on the temporal dependence of observations is made. For each observation x_t x_t , we associate a corresponding unobserved hidden/latent variable z_t z_t The joint distribution of the model becomes p(x_{1:T}, z_{1:T}) = p(z_1)\\prod_{t=2}^Tp(z_t | z_{t-1})\\prod_{t=1}^Tp(x_t | z_t) p(x_{1:T}, z_{1:T}) = p(z_1)\\prod_{t=2}^Tp(z_t | z_{t-1})\\prod_{t=1}^Tp(x_t | z_t) Unlike simple Markov chains, the observations are not limited by a Markov assumption of any order. Assuming we have a homogeneous model, we only have to learn three distributions Initial distribution : \\pi(i) = p(z_1 = i) \\pi(i) = p(z_1 = i) . The probability of the first hidden variable being in state i i (often denoted \\pi \\pi ) Transition distribution : T(i, j) = p(z_{t + 1} = j | z_t = i) \\quad i \\in \\{1, ..., k\\} T(i, j) = p(z_{t + 1} = j | z_t = i) \\quad i \\in \\{1, ..., k\\} . The probability of moving from hidden state i i to hidden state j j . Emission probability : \\varepsilon_i(x) = p(x | z_t = i) \\varepsilon_i(x) = p(x | z_t = i) . The probability of an observed random variable x x given the state of the hidden variable that \"emitted\" it. Simple Example Say we have the following simple chain where x_t \\in [N, Z, A] x_t \\in [N, Z, A] z_t \\in [H, S] z_t \\in [H, S] where our observed states are whether or not we are watching Netflix ( N N ), sleeping ( Z Z ), or working on the assignment ( A A ) and our hidden states are whether we are happy ( H H ) or sad ( S S ). Say futher that we are given the initial ( \\pi \\pi ), transition ( T T ), and emission probabilities ( \\varepsilon \\varepsilon ) \\pi \\pi H 0.70 S 0.30 \\varepsilon \\varepsilon N Z A H 0.40 0.50 0.10 S 0.10 0.30 0.60 T H S H 0.80 0.20 S 0.10 0.90 Note It is the rows of these tables that need to sum to 1, not the columns! From these conditional probabilities, we can preform inference with the model, e.g. p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ or p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515 p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515 HMMs Main Task The main tasks we perform with HMMs are as follows: 1. Compute the probability of a latent sequence given an observation sequence That is, we want to be able to compute p(z_{1:t} | x_{1:t}) p(z_{1:t} | x_{1:t}) . This is achieved with the Forward-Backward algorithm . 2. Infer the most likely sequence of hidden states That is, we want to be able to compute Z^{\\star} = \\underset{z_{1:T}}{\\operatorname{argmax}} p(z_{1:T} | x_{1:T}) Z^{\\star} = \\underset{z_{1:T}}{\\operatorname{argmax}} p(z_{1:T} | x_{1:T}) . This is achieved using the Viterbi algorithm . 3. Learn the parameters The parameters of a HMM are typically learned with the Baum-Welch algorithm , a special case of the Expectation\u2013maximization algorithm algorithm. In this course, we will focus on the first two tasks. Forward-backward algorithm The Forward-backward algorithm is used to efficiently estimate the latent sequence given an observation sequence under a HMM. That is, we want to compute p(z_{t} | x_{1:T}) \\quad \\forall_t \\in [1, T] p(z_{t} | x_{1:T}) \\quad \\forall_t \\in [1, T] assuming that we know the initial p(z_1) p(z_1) , transition p(z_t | z_{t-1}) p(z_t | z_{t-1}) , and emission p(x_t | z_t) p(x_t | z_t) probabilities \\forall_t \\in [1 ,T] \\forall_t \\in [1 ,T] . This task of hidden state inference breaks down into the following: Filtering : compute posterior over current hidden state, p(z_t | x_{1:t}) p(z_t | x_{1:t}) . Prediction : compute posterior over future hidden state, p(z_{t+k} | x_{1:t}) p(z_{t+k} | x_{1:t}) . Smoothing : compute posterior over past hidden state, p(z_n | x_{1:t}) \\quad 1 \\lt n \\lt t p(z_n | x_{1:t}) \\quad 1 \\lt n \\lt t . Note The Forward-backward algorithm is dynamic, i.e. results are stored as they are computed. Our probability of interest, p(z_{t} | x_{1:T}) p(z_{t} | x_{1:T}) is computed using a forward and backward recursion Forward Recursion : p(z_{t} | x_{1:t}) p(z_{t} | x_{1:t}) Backward Recursion : p(x_{1 + t : T} | z_t) p(x_{1 + t : T} | z_t) We note that p(z_t | x_{1:T}) \\propto p(z_t, x_{1:T}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t, x_{1:t}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t) \\\\ = (\\text{Forward Recursion})(\\text{Backward Recursion}) p(z_t | x_{1:T}) \\propto p(z_t, x_{1:T}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t, x_{1:t}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t) \\\\ = (\\text{Forward Recursion})(\\text{Backward Recursion}) Note The third line is arrived at by noting the conditional independence x_{t+1:T} \\bot x_{1:t} | z x_{t+1:T} \\bot x_{1:t} | z . If it is not clear why this conditional independence holds, try to draw out the HMM conditioned on z_t z_t . Forward Recursion \\begin{align} p(z_t, x_{1:t}) = \\sum^k_{z_{t-1} = 1}p(z_{t-1}, z_t, x_{1:t}) \\\\ = \\sum^k_{z_{t-1} = 1} p(x_t | z_{t-1}, z_t, x_{1:t-1})p(z_t | z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1}) \\\\ \\Rightarrow \\alpha_t(z_t) = p(x_t | z_t) \\sum^k_{z_{t-1} = 1} p(z_t | z_{t-1}) \\alpha_{t-1}(z_{t-1})\\\\ \\end{align} \\begin{align} p(z_t, x_{1:t}) &= \\sum^k_{z_{t-1} = 1}p(z_{t-1}, z_t, x_{1:t}) \\\\ &= \\sum^k_{z_{t-1} = 1} p(x_t | z_{t-1}, z_t, x_{1:t-1})p(z_t | z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1}) \\\\ &\\Rightarrow \\alpha_t(z_t) = p(x_t | z_t) \\sum^k_{z_{t-1} = 1} p(z_t | z_{t-1}) \\alpha_{t-1}(z_{t-1})\\\\ \\end{align} Notice that our forward recursion contains our emission, p(x_t | z_t) p(x_t | z_t) and transition, p(z_t | z_{t-1}) p(z_t | z_{t-1}) probabilities. If we recurse all the way down to \\alpha_1(z_1) \\alpha_1(z_1) , we get \\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1 | z_1) \\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1 | z_1) the initial probability times the emission probability of the first observed state, as expected Backward Recursion \\begin{align} p(x_{t+1:T} | z_t) = \\sum_{z_{t+1}}^k p(z_{t+1}, x_{t+1:T} | z_t) \\\\ = \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1}, z_t, x_{t+1})p(x_{t+1} | z_{t+1}, z_t)p(z_{t+1} | z_t) \\\\ \\Rightarrow \\beta_t(z_t) = \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1})p(x_{t+1} | z_{t+1})p(z_{t+1} | z_t) \\\\ \\end{align} \\begin{align} p(x_{t+1:T} | z_t) &= \\sum_{z_{t+1}}^k p(z_{t+1}, x_{t+1:T} | z_t) \\\\ &= \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1}, z_t, x_{t+1})p(x_{t+1} | z_{t+1}, z_t)p(z_{t+1} | z_t) \\\\ &\\Rightarrow \\beta_t(z_t) = \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1})p(x_{t+1} | z_{t+1})p(z_{t+1} | z_t) \\\\ \\end{align} Notice that our backward recursion contains our emission, p(x_{t+1} | z_{t+1}) p(x_{t+1} | z_{t+1}) and transition, p(z_{t+1} | z_t) p(z_{t+1} | z_t) probabilities. If we recurse all the way down to \\beta_1(z_1) \\beta_1(z_1) , we get \\beta_1(z_1) = p(x_{3:T} | z_{2})p(x_{2} | z_{2})p(z_{2} | z_1) \\\\ \\beta_1(z_1) = p(x_{3:T} | z_{2})p(x_{2} | z_{2})p(z_{2} | z_1) \\\\ Viterbi Algorithm Error This will not be on the exam, so I won't bother cleaning up my notes and putting them here!","title":"Week 9"},{"location":"lectures/week_9/#week-9-hidden-markov-models","text":"","title":"Week 9: Hidden Markov Models"},{"location":"lectures/week_9/#assigned-reading","text":"Murphy: Chapter 18","title":"Assigned Reading"},{"location":"lectures/week_9/#overview","text":"Markov Chains Hidden Markov Models Forward / Backward Algorithm Viterbi Algorithm","title":"Overview"},{"location":"lectures/week_9/#sequential-data","text":"Let us turn our attention to sequential data x_{1:T} = \\{x_1, ..., x_T\\} x_{1:T} = \\{x_1, ..., x_T\\} Broadly speaking, sequential data can be time-series (e.g. stock prices, speech, video analysis) or ordered (e.g. textual data, gene sequences). So far, we have assumed our data to be i.i.d, however this is a poor assumption for sequential data, as there is often clear dependencies between data points. Recall the general joint factorization via the chain rule p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1) p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1) But this quickly becomes intractable for high-dimensional data, which is why we made the i.i.d in the first place. We want temporal dependence without the intractability of computing the chain rule, so we make the simplifying assumption that our data can be modeled as a first-order Markov chain p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}) p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}) Tip In plain english: each observation is assumed to be independent of all previous observations except most recent. This assumption greatly simplifies our joint distribution p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}) p(x_{1:T}) = \\prod_{t=1}^T p(x_t | x_{t-1}) Aside Last week we talked about the Markov chain Monte carlo sampling method Metropolis-Hastings. It is called this because the way you draw samples is a Markov chain. A useful distinction to make at this point is between stationary and non-stationary distributions that generate our data Stationary distribution : the distribution generating the data does not change through time Non-stationary distribution : the distribution generating the data is a function of time We are only going to consider the case of a stationary distribution (because it greatly simplifies the math). In this case, we use the same distribution at every timestep, sometimes called a homogenous Markov chain . Therefore, p(x_t | x_{t-1}) \\quad \\forall t p(x_t | x_{t-1}) \\quad \\forall t notice how the process generating the data is independent of t t .","title":"Sequential data"},{"location":"lectures/week_9/#generalizing-to-high-order-chains","text":"The first-order assumption is still very restrictive. There are many examples where this would be a poor modeling choice (such as when modeling natural language, where long-term dependencies occur often). We can generalize to high-order dependence trivially second-order p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}, x_{t-2}) p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}, x_{t-2}) m m -order p(x_t | x_{1:t-1}) = p(x_t | x_{t-m:t-1}) p(x_t | x_{1:t-1}) = p(x_t | x_{t-m:t-1})","title":"Generalizing to high-order chains"},{"location":"lectures/week_9/#parameterization","text":"How does the order of temporal dependence affect the number of parameters in our model? Assume x x is a discrete random variable with k k states. How many parameters are needed to parameterize x_t x_t : k-1 k-1 , as the last state is implicit. first-order chain : k(k-1) k(k-1) , as we need k k number of parameters for each parameter of x_t x_t m m -order chain : k^m(k-1) k^m(k-1) , as we need k^m k^m number of parameters for each parameter of x_t x_t Tip He hinted that this would be useful for assignment 2. Seems like it might be the answer to question 4a.","title":"Parameterization"},{"location":"lectures/week_9/#aside","text":"So far, we have been thinking about models that operate in discrete space and discrete time but we could also think about models that operate in continuous space or continuous time .","title":"Aside"},{"location":"lectures/week_9/#hidden-markov-models","text":"[ Markov chains ], make another restrictive assumption: the state of our variables is fully observed . Hence, we introduce Hidden Markov Models (https://duckduckgo.com/?q=hidden+markov+models t=ffab ia=web) HMMs hide the temporal dependence by keeping it in the unobserved state. No assumptions on the temporal dependence of observations is made. For each observation x_t x_t , we associate a corresponding unobserved hidden/latent variable z_t z_t The joint distribution of the model becomes p(x_{1:T}, z_{1:T}) = p(z_1)\\prod_{t=2}^Tp(z_t | z_{t-1})\\prod_{t=1}^Tp(x_t | z_t) p(x_{1:T}, z_{1:T}) = p(z_1)\\prod_{t=2}^Tp(z_t | z_{t-1})\\prod_{t=1}^Tp(x_t | z_t) Unlike simple Markov chains, the observations are not limited by a Markov assumption of any order. Assuming we have a homogeneous model, we only have to learn three distributions Initial distribution : \\pi(i) = p(z_1 = i) \\pi(i) = p(z_1 = i) . The probability of the first hidden variable being in state i i (often denoted \\pi \\pi ) Transition distribution : T(i, j) = p(z_{t + 1} = j | z_t = i) \\quad i \\in \\{1, ..., k\\} T(i, j) = p(z_{t + 1} = j | z_t = i) \\quad i \\in \\{1, ..., k\\} . The probability of moving from hidden state i i to hidden state j j . Emission probability : \\varepsilon_i(x) = p(x | z_t = i) \\varepsilon_i(x) = p(x | z_t = i) . The probability of an observed random variable x x given the state of the hidden variable that \"emitted\" it.","title":"Hidden Markov Models"},{"location":"lectures/week_9/#simple-example","text":"Say we have the following simple chain where x_t \\in [N, Z, A] x_t \\in [N, Z, A] z_t \\in [H, S] z_t \\in [H, S] where our observed states are whether or not we are watching Netflix ( N N ), sleeping ( Z Z ), or working on the assignment ( A A ) and our hidden states are whether we are happy ( H H ) or sad ( S S ). Say futher that we are given the initial ( \\pi \\pi ), transition ( T T ), and emission probabilities ( \\varepsilon \\varepsilon ) \\pi \\pi H 0.70 S 0.30 \\varepsilon \\varepsilon N Z A H 0.40 0.50 0.10 S 0.10 0.30 0.60 T H S H 0.80 0.20 S 0.10 0.90 Note It is the rows of these tables that need to sum to 1, not the columns! From these conditional probabilities, we can preform inference with the model, e.g. p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\\\ = (0.80)(0.1) + (0.10)(0.90) \\\\ = 0.17 \\\\ or p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515 p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\\\ = (0.10)(0.17) + (0.60)(1 - 0.17) \\\\ = 0.515","title":"Simple Example"},{"location":"lectures/week_9/#hmms-main-task","text":"The main tasks we perform with HMMs are as follows: 1. Compute the probability of a latent sequence given an observation sequence That is, we want to be able to compute p(z_{1:t} | x_{1:t}) p(z_{1:t} | x_{1:t}) . This is achieved with the Forward-Backward algorithm . 2. Infer the most likely sequence of hidden states That is, we want to be able to compute Z^{\\star} = \\underset{z_{1:T}}{\\operatorname{argmax}} p(z_{1:T} | x_{1:T}) Z^{\\star} = \\underset{z_{1:T}}{\\operatorname{argmax}} p(z_{1:T} | x_{1:T}) . This is achieved using the Viterbi algorithm . 3. Learn the parameters The parameters of a HMM are typically learned with the Baum-Welch algorithm , a special case of the Expectation\u2013maximization algorithm algorithm. In this course, we will focus on the first two tasks.","title":"HMMs Main Task"},{"location":"lectures/week_9/#forward-backward-algorithm","text":"The Forward-backward algorithm is used to efficiently estimate the latent sequence given an observation sequence under a HMM. That is, we want to compute p(z_{t} | x_{1:T}) \\quad \\forall_t \\in [1, T] p(z_{t} | x_{1:T}) \\quad \\forall_t \\in [1, T] assuming that we know the initial p(z_1) p(z_1) , transition p(z_t | z_{t-1}) p(z_t | z_{t-1}) , and emission p(x_t | z_t) p(x_t | z_t) probabilities \\forall_t \\in [1 ,T] \\forall_t \\in [1 ,T] . This task of hidden state inference breaks down into the following: Filtering : compute posterior over current hidden state, p(z_t | x_{1:t}) p(z_t | x_{1:t}) . Prediction : compute posterior over future hidden state, p(z_{t+k} | x_{1:t}) p(z_{t+k} | x_{1:t}) . Smoothing : compute posterior over past hidden state, p(z_n | x_{1:t}) \\quad 1 \\lt n \\lt t p(z_n | x_{1:t}) \\quad 1 \\lt n \\lt t . Note The Forward-backward algorithm is dynamic, i.e. results are stored as they are computed. Our probability of interest, p(z_{t} | x_{1:T}) p(z_{t} | x_{1:T}) is computed using a forward and backward recursion Forward Recursion : p(z_{t} | x_{1:t}) p(z_{t} | x_{1:t}) Backward Recursion : p(x_{1 + t : T} | z_t) p(x_{1 + t : T} | z_t) We note that p(z_t | x_{1:T}) \\propto p(z_t, x_{1:T}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t, x_{1:t}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t) \\\\ = (\\text{Forward Recursion})(\\text{Backward Recursion}) p(z_t | x_{1:T}) \\propto p(z_t, x_{1:T}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t, x_{1:t}) \\\\ = p(z_t, x_{1:t})p(x_{t+1:T} | z_t) \\\\ = (\\text{Forward Recursion})(\\text{Backward Recursion}) Note The third line is arrived at by noting the conditional independence x_{t+1:T} \\bot x_{1:t} | z x_{t+1:T} \\bot x_{1:t} | z . If it is not clear why this conditional independence holds, try to draw out the HMM conditioned on z_t z_t .","title":"Forward-backward algorithm"},{"location":"lectures/week_9/#forward-recursion","text":"\\begin{align} p(z_t, x_{1:t}) = \\sum^k_{z_{t-1} = 1}p(z_{t-1}, z_t, x_{1:t}) \\\\ = \\sum^k_{z_{t-1} = 1} p(x_t | z_{t-1}, z_t, x_{1:t-1})p(z_t | z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1}) \\\\ \\Rightarrow \\alpha_t(z_t) = p(x_t | z_t) \\sum^k_{z_{t-1} = 1} p(z_t | z_{t-1}) \\alpha_{t-1}(z_{t-1})\\\\ \\end{align} \\begin{align} p(z_t, x_{1:t}) &= \\sum^k_{z_{t-1} = 1}p(z_{t-1}, z_t, x_{1:t}) \\\\ &= \\sum^k_{z_{t-1} = 1} p(x_t | z_{t-1}, z_t, x_{1:t-1})p(z_t | z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1}) \\\\ &\\Rightarrow \\alpha_t(z_t) = p(x_t | z_t) \\sum^k_{z_{t-1} = 1} p(z_t | z_{t-1}) \\alpha_{t-1}(z_{t-1})\\\\ \\end{align} Notice that our forward recursion contains our emission, p(x_t | z_t) p(x_t | z_t) and transition, p(z_t | z_{t-1}) p(z_t | z_{t-1}) probabilities. If we recurse all the way down to \\alpha_1(z_1) \\alpha_1(z_1) , we get \\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1 | z_1) \\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1 | z_1) the initial probability times the emission probability of the first observed state, as expected","title":"Forward Recursion"},{"location":"lectures/week_9/#backward-recursion","text":"\\begin{align} p(x_{t+1:T} | z_t) = \\sum_{z_{t+1}}^k p(z_{t+1}, x_{t+1:T} | z_t) \\\\ = \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1}, z_t, x_{t+1})p(x_{t+1} | z_{t+1}, z_t)p(z_{t+1} | z_t) \\\\ \\Rightarrow \\beta_t(z_t) = \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1})p(x_{t+1} | z_{t+1})p(z_{t+1} | z_t) \\\\ \\end{align} \\begin{align} p(x_{t+1:T} | z_t) &= \\sum_{z_{t+1}}^k p(z_{t+1}, x_{t+1:T} | z_t) \\\\ &= \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1}, z_t, x_{t+1})p(x_{t+1} | z_{t+1}, z_t)p(z_{t+1} | z_t) \\\\ &\\Rightarrow \\beta_t(z_t) = \\sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1})p(x_{t+1} | z_{t+1})p(z_{t+1} | z_t) \\\\ \\end{align} Notice that our backward recursion contains our emission, p(x_{t+1} | z_{t+1}) p(x_{t+1} | z_{t+1}) and transition, p(z_{t+1} | z_t) p(z_{t+1} | z_t) probabilities. If we recurse all the way down to \\beta_1(z_1) \\beta_1(z_1) , we get \\beta_1(z_1) = p(x_{3:T} | z_{2})p(x_{2} | z_{2})p(z_{2} | z_1) \\\\ \\beta_1(z_1) = p(x_{3:T} | z_{2})p(x_{2} | z_{2})p(z_{2} | z_1) \\\\","title":"Backward Recursion"},{"location":"lectures/week_9/#viterbi-algorithm","text":"Error This will not be on the exam, so I won't bother cleaning up my notes and putting them here!","title":"Viterbi Algorithm"},{"location":"tutorials/week_1/","text":"Tutorial 1: Introduction to Advanced Probability for Graphical Models Overview Basics Probability rules Exponential family models Maximum likelihood Conjugate Bayesian inference (time permitting) Notation A random variable, X X represents outcomes or states of the world. In this class, we will write p(x) p(x) to mean p(X=x) p(X=x) , the probability of the random variable X X taking on state x x . Tip See here for a helpful list of notational norms in probability. The sample space is the space of all possible outcomes, which may be discrete, continuous, or mixed. p(x) p(x) is the probability mass (or density ) function (PMF/PDF), and assigns a non-negative number to each point in the sample space. The PMF/PDF must sum (or integrate) to 1. Intuitively, we can understand the PMF/PDF at x x as representing how often x x occurs, or how much we believe in x x . Note There is no requirement however, that the PMF/PDF cannot take values greater than 1. A commonly cited and intuitive example is the uniform distribution on the interval [0, \\frac{1}{2}] [0, \\frac{1}{2}] . While the value of the pdf f_X(x) f_X(x) is 2 for 0 \\le x \\le \\frac{1}{2} 0 \\le x \\le \\frac{1}{2} , the area under the graph of f_X(x) f_X(x) is rectangular, and therefore equal to base \\times \\times width = \\frac{1}{2} * 2 = 1 = \\frac{1}{2} * 2 = 1 . Probability Distributions 1. Joint probability distribution The joint probability distribution for random variables X X , Y Y is a probability distribution that gives the probability that each of X X , Y Y falls in any particular range or discrete set of values specified for that variable. P(X=x, Y=y) P(X=x, Y=y) which is read as \"the probability of X X taking on x x and Y Y taking on y y \". 2. Conditional Probability Distribution The conditional probability distribution of Y Y given X X is the probability distribution of Y Y when X X is known to be a particular value. P(Y=y | X=x) = \\frac{p(x, y)}{p(x)} P(Y=y | X=x) = \\frac{p(x, y)}{p(x)} which is read as \"the probability of Y Y taking on y y given that X X is x x \". 3. Marginal Probability Distribution The marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset P(X=x) ; P(y=y) P(X=x) ; P(y=y) if X X , Y Y are discrete p(X=x) = \\sum_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\sum_Xp(X=x,Y=y) p(X=x) = \\sum_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\sum_Xp(X=x,Y=y) if X X , Y Y are continuous p(X=x) = \\int_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\int_Xp(X=x,Y=y) p(X=x) = \\int_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\int_Xp(X=x,Y=y) which is read as \"the probability of X X taking on x x \" or \"the probability of Y Y taking on y y \". Probability Rules Some important rules in probability include 1. Rule of Sum (marginalization) Gives the situations in which the probability of a union of events can be calculated by summing probabilities together. It is often used on mutually exclusive events, meaning events that cannot both happen at the same time. p(x) = \\sum_Y p(X=x, Y=y) p(x) = \\sum_Y p(X=x, Y=y) 2. Chain Rule Permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities. 3. Bayes' Rule Bayes' theorem is a formula that describes how to update the probabilities of hypotheses when given evidence. It follows simply from the axioms of conditional probability, but can be used to powerfully reason about a wide range of problems involving belief updates. p(x | y) = \\frac{p(y | x)p(x)}{p(y)} = \\frac{p(y | x)p(x)}{\\sum_{x'}p(y | x')p(x')} p(x | y) = \\frac{p(y | x)p(x)}{p(y)} = \\frac{p(y | x)p(x)}{\\sum_{x'}p(y | x')p(x')} which is read as \"the posterior is equal to the likelihood times the prior divided by the evidence\". Warning Skipped some slides here. Come back and finish them. Exponential Family An exponential family is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, based on some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. Most of the commonly used distributions are in the exponential family, including Bernoulli Binomial/multinomial Normal (Gaussian) def . Eponential family : The exponential family of distributions over x x , given parameter \\eta \\eta (eta) is the set of distributions of the form p(x | \\eta) = h(x)g(\\eta)exp\\{\\eta^TT(x)\\} p(x | \\eta) = h(x)g(\\eta)exp\\{\\eta^TT(x)\\} where x x is a scalar or a vector and is either continuous or discrete \\eta \\eta are the natural parameter or canonical parameters T(x) T(x) is a vector of sufficient statistics h(x) h(x) is the scaling constant or base measure g(\\eta) g(\\eta) is the normalizing constant that guarantees the sum/integral equals 1. Lets show some examples of rearranging distributions into their exponential family forms Bernoulli The Bernoulli distribution is given by p(x | \\mu) = \\mu^x(1 - \\mu)^{1-x} p(x | \\mu) = \\mu^x(1 - \\mu)^{1-x} re-arranging into exponential family form, we get = \\exp\\{\\ln(\\mu^x(1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{\\ln(\\mu^x) + \\ln((1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{x\\ln(\\mu) + (1-x)\\ln(1 - \\mu)\\} \\\\ = \\exp\\{\\ln(1 - \\mu) + x\\ln(\\mu) - x\\ln(1 - \\mu)\\} \\\\ = (1 - \\mu)\\exp\\{\\ln(\\frac{\\mu}{1 - \\mu})x\\} \\\\ = \\exp\\{\\ln(\\mu^x(1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{\\ln(\\mu^x) + \\ln((1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{x\\ln(\\mu) + (1-x)\\ln(1 - \\mu)\\} \\\\ = \\exp\\{\\ln(1 - \\mu) + x\\ln(\\mu) - x\\ln(1 - \\mu)\\} \\\\ = (1 - \\mu)\\exp\\{\\ln(\\frac{\\mu}{1 - \\mu})x\\} \\\\ from here, it is clear that \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) T(x) = x T(x) = x h(x) = 1 h(x) = 1 noting that \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) \\\\ \\Rightarrow \\exp(\\eta) = \\exp(\\ln(\\frac{\\mu}{1 - \\mu})) \\\\ \\Rightarrow 0 = e^\\eta - ue^\\eta - u \\\\ \\Rightarrow u = \\frac{e^\\eta}{e^\\eta + 1} \\\\ = \\frac{1}{1 + e^{-\\eta}} \\\\ = \\sigma(\\eta) \\\\ \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) \\\\ \\Rightarrow \\exp(\\eta) = \\exp(\\ln(\\frac{\\mu}{1 - \\mu})) \\\\ \\Rightarrow 0 = e^\\eta - ue^\\eta - u \\\\ \\Rightarrow u = \\frac{e^\\eta}{e^\\eta + 1} \\\\ = \\frac{1}{1 + e^{-\\eta}} \\\\ = \\sigma(\\eta) \\\\ we can see that g(\\eta) = (1 - \\mu) = \\sigma(-\\eta) g(\\eta) = (1 - \\mu) = \\sigma(-\\eta) . Multinomial The multinomial distribution is given by p(x_1, ..., x_M | \\mu) = \\prod_{k=1}^M \\mu_k^{x_k} p(x_1, ..., x_M | \\mu) = \\prod_{k=1}^M \\mu_k^{x_k} re-arranging into exponential family form, we get = \\exp(\\ln(\\prod_{k=1}^M \\mu_k^{x_k})) \\\\ = \\exp(\\sum_{k=1}^M x_k \\ln\\mu_k) \\\\ = \\exp(\\ln(\\prod_{k=1}^M \\mu_k^{x_k})) \\\\ = \\exp(\\sum_{k=1}^M x_k \\ln\\mu_k) \\\\ from here, it is clear that \\eta = \\begin{bmatrix}\\ln(u_1) \\cdots \\ln(u_M)\\end{bmatrix} \\eta = \\begin{bmatrix}\\ln(u_1) & \\cdots & \\ln(u_M)\\end{bmatrix} T(x) = x T(x) = x h(x) = 1 h(x) = 1 Gaussian The univariate normal or Gaussian distribution is given by p(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\{-\\frac{1}{2\\sigma^2}x^2 + \\frac{1}{\\sigma^2}xu - \\frac{1}{2\\sigma^2}u^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\{- \\frac{1}{2\\sigma^2}u^2\\} \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ p(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\{-\\frac{1}{2\\sigma^2}x^2 + \\frac{1}{\\sigma^2}xu - \\frac{1}{2\\sigma^2}u^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\{- \\frac{1}{2\\sigma^2}u^2\\} \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ from here, it is clear that \\eta = \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\eta = \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} T(x) = \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} T(x) = \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} re-writing in terms of \\eta \\eta = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} \\cdot (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} \\cdot \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} \\cdot (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} \\cdot \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ noting that h(x) = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} h(x) = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} g(\\eta) = (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} g(\\eta) = (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} Tip Chapter 9.2 of K. Murphy, Machine Learning a Probabilistic Perspective fleshes out these examples in more detail.","title":"Week 1"},{"location":"tutorials/week_1/#tutorial-1-introduction-to-advanced-probability-for-graphical-models","text":"","title":"Tutorial 1: Introduction to Advanced Probability for Graphical Models"},{"location":"tutorials/week_1/#overview","text":"Basics Probability rules Exponential family models Maximum likelihood Conjugate Bayesian inference (time permitting)","title":"Overview"},{"location":"tutorials/week_1/#notation","text":"A random variable, X X represents outcomes or states of the world. In this class, we will write p(x) p(x) to mean p(X=x) p(X=x) , the probability of the random variable X X taking on state x x . Tip See here for a helpful list of notational norms in probability. The sample space is the space of all possible outcomes, which may be discrete, continuous, or mixed. p(x) p(x) is the probability mass (or density ) function (PMF/PDF), and assigns a non-negative number to each point in the sample space. The PMF/PDF must sum (or integrate) to 1. Intuitively, we can understand the PMF/PDF at x x as representing how often x x occurs, or how much we believe in x x . Note There is no requirement however, that the PMF/PDF cannot take values greater than 1. A commonly cited and intuitive example is the uniform distribution on the interval [0, \\frac{1}{2}] [0, \\frac{1}{2}] . While the value of the pdf f_X(x) f_X(x) is 2 for 0 \\le x \\le \\frac{1}{2} 0 \\le x \\le \\frac{1}{2} , the area under the graph of f_X(x) f_X(x) is rectangular, and therefore equal to base \\times \\times width = \\frac{1}{2} * 2 = 1 = \\frac{1}{2} * 2 = 1 .","title":"Notation"},{"location":"tutorials/week_1/#probability-distributions","text":"","title":"Probability Distributions"},{"location":"tutorials/week_1/#1-joint-probability-distribution","text":"The joint probability distribution for random variables X X , Y Y is a probability distribution that gives the probability that each of X X , Y Y falls in any particular range or discrete set of values specified for that variable. P(X=x, Y=y) P(X=x, Y=y) which is read as \"the probability of X X taking on x x and Y Y taking on y y \".","title":"1. Joint probability distribution"},{"location":"tutorials/week_1/#2-conditional-probability-distribution","text":"The conditional probability distribution of Y Y given X X is the probability distribution of Y Y when X X is known to be a particular value. P(Y=y | X=x) = \\frac{p(x, y)}{p(x)} P(Y=y | X=x) = \\frac{p(x, y)}{p(x)} which is read as \"the probability of Y Y taking on y y given that X X is x x \".","title":"2. Conditional Probability Distribution"},{"location":"tutorials/week_1/#3-marginal-probability-distribution","text":"The marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset P(X=x) ; P(y=y) P(X=x) ; P(y=y) if X X , Y Y are discrete p(X=x) = \\sum_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\sum_Xp(X=x,Y=y) p(X=x) = \\sum_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\sum_Xp(X=x,Y=y) if X X , Y Y are continuous p(X=x) = \\int_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\int_Xp(X=x,Y=y) p(X=x) = \\int_Yp(X=x,Y=y) \\; ; \\; p(Y=y) = \\int_Xp(X=x,Y=y) which is read as \"the probability of X X taking on x x \" or \"the probability of Y Y taking on y y \".","title":"3. Marginal Probability Distribution"},{"location":"tutorials/week_1/#probability-rules","text":"Some important rules in probability include","title":"Probability Rules"},{"location":"tutorials/week_1/#1-rule-of-sum-marginalization","text":"Gives the situations in which the probability of a union of events can be calculated by summing probabilities together. It is often used on mutually exclusive events, meaning events that cannot both happen at the same time. p(x) = \\sum_Y p(X=x, Y=y) p(x) = \\sum_Y p(X=x, Y=y)","title":"1. Rule of Sum (marginalization)"},{"location":"tutorials/week_1/#2-chain-rule","text":"Permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities.","title":"2. Chain Rule"},{"location":"tutorials/week_1/#3-bayes-rule","text":"Bayes' theorem is a formula that describes how to update the probabilities of hypotheses when given evidence. It follows simply from the axioms of conditional probability, but can be used to powerfully reason about a wide range of problems involving belief updates. p(x | y) = \\frac{p(y | x)p(x)}{p(y)} = \\frac{p(y | x)p(x)}{\\sum_{x'}p(y | x')p(x')} p(x | y) = \\frac{p(y | x)p(x)}{p(y)} = \\frac{p(y | x)p(x)}{\\sum_{x'}p(y | x')p(x')} which is read as \"the posterior is equal to the likelihood times the prior divided by the evidence\". Warning Skipped some slides here. Come back and finish them.","title":"3. Bayes' Rule"},{"location":"tutorials/week_1/#exponential-family","text":"An exponential family is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, based on some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. Most of the commonly used distributions are in the exponential family, including Bernoulli Binomial/multinomial Normal (Gaussian) def . Eponential family : The exponential family of distributions over x x , given parameter \\eta \\eta (eta) is the set of distributions of the form p(x | \\eta) = h(x)g(\\eta)exp\\{\\eta^TT(x)\\} p(x | \\eta) = h(x)g(\\eta)exp\\{\\eta^TT(x)\\} where x x is a scalar or a vector and is either continuous or discrete \\eta \\eta are the natural parameter or canonical parameters T(x) T(x) is a vector of sufficient statistics h(x) h(x) is the scaling constant or base measure g(\\eta) g(\\eta) is the normalizing constant that guarantees the sum/integral equals 1. Lets show some examples of rearranging distributions into their exponential family forms","title":"Exponential Family"},{"location":"tutorials/week_1/#bernoulli","text":"The Bernoulli distribution is given by p(x | \\mu) = \\mu^x(1 - \\mu)^{1-x} p(x | \\mu) = \\mu^x(1 - \\mu)^{1-x} re-arranging into exponential family form, we get = \\exp\\{\\ln(\\mu^x(1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{\\ln(\\mu^x) + \\ln((1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{x\\ln(\\mu) + (1-x)\\ln(1 - \\mu)\\} \\\\ = \\exp\\{\\ln(1 - \\mu) + x\\ln(\\mu) - x\\ln(1 - \\mu)\\} \\\\ = (1 - \\mu)\\exp\\{\\ln(\\frac{\\mu}{1 - \\mu})x\\} \\\\ = \\exp\\{\\ln(\\mu^x(1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{\\ln(\\mu^x) + \\ln((1 - \\mu)^{1-x})\\} \\\\ = \\exp\\{x\\ln(\\mu) + (1-x)\\ln(1 - \\mu)\\} \\\\ = \\exp\\{\\ln(1 - \\mu) + x\\ln(\\mu) - x\\ln(1 - \\mu)\\} \\\\ = (1 - \\mu)\\exp\\{\\ln(\\frac{\\mu}{1 - \\mu})x\\} \\\\ from here, it is clear that \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) T(x) = x T(x) = x h(x) = 1 h(x) = 1 noting that \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) \\\\ \\Rightarrow \\exp(\\eta) = \\exp(\\ln(\\frac{\\mu}{1 - \\mu})) \\\\ \\Rightarrow 0 = e^\\eta - ue^\\eta - u \\\\ \\Rightarrow u = \\frac{e^\\eta}{e^\\eta + 1} \\\\ = \\frac{1}{1 + e^{-\\eta}} \\\\ = \\sigma(\\eta) \\\\ \\eta = \\ln(\\frac{\\mu}{1 - \\mu}) \\\\ \\Rightarrow \\exp(\\eta) = \\exp(\\ln(\\frac{\\mu}{1 - \\mu})) \\\\ \\Rightarrow 0 = e^\\eta - ue^\\eta - u \\\\ \\Rightarrow u = \\frac{e^\\eta}{e^\\eta + 1} \\\\ = \\frac{1}{1 + e^{-\\eta}} \\\\ = \\sigma(\\eta) \\\\ we can see that g(\\eta) = (1 - \\mu) = \\sigma(-\\eta) g(\\eta) = (1 - \\mu) = \\sigma(-\\eta) .","title":"Bernoulli"},{"location":"tutorials/week_1/#multinomial","text":"The multinomial distribution is given by p(x_1, ..., x_M | \\mu) = \\prod_{k=1}^M \\mu_k^{x_k} p(x_1, ..., x_M | \\mu) = \\prod_{k=1}^M \\mu_k^{x_k} re-arranging into exponential family form, we get = \\exp(\\ln(\\prod_{k=1}^M \\mu_k^{x_k})) \\\\ = \\exp(\\sum_{k=1}^M x_k \\ln\\mu_k) \\\\ = \\exp(\\ln(\\prod_{k=1}^M \\mu_k^{x_k})) \\\\ = \\exp(\\sum_{k=1}^M x_k \\ln\\mu_k) \\\\ from here, it is clear that \\eta = \\begin{bmatrix}\\ln(u_1) \\cdots \\ln(u_M)\\end{bmatrix} \\eta = \\begin{bmatrix}\\ln(u_1) & \\cdots & \\ln(u_M)\\end{bmatrix} T(x) = x T(x) = x h(x) = 1 h(x) = 1","title":"Multinomial"},{"location":"tutorials/week_1/#gaussian","text":"The univariate normal or Gaussian distribution is given by p(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\{-\\frac{1}{2\\sigma^2}x^2 + \\frac{1}{\\sigma^2}xu - \\frac{1}{2\\sigma^2}u^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\{- \\frac{1}{2\\sigma^2}u^2\\} \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ p(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\{-\\frac{1}{2\\sigma^2}x^2 + \\frac{1}{\\sigma^2}xu - \\frac{1}{2\\sigma^2}u^2\\} \\\\ = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\{- \\frac{1}{2\\sigma^2}u^2\\} \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ from here, it is clear that \\eta = \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\eta = \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} T(x) = \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} T(x) = \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} re-writing in terms of \\eta \\eta = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} \\cdot (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} \\cdot \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} \\cdot (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} \\cdot \\exp \\{ \\begin{bmatrix}\\frac{1}{\\sigma^2}u -\\frac{1}{2\\sigma^2}\\end{bmatrix} \\begin{bmatrix}x \\\\\\ x^2\\end{bmatrix} \\} \\\\ noting that h(x) = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} h(x) = (\\sqrt{2 \\pi})^{-\\frac{1}{2}} g(\\eta) = (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} g(\\eta) = (-2\\eta_2)^{\\frac{1}{2}} \\cdot \\exp\\{\\frac{\\eta_1^2}{4\\eta_2}\\} Tip Chapter 9.2 of K. Murphy, Machine Learning a Probabilistic Perspective fleshes out these examples in more detail.","title":"Gaussian"}]}