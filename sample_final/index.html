



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /sample_final/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Sample Final - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#sample-final" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Sample Final
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_8/" title="Week 8" class="md-nav__link">
      Week 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_9/" title="Week 9" class="md-nav__link">
      Week 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_11/" title="Week 11" class="md-nav__link">
      Week 11
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_12/" title="Week 12" class="md-nav__link">
      Week 12
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_13/" title="Week 13" class="md-nav__link">
      Week 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../sample_midterm/" title="Sample Midterm" class="md-nav__link">
      Sample Midterm
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Sample Final
      </label>
    
    <a href="./" title="Sample Final" class="md-nav__link md-nav__link--active">
      Sample Final
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#week-1" title="Week 1" class="md-nav__link">
    Week 1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorial-1" title="Tutorial 1" class="md-nav__link">
    Tutorial 1
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-2-introduction-to-probabilistic-models" title="Week 2: Introduction to Probabilistic Models" class="md-nav__link">
    Week 2: Introduction to Probabilistic Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-3-directed-graphical-models" title="Week 3: Directed Graphical Models" class="md-nav__link">
    Week 3: Directed Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_2" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-4-undirected-graphical-models" title="Week 4: Undirected Graphical Models" class="md-nav__link">
    Week 4: Undirected Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_3" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_1" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3_1" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4_1" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-5-exact-inference" title="Week 5: Exact Inference" class="md-nav__link">
    Week 5: Exact Inference
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_4" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-6" title="Week 6" class="md-nav__link">
    Week 6
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-8-sampling-and-monte-carlo-methods" title="Week 8: Sampling and Monte Carlo Methods" class="md-nav__link">
    Week 8: Sampling and Monte Carlo Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_5" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-9-hidden-markov-models" title="Week 9: Hidden Markov Models" class="md-nav__link">
    Week 9: Hidden Markov Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_6" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_3" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi" title="Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)" class="md-nav__link">
    Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_7" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-12" title="Week 12" class="md-nav__link">
    Week 12
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_8" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#week-1" title="Week 1" class="md-nav__link">
    Week 1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorial-1" title="Tutorial 1" class="md-nav__link">
    Tutorial 1
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-2-introduction-to-probabilistic-models" title="Week 2: Introduction to Probabilistic Models" class="md-nav__link">
    Week 2: Introduction to Probabilistic Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-3-directed-graphical-models" title="Week 3: Directed Graphical Models" class="md-nav__link">
    Week 3: Directed Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_2" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-4-undirected-graphical-models" title="Week 4: Undirected Graphical Models" class="md-nav__link">
    Week 4: Undirected Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_3" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_1" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3_1" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4_1" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-5-exact-inference" title="Week 5: Exact Inference" class="md-nav__link">
    Week 5: Exact Inference
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_4" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-6" title="Week 6" class="md-nav__link">
    Week 6
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-8-sampling-and-monte-carlo-methods" title="Week 8: Sampling and Monte Carlo Methods" class="md-nav__link">
    Week 8: Sampling and Monte Carlo Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_5" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-9-hidden-markov-models" title="Week 9: Hidden Markov Models" class="md-nav__link">
    Week 9: Hidden Markov Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_6" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_3" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi" title="Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)" class="md-nav__link">
    Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_7" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-12" title="Week 12" class="md-nav__link">
    Week 12
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_8" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/sample_final.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="sample-final">Sample Final<a class="headerlink" href="#sample-final" title="Permanent link">&para;</a></h1>
<p>These are question pulled from the lecture, assignments and the sample midterm, alongside questions that were written based on the study guide. These were <em>not</em> given by an instructor and are merely guesses as to what kind of questions might be on the final.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the study guide <a href="http://www.cs.toronto.edu/~jessebett/CSC412/content/Final_Topics/final_topics.pdf">here</a>.</p>
</div>
<h2 id="week-1">Week 1<a class="headerlink" href="#week-1" title="Permanent link">&para;</a></h2>
<h2 id="tutorial-1">Tutorial 1<a class="headerlink" href="#tutorial-1" title="Permanent link">&para;</a></h2>
<h3 id="question-1">Question 1<a class="headerlink" href="#question-1" title="Permanent link">&para;</a></h3>
<p>Recall that the definition of an exponential family model is:</p>
<div>
<div class="MathJax_Preview">
f(x | \eta) = h(x)g(\eta)\exp(\eta^TT(x))
</div>
<script type="math/tex; mode=display">
f(x | \eta) = h(x)g(\eta)\exp(\eta^TT(x))
</script>
</div>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> are the parameters</li>
<li><span><span class="MathJax_Preview">T(x)</span><script type="math/tex">T(x)</script></span> are the sufficient statistics</li>
<li><span><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span> is the base measure</li>
<li><span><span class="MathJax_Preview">g(\eta)</span><script type="math/tex">g(\eta)</script></span> is the normalizing constant</li>
</ul>
<p>Consider the univariate Gaussian, with mean <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and precision <span><span class="MathJax_Preview">\lambda = \frac{1}{\sigma^2}</span><script type="math/tex">\lambda = \frac{1}{\sigma^2}</script></span></p>
<div>
<div class="MathJax_Preview">
p(D | \mu \lambda) = \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(-\frac{\lambda}{2}(x_i - \mu)^2)
</div>
<script type="math/tex; mode=display">
p(D | \mu \lambda) = \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(-\frac{\lambda}{2}(x_i - \mu)^2)
</script>
</div>
<p>What are <span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> and <span><span class="MathJax_Preview">T(x)</span><script type="math/tex">T(x)</script></span> for this distribution when represented in exponential family form?</p>
<p><strong>ANSWER</strong></p>
<p>Start by expanding the terms in the exponent</p>
<div>
<div class="MathJax_Preview">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}} \exp(\sum_{i=1}^N -\frac{\lambda}{2}x_i^2 + \lambda u x_i - \frac{\lambda}{2}\mu^2) \\
</div>
<script type="math/tex; mode=display">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}} \exp(\sum_{i=1}^N -\frac{\lambda}{2}x_i^2 + \lambda u x_i - \frac{\lambda}{2}\mu^2) \\
</script>
</div>
<p>from here, we can rearrange the exponent into <span><span class="MathJax_Preview">\eta^TT(x)</span><script type="math/tex">\eta^TT(x)</script></span></p>
<div>
<div class="MathJax_Preview">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(\sum_{i=1}^N - \frac{\lambda}{2}\mu^2)\exp(\begin{bmatrix}\lambda u &amp; -\frac{\lambda}{2} &amp; \dotsc &amp; \lambda u &amp; -\frac{\lambda}{2}\end{bmatrix} \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}) \\
</div>
<script type="math/tex; mode=display">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(\sum_{i=1}^N - \frac{\lambda}{2}\mu^2)\exp(\begin{bmatrix}\lambda u & -\frac{\lambda}{2} & \dotsc & \lambda u & -\frac{\lambda}{2}\end{bmatrix} \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}) \\
</script>
</div>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">\eta^T = \begin{bmatrix}\lambda u &amp; -\frac{\lambda}{2} &amp; \dotsc &amp; \lambda u &amp; -\frac{\lambda}{2}\end{bmatrix}</span><script type="math/tex">\eta^T = \begin{bmatrix}\lambda u & -\frac{\lambda}{2} & \dotsc & \lambda u & -\frac{\lambda}{2}\end{bmatrix}</script></span></li>
<li><span><span class="MathJax_Preview">T(x) = \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}</span><script type="math/tex">T(x) = \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}</script></span></li>
</ul>
<h2 id="week-2-introduction-to-probabilistic-models">Week 2: Introduction to Probabilistic Models<a class="headerlink" href="#week-2-introduction-to-probabilistic-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_1">Question 1<a class="headerlink" href="#question-1_1" title="Permanent link">&para;</a></h3>
<p>In this question, we'll fit a naive Bayes model to the MNIST digits using maximum likelihood.
Naive Bayes defines the joint probability of the each datapoint <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and its class label <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> as follows:</p>
<p>\begin{align}
p(x, c | \theta, \pi) = p(c | \pi) p(x | c, \theta_c) = p(c | \pi) \prod_{d=1}^{784} p( x_d | c, \theta_{cd})
\end{align}
For binary data, we can use the Bernoulli likelihood:
\begin{align}
p( x_d | c, \theta_{cd}) = Ber(x_d | \theta_{cd}) = \theta_{cd}^{x_d} ( 1 - \theta_{cd})^{(1 - x_d)}
\end{align}
Which is just a way of expressing that <span><span class="MathJax_Preview">p(x_d = 1 | c, \theta_{cd}) = \theta_{cd}</span><script type="math/tex">p(x_d = 1 | c, \theta_{cd}) = \theta_{cd}</script></span>.</p>
<p>For <span><span class="MathJax_Preview">p(c | \pi)</span><script type="math/tex">p(c | \pi)</script></span>, we can just use a categorical distribution:
\begin{align}
p(c | \pi) = Cat(c|\pi) = \pi_c
\end{align}
Note that we need <span><span class="MathJax_Preview">\sum_{i=0}^9 \pi_{i} = 1</span><script type="math/tex">\sum_{i=0}^9 \pi_{i} = 1</script></span>.</p>
<p>(a) Derive the <em>maximum likelihood estimate</em> (MLE) for the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>. Hint: We saw in lecture that MLE can be thought of as 'counts' for the data, so what should <span><span class="MathJax_Preview">\hat \theta_{cd}</span><script type="math/tex">\hat \theta_{cd}</script></span> be counting?</p>
<p>(b) Derive the <em>maximum a posteriori</em> (MAP) estimate for the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>, using a <span><span class="MathJax_Preview">Beta(2, 2)</span><script type="math/tex">Beta(2, 2)</script></span> prior on each <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.  Hint: it has a simple final form, and you can ignore the Beta normalizing constant.</p>
<p><strong>ANSWER</strong></p>
<p>(a) The maximum likelihood estimate of the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> for class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> is given by</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \prod_{i=1}^N \prod_{d=1}^{784}\theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})}
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \prod_{i=1}^N \prod_{d=1}^{784}\theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})}
</script>
</div>
<p>where <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is the number of training examples. Taking the log, we get</p>
<div>
<div class="MathJax_Preview">\begin{align*}
&amp;= \operatorname*{argmax}_{\theta_c} \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} \log \bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \bigg )  \Bigg ] \\
&amp;= \operatorname*{argmax}_{\theta_c}  \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} x_d^{(i)} \log \theta_{cd} + (1-x_d^{(i)}) \log (1 - \theta_{cd})  \Bigg ] \\
&amp; = \operatorname*{argmax}_{\theta_c} \Bigg [  n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c})  \Bigg ] \\
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
&= \operatorname*{argmax}_{\theta_c} \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} \log \bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \bigg )  \Bigg ] \\
&= \operatorname*{argmax}_{\theta_c}  \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} x_d^{(i)} \log \theta_{cd} + (1-x_d^{(i)}) \log (1 - \theta_{cd})  \Bigg ] \\
& = \operatorname*{argmax}_{\theta_c} \Bigg [  n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c})  \Bigg ] \\
\end{align*}</script>
</div>
<p>where <span><span class="MathJax_Preview">n_c^{d=1}</span><script type="math/tex">n_c^{d=1}</script></span> is a vector of counts containing the number of training examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> where <span><span class="MathJax_Preview">d=1</span><script type="math/tex">d=1</script></span> for each pixel dimension <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> in 784, <span><span class="MathJax_Preview">n_c^{d=0}</span><script type="math/tex">n_c^{d=0}</script></span> is the corresponding count vector for the number of training examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> where <span><span class="MathJax_Preview">d = 0</span><script type="math/tex">d = 0</script></span>, and <span><span class="MathJax_Preview">\cdot</span><script type="math/tex">\cdot</script></span> denotes the dot product. Taking the derivative of this expression and setting it to 0, we can solve for the MLE of the parameters</p>
<div>
<div class="MathJax_Preview">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c}  \bigg [ n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c}) \bigg ] &amp;= 0 \\
\frac{n_c^{d=1}}{\theta_{c}} - \frac{n_c^{d=0}}{1 - \theta_{c}} &amp; = 0
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c}  \bigg [ n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c}) \bigg ] &= 0 \\
\frac{n_c^{d=1}}{\theta_{c}} - \frac{n_c^{d=0}}{1 - \theta_{c}} & = 0
\end{align*}</script>
</div>
<p>Rearranging, we get</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \frac{n_c^{d=1}}{N}
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \frac{n_c^{d=1}}{N}
</script>
</div>
<p>therefore, the MLE for the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span>, <span><span class="MathJax_Preview">\hat \theta_c</span><script type="math/tex">\hat \theta_c</script></span>, is given by the number of examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> where <span><span class="MathJax_Preview">d=1</span><script type="math/tex">d=1</script></span> divided by the total number of examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span>, as expected.</p>
<p>(b) The posterior probability of our class-conditional pixel means, <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> for class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> is given by</p>
<div>
<div class="MathJax_Preview">
f(\theta_c; \alpha, \beta) = f(\theta_c; 2, 2) = \frac{1}{B(2, 2)}\theta_c^{2 - 1}(1-\theta_c)^{2-1} = \theta_c(1-\theta_c)
</div>
<script type="math/tex; mode=display">
f(\theta_c; \alpha, \beta) = f(\theta_c; 2, 2) = \frac{1}{B(2, 2)}\theta_c^{2 - 1}(1-\theta_c)^{2-1} = \theta_c(1-\theta_c)
</script>
</div>
<p>\noindent where we have ignored the Beta normalizing constant. The MAP estimate of the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> for class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> is given by</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \Bigg [ \theta_c(1-\theta_c) \prod_{i=1}^N \prod_{d=1}^{784} \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ]
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \Bigg [ \theta_c(1-\theta_c) \prod_{i=1}^N \prod_{d=1}^{784} \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ]
</script>
</div>
<p>Taking the log, we get</p>
<div>
<div class="MathJax_Preview">\begin{align*}
\hat \theta_c &amp;= \operatorname*{argmax}_{\theta_c} \Bigg [ \log \big ( \theta_c(1-\theta_c) \big ) + \sum_{i=1}^N \sum_{d=1}^{784} \log \Bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ) \Bigg ] \\
&amp;= \operatorname*{argmax}_{\theta_c} \Bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \Bigg ]
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
\hat \theta_c &= \operatorname*{argmax}_{\theta_c} \Bigg [ \log \big ( \theta_c(1-\theta_c) \big ) + \sum_{i=1}^N \sum_{d=1}^{784} \log \Bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ) \Bigg ] \\
&= \operatorname*{argmax}_{\theta_c} \Bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \Bigg ]
\end{align*}</script>
</div>
<p>taking the derivative of this expression and setting it to 0, we can solve for the MAP estimate of the parameters</p>
<div>
<div class="MathJax_Preview">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c} \bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \bigg ] &amp;= 0 \\
\frac{1 + n_c^{d=1}}{\theta_c} - \frac{1 + n_c^{d=0}}{(1-\theta_c)} &amp;= 0 \\
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c} \bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \bigg ] &= 0 \\
\frac{1 + n_c^{d=1}}{\theta_c} - \frac{1 + n_c^{d=0}}{(1-\theta_c)} &= 0 \\
\end{align*}</script>
</div>
<p>Rearranging, we get</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \frac{n_c^{d=1} + 1}{N + 2}
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \frac{n_c^{d=1} + 1}{N + 2}
</script>
</div>
<h2 id="week-3-directed-graphical-models">Week 3: Directed Graphical Models<a class="headerlink" href="#week-3-directed-graphical-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_2">Question 1<a class="headerlink" href="#question-1_2" title="Permanent link">&para;</a></h3>
<p>When we condition on <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, are <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> independent?</p>
<p>(a)</p>
<p><img alt="" src="../img/lecture_3_5.png" /></p>
<p>(b)</p>
<p><img alt="" src="../img/lecture_3_6.png" /></p>
<p>(c)</p>
<p><img alt="" src="../img/lecture_3_7.png" /></p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(x)P(y|x)P(z|y)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(x)P(y|x)P(z|y)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(z | x, y) &amp;= \frac{P(x, y, z)}{P(x, y)} \\
&amp;= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
&amp;= P(z | y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(z | x, y) &= \frac{P(x, y, z)}{P(x, y)} \\
&= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
&= P(z | y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(z | x, y) = P(z | y)</span><script type="math/tex">P(z | x, y) = P(z | y)</script></span> and so by <span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span>, <span><span class="MathJax_Preview">x \bot z | y</span><script type="math/tex">x \bot z | y</script></span>.</p>
<p>(b)</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(y)P(x|y)P(z|y)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(y)P(x|y)P(z|y)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(x, z | y) &amp;= \frac{P(x, y, z)}{P(y)} \\
&amp;= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
&amp;= P(x|y)P(z|y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(x, z | y) &= \frac{P(x, y, z)}{P(y)} \\
&= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
&= P(x|y)P(z|y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(x, z| y) = P(x|y)P(z|y)</span><script type="math/tex">P(x, z| y) = P(x|y)P(z|y)</script></span> and so by <span><span class="MathJax_Preview">\star</span><script type="math/tex">\star</script></span>, <span><span class="MathJax_Preview">x \bot z | y</span><script type="math/tex">x \bot z | y</script></span>.</p>
<p>(c)</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(x)P(z)P(y|x, z)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(x)P(z)P(y|x, z)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(z | x, y) &amp;= \frac{P(x)P(z)P(y | x,  z)}{P(x)P(y|x)} \\
&amp;= \frac{P(z)P(y | x,  z)}{P(y|x)}  \\
&amp;\not = P(z|y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(z | x, y) &= \frac{P(x)P(z)P(y | x,  z)}{P(x)P(y|x)} \\
&= \frac{P(z)P(y | x,  z)}{P(y|x)}  \\
&\not = P(z|y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(z | x, y) \not = P(z|y)</span><script type="math/tex">P(z | x, y) \not = P(z|y)</script></span> and so by <span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span>, <span><span class="MathJax_Preview">x \not \bot z | y</span><script type="math/tex">x \not \bot z | y</script></span>.</p>
<p>In fact, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> are <em>marginally independent</em>, but given <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> they are <em>conditionally dependent</em>. This important effect is called explaining away (<a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox">Berkson’s paradox</a>).</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Imaging flipping two coins independently, represented by events <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>. Furthermore, let <span><span class="MathJax_Preview">y=1</span><script type="math/tex">y=1</script></span> if the coins come up the same and <span><span class="MathJax_Preview">y=0</span><script type="math/tex">y=0</script></span> if they come up differently. Clearly, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> are independent, but if I tell you <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, they become coupled!</p>
</div>
<h3 id="question-2">Question 2<a class="headerlink" href="#question-2" title="Permanent link">&para;</a></h3>
<p>(a) In the following graph, is <span><span class="MathJax_Preview">x_1 \bot x_6 | \{x_2, x_3\}</span><script type="math/tex">x_1 \bot x_6 | \{x_2, x_3\}</script></span>?</p>
<p><img alt="" src="../img/lecture_3_12.png" /></p>
<p>(b) In the following graph, is <span><span class="MathJax_Preview">x_2 \bot x_3 | \{x_1, x_6\}</span><script type="math/tex">x_2 \bot x_3 | \{x_1, x_6\}</script></span>?</p>
<p><img alt="" src="../img/lecture_3_14.png" /></p>
<p><strong>ANSWER</strong></p>
<p>(a) Yes, by the Bayes Balls algorithm.</p>
<p><img alt="" src="../img/lecture_3_13.png" /></p>
<p>(b) No, by the Bayes Balls algorithm.</p>
<p><img alt="" src="../img/lecture_3_15.png" /></p>
<h3 id="question-3">Question 3<a class="headerlink" href="#question-3" title="Permanent link">&para;</a></h3>
<p>Consider the following directed graphical model:</p>
<p><img alt="" src="../img/sample_midterm_1.png" /></p>
<p>(a) List all variables that are independent of <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> given evidence on <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span></p>
<p>(b) Write down the factorized normalized joint distribution that this graphical model represents.</p>
<p>(c) If each node is a single discrete random variable in <span><span class="MathJax_Preview">{1, ..., K}</span><script type="math/tex">{1, ..., K}</script></span> how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set?</p>
<p><strong>ANSWER</strong></p>
<p>(a) By Bayes' Balls, no variables are conditionally independent of <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> given evidence on <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>.</p>
<p><img alt="" src="../img/sample_midterm_2.png" /></p>
<p>(b)</p>
<div>
<div class="MathJax_Preview">
p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H)
</div>
<script type="math/tex; mode=display">
p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H)
</script>
</div>
<p>(c) For each node (random variable) there is <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> states. There are <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> possible configurations where <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> is the number of states and <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> the number of nodes (<span><span class="MathJax_Preview">x_{\pi_i}</span><script type="math/tex">x_{\pi_i}</script></span>)</p>
<div>
<div class="MathJax_Preview">
\therefore \text{number of possible configurations} = k^9
</div>
<script type="math/tex; mode=display">
\therefore \text{number of possible configurations} = k^9
</script>
</div>
<h3 id="question-4">Question 4<a class="headerlink" href="#question-4" title="Permanent link">&para;</a></h3>
<p>Consider the Hidden Markov Model</p>
<p><img alt="" src="../img/sample_midterm_4.png" /></p>
<p>(a) Assume you are able to sample from these conditional distributions, i.e.</p>
<div>
<div class="MathJax_Preview">
x_i \sim p(X_i \ | \ \text{parents of } X_i)
</div>
<script type="math/tex; mode=display">
x_i \sim p(X_i \ | \ \text{parents of } X_i)
</script>
</div>
<p>Write down a step-by-step process to produce a sample observation from this model, i.e. <span><span class="MathJax_Preview">(x_1, x_2, x_3, ..., x_T)</span><script type="math/tex">(x_1, x_2, x_3, ..., x_T)</script></span> in terms of samples from the individual factors.</p>
<p><strong>ANSWER</strong></p>
<p>We want to sample a sequence of observations <span><span class="MathJax_Preview">x_1, x_2, x_3, ..., x_T</span><script type="math/tex">x_1, x_2, x_3, ..., x_T</script></span> from the model according to</p>
<div>
<div class="MathJax_Preview">
x_{1:T} = \prod_{t=1}^T p(X_t \ | \ \text{parents of } X_t)
</div>
<script type="math/tex; mode=display">
x_{1:T} = \prod_{t=1}^T p(X_t \ | \ \text{parents of } X_t)
</script>
</div>
<p>since observations <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> are independent of one another. Notice that this forms a chain, with probability</p>
<div>
<div class="MathJax_Preview">
p(x_{1:T}) \sim \bigg [ \prod_{t=1}^T p(X_t | z_t) \bigg ] \bigg [ p(z_1) \prod_{t=2}^T p(Z_t | z_{t-1}) \bigg ]
</div>
<script type="math/tex; mode=display">
p(x_{1:T}) \sim \bigg [ \prod_{t=1}^T p(X_t | z_t) \bigg ] \bigg [ p(z_1) \prod_{t=2}^T p(Z_t | z_{t-1}) \bigg ]
</script>
</div>
<p><em>Step-by-step</em></p>
<ol>
<li>Start with <span><span class="MathJax_Preview">t=1</span><script type="math/tex">t=1</script></span></li>
<li>Sample <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span> according to <span><span class="MathJax_Preview">z_t \sim p(z_1) \prod_{i=t}^{t + 1} p(Z_i | z_{i-1})</span><script type="math/tex">z_t \sim p(z_1) \prod_{i=t}^{t + 1} p(Z_i | z_{i-1})</script></span></li>
<li>Given the sampled <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span>, sample <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> according to <span><span class="MathJax_Preview">x_t \sim \ p(X_t | z_t)</span><script type="math/tex">x_t \sim \ p(X_t | z_t)</script></span></li>
<li>Increment <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> by 1</li>
<li>Repeat steps 2-4 until <span><span class="MathJax_Preview">t=T</span><script type="math/tex">t=T</script></span></li>
</ol>
<h2 id="week-4-undirected-graphical-models">Week 4: Undirected Graphical Models<a class="headerlink" href="#week-4-undirected-graphical-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_3">Question 1<a class="headerlink" href="#question-1_3" title="Permanent link">&para;</a></h3>
<p>(a) State the Global, Local and Pairwise Markov properties used to determine conditional independence in a undirected graphical model.</p>
<p>(b) Given the following UGMs:</p>
<p><img alt="" src="../img/lecture_4_4.png" /></p>
<p>use each to Markov property to give an example of two sets of conditionally independent nodes in the graph.</p>
<p>(c) Do the same for the following UGM:</p>
<p><img alt="" src="../img/lecture_4_3.png" /></p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p><em>def</em>. <strong>Global Markov Property</strong> (G): <span><span class="MathJax_Preview">X_A \bot X_B | X_C</span><script type="math/tex">X_A \bot X_B | X_C</script></span> iff <span><span class="MathJax_Preview">X_C</span><script type="math/tex">X_C</script></span> separates <span><span class="MathJax_Preview">X_A</span><script type="math/tex">X_A</script></span> from <span><span class="MathJax_Preview">X_B</span><script type="math/tex">X_B</script></span></p>
<p><em>def</em>. <strong>Local Markov Property (Markov Blanket)</strong> (L): The set of nodes that renders a node <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> conditionally independent of all the other nodes in the graph</p>
<div>
<div class="MathJax_Preview">
t \bot \mathcal V \setminus cl(t) | mb(t)
</div>
<script type="math/tex; mode=display">
t \bot \mathcal V \setminus cl(t) | mb(t)
</script>
</div>
<p><em>def</em>. <strong>Pairwise (Markov) Property</strong> (P): The set of nodes that renders two nodes, <span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> and <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>, conditionally independent of each other.</p>
<div>
<div class="MathJax_Preview">
s \bot t | \mathcal V \setminus \{s, t\} \Leftrightarrow G_{st} = 0
</div>
<script type="math/tex; mode=display">
s \bot t | \mathcal V \setminus \{s, t\} \Leftrightarrow G_{st} = 0
</script>
</div>
<p>(b)</p>
<ul>
<li>Global: <span><span class="MathJax_Preview">\{1, 2\} \bot \{6, 7\} | \{3, 4, 5\}</span><script type="math/tex">\{1, 2\} \bot \{6, 7\} | \{3, 4, 5\}</script></span></li>
<li>Local: <span><span class="MathJax_Preview">1 \bot \text{rest} | \{2, 3\}</span><script type="math/tex">1 \bot \text{rest} | \{2, 3\}</script></span></li>
<li>Pairwise: <span><span class="MathJax_Preview">1 \bot 7 | \text{rest}</span><script type="math/tex">1 \bot 7 | \text{rest}</script></span></li>
</ul>
<p>(c)</p>
<ul>
<li>Global: <span><span class="MathJax_Preview">\{X_1, X_2\} \bot \{X_{15}, X_{20}\} | \{X_3, X_6, X_7\}</span><script type="math/tex">\{X_1, X_2\} \bot \{X_{15}, X_{20}\} | \{X_3, X_6, X_7\}</script></span></li>
<li>Local: <span><span class="MathJax_Preview">1 \bot \text{rest} | \{X_2, X_6\}</span><script type="math/tex">1 \bot \text{rest} | \{X_2, X_6\}</script></span></li>
<li>Pairwise: <span><span class="MathJax_Preview">1 \bot 20 | \text{rest}</span><script type="math/tex">1 \bot 20 | \text{rest}</script></span></li>
</ul>
<h3 id="question-2_1">Question 2<a class="headerlink" href="#question-2_1" title="Permanent link">&para;</a></h3>
<p>Given the following graph:</p>
<p><img alt="" src="../img/lecture_4_4.png" /></p>
<p>(a) What is a <em>maximal</em> clique? State one example from the graph.</p>
<p>(b) What is a <em>maximum</em> clique? State on example from the graph.</p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p><em>def</em>. A <a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)#Definitions"><strong>maximal clique</strong></a> is a clique that cannot be extended by including one more adjacent vertex.</p>
<p><em>def</em>. A <a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)#Definitions"><strong>maximum clique</strong></a> is a clique of the <em>largest possible size</em> in a given graph.</p>
<p>(b)</p>
<p>A <em>maximal clique</em> is show in blue, while a <em>maximum clique</em> is shown in green.</p>
<p><img alt="" src="../img/lecture_4_7.png" /></p>
<h3 id="question-3_1">Question 3<a class="headerlink" href="#question-3_1" title="Permanent link">&para;</a></h3>
<p>Given the following graph:</p>
<p><img alt="" src="../img/lecture_4_4.png" /></p>
<p>(a) Write down the factorized joint distribution that this graphical model represents</p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<div>
<div class="MathJax_Preview">
p(x) \propto \psi_{1, 2, 3}(x_1, x_2, x_3) \psi_{2, 3, 5}(x_2, x_3, x_5) \psi_{2, 4, 5}(x_2, x_4, x_5) \psi_{3, 5, 6}(x_3, x_5, x_6) \psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7)
</div>
<script type="math/tex; mode=display">
p(x) \propto \psi_{1, 2, 3}(x_1, x_2, x_3) \psi_{2, 3, 5}(x_2, x_3, x_5) \psi_{2, 4, 5}(x_2, x_4, x_5) \psi_{3, 5, 6}(x_3, x_5, x_6) \psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7)
</script>
</div>
<h3 id="question-4_1">Question 4<a class="headerlink" href="#question-4_1" title="Permanent link">&para;</a></h3>
<p>Compare and contrast directed vs undirected graphical models</p>
<p><center></p>
<table>
<thead>
<tr>
<th></th>
<th>DGMs</th>
<th>UGMs</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p></center></p>
<h2 id="week-5-exact-inference">Week 5: Exact Inference<a class="headerlink" href="#week-5-exact-inference" title="Permanent link">&para;</a></h2>
<h3 id="question-1_4">Question 1<a class="headerlink" href="#question-1_4" title="Permanent link">&para;</a></h3>
<p>Given the graph</p>
<p><img alt="" src="../img/sample_midterm_3.png" /></p>
<p>(a) Suppose we want to compute the partition function (<span><span class="MathJax_Preview">Z(\theta)</span><script type="math/tex">Z(\theta)</script></span>, see <a href="../lectures/week_4/#parameterization-of-an-ugm">here</a>) using the elimination ordering <span><span class="MathJax_Preview">\prec= (1, 2, 3, 4, 5, 6)</span><script type="math/tex">\prec= (1, 2, 3, 4, 5, 6)</script></span>. If we use the <a href="../lectures/week_5/#variable-elimination">variable elimination algorithm</a>, we will create new intermediate factors. What is the largest intermediate factor?</p>
<p><strong>ANSWER</strong></p>
<p>a) The size of the maximum factor is 3.</p>
<p>The set of potentials given by the graph is</p>
<div>
<div class="MathJax_Preview">
\Psi = \{\psi_{X_1, X_2}(X_1, X_2), \psi_{X_1, X_3}(X_1, X_3), \psi_{X_2, X_4}(X_2, X_4), \psi_{X_3, X_4}(X_3, X_4), \psi_{X_4, X_5}(X_4, X_5), \psi_{X_5, X_6}(X_5, X_6) \}
</div>
<script type="math/tex; mode=display">
\Psi = \{\psi_{X_1, X_2}(X_1, X_2), \psi_{X_1, X_3}(X_1, X_3), \psi_{X_2, X_4}(X_2, X_4), \psi_{X_3, X_4}(X_3, X_4), \psi_{X_4, X_5}(X_4, X_5), \psi_{X_5, X_6}(X_5, X_6) \}
</script>
</div>
<p>and the joint probability is therefore</p>
<div>
<div class="MathJax_Preview">
p(X) \propto \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
</div>
<script type="math/tex; mode=display">
p(X) \propto \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
</script>
</div>
<p>finally, the partition function with elimination ordering <span><span class="MathJax_Preview">\prec= (1, 2, 3, 4, 5, 6)</span><script type="math/tex">\prec= (1, 2, 3, 4, 5, 6)</script></span> is given by</p>
<div>
<div class="MathJax_Preview">\begin{align}
\tau(X) &amp;= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&amp;= \sum_{x_6}\sum_{x_5}\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\tau(X) &= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&= \sum_{x_6}\sum_{x_5}\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</script>
</div>
<p>Carrying out the elimination, (not shown here), we get intermediate factors</p>
<div>
<div class="MathJax_Preview">
\{\tau_1(x_2, x_3), \tau_2(x_3, x_4), \tau_3(x_4), \tau_4(x_5), \tau_5(x_6)\}
</div>
<script type="math/tex; mode=display">
\{\tau_1(x_2, x_3), \tau_2(x_3, x_4), \tau_3(x_4), \tau_4(x_5), \tau_5(x_6)\}
</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> the maximum factor is of size 3.</p>
<p>b) The only edge that does not already exist is the edge between <span><span class="MathJax_Preview">X_2</span><script type="math/tex">X_2</script></span> and <span><span class="MathJax_Preview">X_3</span><script type="math/tex">X_3</script></span> (added by intermediate factor <span><span class="MathJax_Preview">\tau_1(x_2, x_3)</span><script type="math/tex">\tau_1(x_2, x_3)</script></span>). The largest maximal clique is now of size 3 (<span><span class="MathJax_Preview">\{x_2, x_3, x_4\}</span><script type="math/tex">\{x_2, x_3, x_4\}</script></span>).</p>
<p>(c)</p>
<p>The partition function with elimination ordering <span><span class="MathJax_Preview">\prec= (4, 1, 2, 3, 5, 6)</span><script type="math/tex">\prec= (4, 1, 2, 3, 5, 6)</script></span> is given by</p>
<div>
<div class="MathJax_Preview">\begin{align}
\tau(X) &amp;= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&amp;= \sum_{x_6}\sum_{x_5}\sum_{x_3}\sum_{x_2}\sum_{x_1}\sum_{x_4} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\tau(X) &= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&= \sum_{x_6}\sum_{x_5}\sum_{x_3}\sum_{x_2}\sum_{x_1}\sum_{x_4} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</script>
</div>
<p>Carrying out the elimination, (not shown here), we get intermediate factors</p>
<div>
<div class="MathJax_Preview">
\{\tau_1(x_2, x_3, x_5), \tau_2(x_2, x_3, x_5), \tau_3(x_3, x_5), \tau_4(x_5), \tau_5(x_6)\}
</div>
<script type="math/tex; mode=display">
\{\tau_1(x_2, x_3, x_5), \tau_2(x_2, x_3, x_5), \tau_3(x_3, x_5), \tau_4(x_5), \tau_5(x_6)\}
</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> the maximum factor is of size 4.</p>
<p>d) The added edges are between</p>
<ul>
<li><span><span class="MathJax_Preview">X_2</span><script type="math/tex">X_2</script></span> and <span><span class="MathJax_Preview">X_3</span><script type="math/tex">X_3</script></span> (added by intermediate factors <span><span class="MathJax_Preview">\tau_1(x_2, x_3, x_5)</span><script type="math/tex">\tau_1(x_2, x_3, x_5)</script></span> / <span><span class="MathJax_Preview">\tau_2(x_2, x_3, x_5)</span><script type="math/tex">\tau_2(x_2, x_3, x_5)</script></span>)</li>
<li><span><span class="MathJax_Preview">X_2</span><script type="math/tex">X_2</script></span> and <span><span class="MathJax_Preview">X_5</span><script type="math/tex">X_5</script></span> (added by intermediate factors <span><span class="MathJax_Preview">\tau_1(x_2, x_3, x_5)</span><script type="math/tex">\tau_1(x_2, x_3, x_5)</script></span> / <span><span class="MathJax_Preview">\tau_2(x_2, x_3, x_5)</span><script type="math/tex">\tau_2(x_2, x_3, x_5)</script></span>)</li>
<li><span><span class="MathJax_Preview">X_3</span><script type="math/tex">X_3</script></span> and <span><span class="MathJax_Preview">X_5</span><script type="math/tex">X_5</script></span> (added by intermediate factors <span><span class="MathJax_Preview">\tau_1(x_2, x_3, x_5)</span><script type="math/tex">\tau_1(x_2, x_3, x_5)</script></span> , <span><span class="MathJax_Preview">\tau_2(x_2, x_3, x_5)</span><script type="math/tex">\tau_2(x_2, x_3, x_5)</script></span>, <span><span class="MathJax_Preview">\tau_3(x_3, x_5)</span><script type="math/tex">\tau_3(x_3, x_5)</script></span>)</li>
</ul>
<p>The largest maximal clique is now of size 4 (<span><span class="MathJax_Preview">\{x_2, x_3, x_4, x_5\}</span><script type="math/tex">\{x_2, x_3, x_4, x_5\}</script></span>).</p>
<h2 id="week-6">Week 6<a class="headerlink" href="#week-6" title="Permanent link">&para;</a></h2>
<h2 id="week-8-sampling-and-monte-carlo-methods">Week 8: Sampling and Monte Carlo Methods<a class="headerlink" href="#week-8-sampling-and-monte-carlo-methods" title="Permanent link">&para;</a></h2>
<h3 id="question-1_5">Question 1<a class="headerlink" href="#question-1_5" title="Permanent link">&para;</a></h3>
<p>Given some data <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1} \sim p(x)</span><script type="math/tex">\{x^{(r)}\}^R_{r=1} \sim p(x)</script></span>, the simple Monte Carlo estimator is</p>
<div>
<div class="MathJax_Preview">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</div>
<script type="math/tex; mode=display">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</script>
</div>
<p>(a) Show that this estimator is unbiased.</p>
<p>(b) Show that, as the number of samples of <span><span class="MathJax_Preview">R</span><script type="math/tex">R</script></span> increases, the variance of <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> will decrease proportional to <span><span class="MathJax_Preview">\frac{1}{R}</span><script type="math/tex">\frac{1}{R}</script></span>.</p>
<p><strong>ANSWER</strong></p>
<p>(a) To show that <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> is an unbiased estimator of <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span> we must show that for random samples <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1}</span><script type="math/tex">\{x^{(r)}\}^R_{r=1}</script></span> generated from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>, the expectation of <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> is <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span>:</p>
<div>
<div class="MathJax_Preview">
\mathbb E [\hat \Phi]_{x \sim p(\{x^{(r)}\}^R_{r=1})} = \mathbb E \bigg [ \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \bigg ] \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E \big [ \phi(x^{(r)}) \big ]  \\
= \frac{1}{R} \sum_{r=1}^R \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \frac{R}{R} \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \Phi \quad \square
</div>
<script type="math/tex; mode=display">
\mathbb E [\hat \Phi]_{x \sim p(\{x^{(r)}\}^R_{r=1})} = \mathbb E \bigg [ \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \bigg ] \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E \big [ \phi(x^{(r)}) \big ]  \\
= \frac{1}{R} \sum_{r=1}^R \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \frac{R}{R} \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \Phi \quad \square
</script>
</div>
<p>(b)</p>
<div>
<div class="MathJax_Preview">
\text{var}[\hat \Phi] = \text{var} \bigg [ \frac{1}{R}\sum^R_{r=1}\phi(x^{(r)}) \bigg ] \\
= \frac{1}{R^2} \text{var} \bigg [\sum^R_{r=1}\phi(x^{(r)}) \bigg ]\\
= \frac{1}{R^2} \sum^R_{r=1} \text{var} \bigg [\phi(x^{(r)}) \bigg ] \\
= \frac{R}{R^2} \text{var} [\phi(x) ] \\
= \frac{1}{R} \text{var} [\phi(x) ] \quad \square
</div>
<script type="math/tex; mode=display">
\text{var}[\hat \Phi] = \text{var} \bigg [ \frac{1}{R}\sum^R_{r=1}\phi(x^{(r)}) \bigg ] \\
= \frac{1}{R^2} \text{var} \bigg [\sum^R_{r=1}\phi(x^{(r)}) \bigg ]\\
= \frac{1}{R^2} \sum^R_{r=1} \text{var} \bigg [\phi(x^{(r)}) \bigg ] \\
= \frac{R}{R^2} \text{var} [\phi(x) ] \\
= \frac{1}{R} \text{var} [\phi(x) ] \quad \square
</script>
</div>
<h3 id="question-2_2">Question 2<a class="headerlink" href="#question-2_2" title="Permanent link">&para;</a></h3>
<p>Starting from <span><span class="MathJax_Preview">\Phi = \int \phi(x)p(x)dx</span><script type="math/tex">\Phi = \int \phi(x)p(x)dx</script></span>, derive the importance weighted estimator <span><span class="MathJax_Preview">\hat \Phi_{iw}</span><script type="math/tex">\hat \Phi_{iw}</script></span> given</p>
<div>
<div class="MathJax_Preview">
\tilde w_r = \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}
</div>
<script type="math/tex; mode=display">
\tilde w_r = \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}
</script>
</div>
<p><strong>ANSWER</strong></p>
<div>
<div class="MathJax_Preview">\begin{align}
\Phi &amp;= \int \phi(x)p(x)dx \\
&amp;= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x)dx \\
&amp;\approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)})\frac{p(x^{(r)})}{q(x^{(r)})} \\
&amp;= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})} \\
&amp;= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\
&amp;= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\
&amp;= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  w_r \\
&amp;= \hat \Phi_{iw}
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\Phi &= \int \phi(x)p(x)dx \\
&= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x)dx \\
&\approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)})\frac{p(x^{(r)})}{q(x^{(r)})} \\
&= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})} \\
&= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\
&= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\
&= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  w_r \\
&= \hat \Phi_{iw}
\end{align}</script>
</div>
<p>where <span><span class="MathJax_Preview">\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R \tilde w_r</span><script type="math/tex">\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R \tilde w_r</script></span>, <span><span class="MathJax_Preview">w_r = \frac{\tilde w_r}{\sum_{r=1}^R \tilde w_r}</span><script type="math/tex">w_r = \frac{\tilde w_r}{\sum_{r=1}^R \tilde w_r}</script></span> and <span><span class="MathJax_Preview">\hat \Phi_{iw}</span><script type="math/tex">\hat \Phi_{iw}</script></span> is our importance weighted estimator.</p>
<h2 id="week-9-hidden-markov-models">Week 9: Hidden Markov Models<a class="headerlink" href="#week-9-hidden-markov-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_6">Question 1<a class="headerlink" href="#question-1_6" title="Permanent link">&para;</a></h3>
<p>Assume <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a discrete random variable with <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> states. How many parameters are needed to parameterize</p>
<p>(a) <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span>?</p>
<p>(b) A <em>first-order</em> Markov chain?</p>
<p>(c) An <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span><em>-ordered</em> Markov chain?</p>
<p><strong>ANSWER</strong></p>
<p>(a) <span><span class="MathJax_Preview">k-1</span><script type="math/tex">k-1</script></span>, as the last state is implicit.</p>
<p>(b) <span><span class="MathJax_Preview">k(k-1)</span><script type="math/tex">k(k-1)</script></span>, as we need <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> number of parameters for each parameter of <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span></p>
<p>(c) <span><span class="MathJax_Preview">k^m(k-1)</span><script type="math/tex">k^m(k-1)</script></span>, as we need <span><span class="MathJax_Preview">k^m</span><script type="math/tex">k^m</script></span> number of parameters for each parameter of <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span></p>
<h3 id="question-2_3">Question 2<a class="headerlink" href="#question-2_3" title="Permanent link">&para;</a></h3>
<p>Say we have the following simple chain</p>
<p><img alt="" src="../img/lecture_8_3.png" /></p>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">x_t \in [N, Z, A]</span><script type="math/tex">x_t \in [N, Z, A]</script></span></li>
<li><span><span class="MathJax_Preview">z_t \in [H, S]</span><script type="math/tex">z_t \in [H, S]</script></span></li>
</ul>
<p>where our observed states are whether or not we are watching Netflix (<span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>), sleeping (<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>), or working on the assignment (<span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>) and our hidden states are whether we are happy (<span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span>) or sad (<span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span>). Say futher that we are given the initial (<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>), transition (<span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>), and emission probabilities (<span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span>)</p>
<p><center></p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.70</td>
</tr>
<tr>
<td>S</td>
<td>0.30</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span></th>
<th>N</th>
<th>Z</th>
<th>A</th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.40</td>
<td>0.50</td>
<td>0.10</td>
</tr>
<tr>
<td>S</td>
<td>0.10</td>
<td>0.30</td>
<td>0.60</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th>T</th>
<th>H</th>
<th>S</th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.80</td>
<td>0.20</td>
</tr>
<tr>
<td>S</td>
<td>0.10</td>
<td>0.90</td>
</tr>
</tbody>
</table>
<p></center></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is the <em>rows</em> of these tables that need to sum to 1, not the columns!</p>
</div>
<p>From these conditional probabilities, compute</p>
<ul>
<li><span><span class="MathJax_Preview">p(z_3 = H | z_1 = S) = ?</span><script type="math/tex">p(z_3 = H | z_1 = S) = ?</script></span></li>
<li><span><span class="MathJax_Preview">p(x_3 = A | z_1 = S) = ?</span><script type="math/tex">p(x_3 = A | z_1 = S) = ?</script></span></li>
</ul>
<p><strong>ANSWER</strong></p>
<div>
<div class="MathJax_Preview">
p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\
= (0.80)(0.1) + (0.10)(0.90) \\
= 0.17 \\
</div>
<script type="math/tex; mode=display">
p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\
= (0.80)(0.1) + (0.10)(0.90) \\
= 0.17 \\
</script>
</div>
<p>and</p>
<div>
<div class="MathJax_Preview">
p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\
= (0.10)(0.17) + (0.60)(1 - 0.17) \\
= 0.515
</div>
<script type="math/tex; mode=display">
p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\
= (0.10)(0.17) + (0.60)(1 - 0.17) \\
= 0.515
</script>
</div>
<h2 id="week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi">Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)<a class="headerlink" href="#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi" title="Permanent link">&para;</a></h2>
<h3 id="question-1_7">Question 1<a class="headerlink" href="#question-1_7" title="Permanent link">&para;</a></h3>
<p>Starting from the Kullback–Leibler divergence (<span><span class="MathJax_Preview">D_{KL}</span><script type="math/tex">D_{KL}</script></span>), derive the Evidence Lower Bound (ELBO) for a true distribution <span><span class="MathJax_Preview">p_\theta (z | x)</span><script type="math/tex">p_\theta (z | x)</script></span> and approximate distribution <span><span class="MathJax_Preview">q_\phi(z|x)</span><script type="math/tex">q_\phi(z|x)</script></span> and show that maximizing the ELBO is equivalent to minimizing <span><span class="MathJax_Preview">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</span><script type="math/tex">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</script></span>.</p>
<p><strong>ANSWER</strong></p>
<div>
<div class="MathJax_Preview">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &amp;= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z | x)} \\
  &amp;= E_{z_\phi \sim q_\phi} \Bigg [ \log \Bigg ( q_\phi(z | x) \cdot \frac{p_\theta(x)}{p_\theta(z, x)} \Bigg ) \Bigg ] \\
  &amp;= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z, x)}  + E_{z_\phi \sim q_\phi} \log p_\theta(x) \\
  &amp;= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z | x)} \\
  &= E_{z_\phi \sim q_\phi} \Bigg [ \log \Bigg ( q_\phi(z | x) \cdot \frac{p_\theta(x)}{p_\theta(z, x)} \Bigg ) \Bigg ] \\
  &= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z, x)}  + E_{z_\phi \sim q_\phi} \log p_\theta(x) \\
  &= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
\end{align}</script>
</div>
<p>Where <span><span class="MathJax_Preview">\mathcal L(\theta, \phi ; x)</span><script type="math/tex">\mathcal L(\theta, \phi ; x)</script></span> is the <strong>ELBO</strong>. Rearranging,</p>
<div>
<div class="MathJax_Preview">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &amp;= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
  \Rightarrow \mathcal L(\theta, \phi ; x) + D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &amp;= \log p_\theta(x) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
  \Rightarrow \mathcal L(\theta, \phi ; x) + D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &= \log p_\theta(x) \\
\end{align}</script>
</div>
<p>because <span><span class="MathJax_Preview">D_{KL}(q_\phi (z | x) || p_\theta (z | x)) \ge 0</span><script type="math/tex">D_{KL}(q_\phi (z | x) || p_\theta (z | x)) \ge 0</script></span></p>
<div>
<div class="MathJax_Preview">
\mathcal L(\theta, \phi ; x) \le \log p_\theta(x)
</div>
<script type="math/tex; mode=display">
\mathcal L(\theta, \phi ; x) \le \log p_\theta(x)
</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> maximizing the ELBO <span><span class="MathJax_Preview">\Rightarrow</span><script type="math/tex">\Rightarrow</script></span> minimizing <span><span class="MathJax_Preview">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</span><script type="math/tex">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</script></span>.</p>
<h2 id="week-12">Week 12<a class="headerlink" href="#week-12" title="Permanent link">&para;</a></h2>
<h3 id="question-1_8">Question 1<a class="headerlink" href="#question-1_8" title="Permanent link">&para;</a></h3>
<p>Compare and contrast <em>explicit</em> vs. <em>implicit</em> density models.</p>
<p><strong>ANSWER</strong></p>
<p><strong>Explicit density models</strong> define an explicit density function <span><span class="MathJax_Preview">p_{model}\theta(x ; \theta)</span><script type="math/tex">p_{model}\theta(x ; \theta)</script></span>. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill.</p>
<p>e.g. for the i.i.d case:</p>
<div>
<div class="MathJax_Preview">
\hat \theta_{MLE} = \underset{\theta}{\operatorname{argmax}} \sum \log P_{model} (x^{(i)} ; \theta)
</div>
<script type="math/tex; mode=display">
\hat \theta_{MLE} = \underset{\theta}{\operatorname{argmax}} \sum \log P_{model} (x^{(i)} ; \theta)
</script>
</div>
<p>which is equivalent to the minimizing the KL divergence between <span><span class="MathJax_Preview">P_{data}(x)</span><script type="math/tex">P_{data}(x)</script></span> and <span><span class="MathJax_Preview">P_{model}(x ; \theta)</span><script type="math/tex">P_{model}(x ; \theta)</script></span></p>
<div>
<div class="MathJax_Preview">
\underset{\theta}{\operatorname{argmin}} D_{KL}(P_{data}(x) || P_{model}(x ; \theta))
</div>
<script type="math/tex; mode=display">
\underset{\theta}{\operatorname{argmin}} D_{KL}(P_{data}(x) || P_{model}(x ; \theta))
</script>
</div>
<p>when <span><span class="MathJax_Preview">p_{model}\theta(x ; \theta)</span><script type="math/tex">p_{model}\theta(x ; \theta)</script></span> is intractable, we use variational (e.g. VAEs) or MCMC approximations to compute the likelihood.</p>
<p>In contrast, <strong>implicit density models</strong> are trained without explicitly defining a density function. The way we interact <span><span class="MathJax_Preview">P_{model}</span><script type="math/tex">P_{model}</script></span> is through samples. An example is a General Adversarial Network (GAN).</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../sample_midterm/" title="Sample Midterm" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Sample Midterm
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>