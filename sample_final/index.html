



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /sample_final/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Sample Final - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#sample-final" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Sample Final
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_8/" title="Week 8" class="md-nav__link">
      Week 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_9/" title="Week 9" class="md-nav__link">
      Week 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_11/" title="Week 11" class="md-nav__link">
      Week 11
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_12/" title="Week 12" class="md-nav__link">
      Week 12
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lectures/week_13/" title="Week 13" class="md-nav__link">
      Week 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../sample_midterm/" title="Sample Midterm" class="md-nav__link">
      Sample Midterm
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Sample Final
      </label>
    
    <a href="./" title="Sample Final" class="md-nav__link md-nav__link--active">
      Sample Final
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#week-1" title="Week 1" class="md-nav__link">
    Week 1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorial-1" title="Tutorial 1" class="md-nav__link">
    Tutorial 1
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-2-introduction-to-probabilistic-models" title="Week 2: Introduction to Probabilistic Models" class="md-nav__link">
    Week 2: Introduction to Probabilistic Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-3-directed-graphical-models" title="Week 3: Directed Graphical Models" class="md-nav__link">
    Week 3: Directed Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_2" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-4-undirected-graphical-models" title="Week 4: Undirected Graphical Models" class="md-nav__link">
    Week 4: Undirected Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_3" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_1" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3_1" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-5-exact-inference" title="Week 5: Exact Inference" class="md-nav__link">
    Week 5: Exact Inference
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_4" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-6-variational-inference" title="Week 6: Variational Inference" class="md-nav__link">
    Week 6: Variational Inference
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_5" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-8-sampling-and-monte-carlo-methods" title="Week 8: Sampling and Monte Carlo Methods" class="md-nav__link">
    Week 8: Sampling and Monte Carlo Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_6" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_3" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-9-hidden-markov-models" title="Week 9: Hidden Markov Models" class="md-nav__link">
    Week 9: Hidden Markov Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_7" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_4" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi" title="Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)" class="md-nav__link">
    Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_8" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-12-generative-adversarial-networks-gans" title="Week 12: Generative Adversarial Networks (GANs)" class="md-nav__link">
    Week 12: Generative Adversarial Networks (GANs)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_9" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_5" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3_2" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4_1" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#week-1" title="Week 1" class="md-nav__link">
    Week 1
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorial-1" title="Tutorial 1" class="md-nav__link">
    Tutorial 1
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-2-introduction-to-probabilistic-models" title="Week 2: Introduction to Probabilistic Models" class="md-nav__link">
    Week 2: Introduction to Probabilistic Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_1" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-3-directed-graphical-models" title="Week 3: Directed Graphical Models" class="md-nav__link">
    Week 3: Directed Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_2" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-4-undirected-graphical-models" title="Week 4: Undirected Graphical Models" class="md-nav__link">
    Week 4: Undirected Graphical Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_3" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_1" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3_1" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-5-exact-inference" title="Week 5: Exact Inference" class="md-nav__link">
    Week 5: Exact Inference
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_4" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-6-variational-inference" title="Week 6: Variational Inference" class="md-nav__link">
    Week 6: Variational Inference
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_5" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_2" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-8-sampling-and-monte-carlo-methods" title="Week 8: Sampling and Monte Carlo Methods" class="md-nav__link">
    Week 8: Sampling and Monte Carlo Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_6" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_3" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-9-hidden-markov-models" title="Week 9: Hidden Markov Models" class="md-nav__link">
    Week 9: Hidden Markov Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_7" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_4" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi" title="Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)" class="md-nav__link">
    Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_8" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#week-12-generative-adversarial-networks-gans" title="Week 12: Generative Adversarial Networks (GANs)" class="md-nav__link">
    Week 12: Generative Adversarial Networks (GANs)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#question-1_9" title="Question 1" class="md-nav__link">
    Question 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-2_5" title="Question 2" class="md-nav__link">
    Question 2
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-3_2" title="Question 3" class="md-nav__link">
    Question 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-4_1" title="Question 4" class="md-nav__link">
    Question 4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/sample_final.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="sample-final">Sample Final<a class="headerlink" href="#sample-final" title="Permanent link">&para;</a></h1>
<p>These are question pulled from the lecture, assignments and the sample midterm, alongside questions that were written based on the study guide. These were <em>not</em> given by an instructor and are merely guesses as to what kind of questions might be on the final.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the study guide <a href="http://www.cs.toronto.edu/~jessebett/CSC412/content/Final_Topics/final_topics.pdf">here</a>.</p>
</div>
<h2 id="week-1">Week 1<a class="headerlink" href="#week-1" title="Permanent link">&para;</a></h2>
<h2 id="tutorial-1">Tutorial 1<a class="headerlink" href="#tutorial-1" title="Permanent link">&para;</a></h2>
<h3 id="question-1">Question 1<a class="headerlink" href="#question-1" title="Permanent link">&para;</a></h3>
<p>Recall that the definition of an exponential family model is:</p>
<div>
<div class="MathJax_Preview">
f(x | \eta) = h(x)g(\eta)\exp(\eta^TT(x))
</div>
<script type="math/tex; mode=display">
f(x | \eta) = h(x)g(\eta)\exp(\eta^TT(x))
</script>
</div>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> are the parameters</li>
<li><span><span class="MathJax_Preview">T(x)</span><script type="math/tex">T(x)</script></span> are the sufficient statistics</li>
<li><span><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span> is the base measure</li>
<li><span><span class="MathJax_Preview">g(\eta)</span><script type="math/tex">g(\eta)</script></span> is the normalizing constant</li>
</ul>
<p>Consider the univariate Gaussian, with mean <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and precision <span><span class="MathJax_Preview">\lambda = \frac{1}{\sigma^2}</span><script type="math/tex">\lambda = \frac{1}{\sigma^2}</script></span></p>
<div>
<div class="MathJax_Preview">
p(D | \mu \lambda) = \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(-\frac{\lambda}{2}(x_i - \mu)^2)
</div>
<script type="math/tex; mode=display">
p(D | \mu \lambda) = \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(-\frac{\lambda}{2}(x_i - \mu)^2)
</script>
</div>
<p>What are <span><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> and <span><span class="MathJax_Preview">T(x)</span><script type="math/tex">T(x)</script></span> for this distribution when represented in exponential family form?</p>
<p><strong>ANSWER</strong></p>
<p>Start by expanding the terms in the exponent</p>
<div>
<div class="MathJax_Preview">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}} \exp(\sum_{i=1}^N -\frac{\lambda}{2}x_i^2 + \lambda u x_i - \frac{\lambda}{2}\mu^2) \\
</div>
<script type="math/tex; mode=display">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}} \exp(\sum_{i=1}^N -\frac{\lambda}{2}x_i^2 + \lambda u x_i - \frac{\lambda}{2}\mu^2) \\
</script>
</div>
<p>from here, we can rearrange the exponent into <span><span class="MathJax_Preview">\eta^TT(x)</span><script type="math/tex">\eta^TT(x)</script></span></p>
<div>
<div class="MathJax_Preview">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(\sum_{i=1}^N - \frac{\lambda}{2}\mu^2)\exp(\begin{bmatrix}\lambda u &amp; -\frac{\lambda}{2} &amp; \dotsc &amp; \lambda u &amp; -\frac{\lambda}{2}\end{bmatrix} \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}) \\
</div>
<script type="math/tex; mode=display">
= \prod^N_{i=1}(\frac{\lambda}{2\pi})^{\frac{1}{2}}\exp(\sum_{i=1}^N - \frac{\lambda}{2}\mu^2)\exp(\begin{bmatrix}\lambda u & -\frac{\lambda}{2} & \dotsc & \lambda u & -\frac{\lambda}{2}\end{bmatrix} \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}) \\
</script>
</div>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">\eta^T = \begin{bmatrix}\lambda u &amp; -\frac{\lambda}{2} &amp; \dotsc &amp; \lambda u &amp; -\frac{\lambda}{2}\end{bmatrix}</span><script type="math/tex">\eta^T = \begin{bmatrix}\lambda u & -\frac{\lambda}{2} & \dotsc & \lambda u & -\frac{\lambda}{2}\end{bmatrix}</script></span></li>
<li><span><span class="MathJax_Preview">T(x) = \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}</span><script type="math/tex">T(x) = \begin{bmatrix}x_1 \\ x_1^2 \\ \vdots \\ x_N \\ x_N^2\end{bmatrix}</script></span></li>
</ul>
<h2 id="week-2-introduction-to-probabilistic-models">Week 2: Introduction to Probabilistic Models<a class="headerlink" href="#week-2-introduction-to-probabilistic-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_1">Question 1<a class="headerlink" href="#question-1_1" title="Permanent link">&para;</a></h3>
<p>In this question, we'll fit a naive Bayes model to the MNIST digits using maximum likelihood.
Naive Bayes defines the joint probability of the each datapoint <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and its class label <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> as follows:</p>
<p>\begin{align}
p(x, c | \theta, \pi) = p(c | \pi) p(x | c, \theta_c) = p(c | \pi) \prod_{d=1}^{784} p( x_d | c, \theta_{cd})
\end{align}
For binary data, we can use the Bernoulli likelihood:
\begin{align}
p( x_d | c, \theta_{cd}) = Ber(x_d | \theta_{cd}) = \theta_{cd}^{x_d} ( 1 - \theta_{cd})^{(1 - x_d)}
\end{align}
Which is just a way of expressing that <span><span class="MathJax_Preview">p(x_d = 1 | c, \theta_{cd}) = \theta_{cd}</span><script type="math/tex">p(x_d = 1 | c, \theta_{cd}) = \theta_{cd}</script></span>.</p>
<p>For <span><span class="MathJax_Preview">p(c | \pi)</span><script type="math/tex">p(c | \pi)</script></span>, we can just use a categorical distribution:
\begin{align}
p(c | \pi) = Cat(c|\pi) = \pi_c
\end{align}
Note that we need <span><span class="MathJax_Preview">\sum_{i=0}^9 \pi_{i} = 1</span><script type="math/tex">\sum_{i=0}^9 \pi_{i} = 1</script></span>.</p>
<p>(a) Derive the <em>maximum likelihood estimate</em> (MLE) for the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>. Hint: We saw in lecture that MLE can be thought of as 'counts' for the data, so what should <span><span class="MathJax_Preview">\hat \theta_{cd}</span><script type="math/tex">\hat \theta_{cd}</script></span> be counting?</p>
<p>(b) Derive the <em>maximum a posteriori</em> (MAP) estimate for the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>, using a <span><span class="MathJax_Preview">Beta(2, 2)</span><script type="math/tex">Beta(2, 2)</script></span> prior on each <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.  Hint: it has a simple final form, and you can ignore the Beta normalizing constant.</p>
<p><strong>ANSWER</strong></p>
<p>(a) The maximum likelihood estimate of the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> for class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> is given by</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \prod_{i=1}^N \prod_{d=1}^{784}\theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})}
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \prod_{i=1}^N \prod_{d=1}^{784}\theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})}
</script>
</div>
<p>where <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is the number of training examples. Taking the log, we get</p>
<div>
<div class="MathJax_Preview">\begin{align*}
&amp;= \operatorname*{argmax}_{\theta_c} \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} \log \bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \bigg )  \Bigg ] \\
&amp;= \operatorname*{argmax}_{\theta_c}  \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} x_d^{(i)} \log \theta_{cd} + (1-x_d^{(i)}) \log (1 - \theta_{cd})  \Bigg ] \\
&amp; = \operatorname*{argmax}_{\theta_c} \Bigg [  n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c})  \Bigg ] \\
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
&= \operatorname*{argmax}_{\theta_c} \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} \log \bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \bigg )  \Bigg ] \\
&= \operatorname*{argmax}_{\theta_c}  \Bigg [ \sum_{i=1}^N \sum_{d=1}^{784} x_d^{(i)} \log \theta_{cd} + (1-x_d^{(i)}) \log (1 - \theta_{cd})  \Bigg ] \\
& = \operatorname*{argmax}_{\theta_c} \Bigg [  n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c})  \Bigg ] \\
\end{align*}</script>
</div>
<p>where <span><span class="MathJax_Preview">n_c^{d=1}</span><script type="math/tex">n_c^{d=1}</script></span> is a vector of counts containing the number of training examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> where <span><span class="MathJax_Preview">d=1</span><script type="math/tex">d=1</script></span> for each pixel dimension <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> in 784, <span><span class="MathJax_Preview">n_c^{d=0}</span><script type="math/tex">n_c^{d=0}</script></span> is the corresponding count vector for the number of training examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> where <span><span class="MathJax_Preview">d = 0</span><script type="math/tex">d = 0</script></span>, and <span><span class="MathJax_Preview">\cdot</span><script type="math/tex">\cdot</script></span> denotes the dot product. Taking the derivative of this expression and setting it to 0, we can solve for the MLE of the parameters</p>
<div>
<div class="MathJax_Preview">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c}  \bigg [ n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c}) \bigg ] &amp;= 0 \\
\frac{n_c^{d=1}}{\theta_{c}} - \frac{n_c^{d=0}}{1 - \theta_{c}} &amp; = 0
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c}  \bigg [ n_c^{d=1} \cdot \log \theta_{c} + n_c^{d=0} \cdot \log (1 - \theta_{c}) \bigg ] &= 0 \\
\frac{n_c^{d=1}}{\theta_{c}} - \frac{n_c^{d=0}}{1 - \theta_{c}} & = 0
\end{align*}</script>
</div>
<p>Rearranging, we get</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \frac{n_c^{d=1}}{N}
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \frac{n_c^{d=1}}{n_c^{d=1} + n_c^{d=0}} = \frac{n_c^{d=1}}{N}
</script>
</div>
<p>therefore, the MLE for the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span>, <span><span class="MathJax_Preview">\hat \theta_c</span><script type="math/tex">\hat \theta_c</script></span>, is given by the number of examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> where <span><span class="MathJax_Preview">d=1</span><script type="math/tex">d=1</script></span> divided by the total number of examples of class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span>, as expected.</p>
<p>(b) The prior probability of our class-conditional pixel means, <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> for class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> is given by</p>
<div>
<div class="MathJax_Preview">
f(\theta_c; \alpha, \beta) = f(\theta_c; 2, 2) = \frac{1}{B(2, 2)}\theta_c^{2 - 1}(1-\theta_c)^{2-1} = \theta_c(1-\theta_c)
</div>
<script type="math/tex; mode=display">
f(\theta_c; \alpha, \beta) = f(\theta_c; 2, 2) = \frac{1}{B(2, 2)}\theta_c^{2 - 1}(1-\theta_c)^{2-1} = \theta_c(1-\theta_c)
</script>
</div>
<p>where we have ignored the Beta normalizing constant. The MAP estimate of the class-conditional pixel means <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> for class <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> is given by</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \Bigg [ \theta_c(1-\theta_c) \prod_{i=1}^N \prod_{d=1}^{784} \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ]
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \operatorname*{argmax}_{\theta_c} \Bigg [ \theta_c(1-\theta_c) \prod_{i=1}^N \prod_{d=1}^{784} \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ]
</script>
</div>
<p>Taking the log, we get</p>
<div>
<div class="MathJax_Preview">\begin{align*}
\hat \theta_c &amp;= \operatorname*{argmax}_{\theta_c} \Bigg [ \log \big ( \theta_c(1-\theta_c) \big ) + \sum_{i=1}^N \sum_{d=1}^{784} \log \Bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ) \Bigg ] \\
&amp;= \operatorname*{argmax}_{\theta_c} \Bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \Bigg ]
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
\hat \theta_c &= \operatorname*{argmax}_{\theta_c} \Bigg [ \log \big ( \theta_c(1-\theta_c) \big ) + \sum_{i=1}^N \sum_{d=1}^{784} \log \Bigg ( \theta^{x_d^{(i)}}_{cd}(1 - \theta_{cd})^{(1-x_d^{(i)})} \Bigg ) \Bigg ] \\
&= \operatorname*{argmax}_{\theta_c} \Bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \Bigg ]
\end{align*}</script>
</div>
<p>taking the derivative of this expression and setting it to 0, we can solve for the MAP estimate of the parameters</p>
<div>
<div class="MathJax_Preview">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c} \bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \bigg ] &amp;= 0 \\
\frac{1 + n_c^{d=1}}{\theta_c} - \frac{1 + n_c^{d=0}}{(1-\theta_c)} &amp;= 0 \\
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*}
\Rightarrow \frac{\partial}{\partial \theta_c} \bigg [ \log(\theta_c) + \log(1-\theta_c) + n_c^{d=1} \cdot \log(\theta_c) + n_c^{d=0} \cdot \log(1 - \theta_c) \bigg ] &= 0 \\
\frac{1 + n_c^{d=1}}{\theta_c} - \frac{1 + n_c^{d=0}}{(1-\theta_c)} &= 0 \\
\end{align*}</script>
</div>
<p>Rearranging, we get</p>
<div>
<div class="MathJax_Preview">
\hat \theta_c = \frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \frac{n_c^{d=1} + 1}{N + 2}
</div>
<script type="math/tex; mode=display">
\hat \theta_c = \frac{n_c^{d=1} + 1}{n_c^{d=1} + n_c^{d=0} + 2} = \frac{n_c^{d=1} + 1}{N + 2}
</script>
</div>
<h2 id="week-3-directed-graphical-models">Week 3: Directed Graphical Models<a class="headerlink" href="#week-3-directed-graphical-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_2">Question 1<a class="headerlink" href="#question-1_2" title="Permanent link">&para;</a></h3>
<p>When we condition on <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, are <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> independent?</p>
<p>(a)</p>
<p><img alt="" src="../img/lecture_3_5.png" /></p>
<p>(b)</p>
<p><img alt="" src="../img/lecture_3_6.png" /></p>
<p>(c)</p>
<p><img alt="" src="../img/lecture_3_7.png" /></p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(x)P(y|x)P(z|y)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(x)P(y|x)P(z|y)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(z | x, y) &amp;= \frac{P(x, y, z)}{P(x, y)} \\
&amp;= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
&amp;= P(z | y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(z | x, y) &= \frac{P(x, y, z)}{P(x, y)} \\
&= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
&= P(z | y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(z | x, y) = P(z | y)</span><script type="math/tex">P(z | x, y) = P(z | y)</script></span> and so by <span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span>, <span><span class="MathJax_Preview">x \bot z | y</span><script type="math/tex">x \bot z | y</script></span>.</p>
<p>(b)</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(y)P(x|y)P(z|y)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(y)P(x|y)P(z|y)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(x, z | y) &amp;= \frac{P(x, y, z)}{P(y)} \\
&amp;= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
&amp;= P(x|y)P(z|y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(x, z | y) &= \frac{P(x, y, z)}{P(y)} \\
&= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
&= P(x|y)P(z|y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(x, z| y) = P(x|y)P(z|y)</span><script type="math/tex">P(x, z| y) = P(x|y)P(z|y)</script></span> and so by <span><span class="MathJax_Preview">\star</span><script type="math/tex">\star</script></span>, <span><span class="MathJax_Preview">x \bot z | y</span><script type="math/tex">x \bot z | y</script></span>.</p>
<p>(c)</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(x)P(z)P(y|x, z)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(x)P(z)P(y|x, z)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(z | x, y) &amp;= \frac{P(x)P(z)P(y | x,  z)}{P(x)P(y|x)} \\
&amp;= \frac{P(z)P(y | x,  z)}{P(y|x)}  \\
&amp;\not = P(z|y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(z | x, y) &= \frac{P(x)P(z)P(y | x,  z)}{P(x)P(y|x)} \\
&= \frac{P(z)P(y | x,  z)}{P(y|x)}  \\
&\not = P(z|y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(z | x, y) \not = P(z|y)</span><script type="math/tex">P(z | x, y) \not = P(z|y)</script></span> and so by <span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span>, <span><span class="MathJax_Preview">x \not \bot z | y</span><script type="math/tex">x \not \bot z | y</script></span>.</p>
<p>In fact, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> are <em>marginally independent</em>, but given <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> they are <em>conditionally dependent</em>. This important effect is called explaining away (<a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox">Berkson’s paradox</a>).</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Imaging flipping two coins independently, represented by events <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>. Furthermore, let <span><span class="MathJax_Preview">y=1</span><script type="math/tex">y=1</script></span> if the coins come up the same and <span><span class="MathJax_Preview">y=0</span><script type="math/tex">y=0</script></span> if they come up differently. Clearly, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> are independent, but if I tell you <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, they become coupled!</p>
</div>
<h3 id="question-2">Question 2<a class="headerlink" href="#question-2" title="Permanent link">&para;</a></h3>
<p>(a) In the following graph, is <span><span class="MathJax_Preview">x_1 \bot x_6 | \{x_2, x_3\}</span><script type="math/tex">x_1 \bot x_6 | \{x_2, x_3\}</script></span>?</p>
<p><img alt="" src="../img/lecture_3_12.png" /></p>
<p>(b) In the following graph, is <span><span class="MathJax_Preview">x_2 \bot x_3 | \{x_1, x_6\}</span><script type="math/tex">x_2 \bot x_3 | \{x_1, x_6\}</script></span>?</p>
<p><img alt="" src="../img/lecture_3_14.png" /></p>
<p><strong>ANSWER</strong></p>
<p>(a) Yes, by the Bayes Balls algorithm.</p>
<p><img alt="" src="../img/lecture_3_13.png" /></p>
<p>(b) No, by the Bayes Balls algorithm.</p>
<p><img alt="" src="../img/lecture_3_15.png" /></p>
<h3 id="question-3">Question 3<a class="headerlink" href="#question-3" title="Permanent link">&para;</a></h3>
<p>Consider the following directed graphical model:</p>
<p><img alt="" src="../img/sample_midterm_1.png" /></p>
<p>(a) List all variables that are independent of <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> given evidence on <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span></p>
<p>(b) Write down the factorized normalized joint distribution that this graphical model represents.</p>
<p>(c) If each node is a single discrete random variable in <span><span class="MathJax_Preview">{1, ..., K}</span><script type="math/tex">{1, ..., K}</script></span> how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set?</p>
<p><strong>ANSWER</strong></p>
<p>(a) By Bayes' Balls, no variables are conditionally independent of <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> given evidence on <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>.</p>
<p><img alt="" src="../img/sample_midterm_2.png" /></p>
<p>(b)</p>
<div>
<div class="MathJax_Preview">
p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H)
</div>
<script type="math/tex; mode=display">
p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H)
</script>
</div>
<p>(c) For each node (random variable) there is <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> states. There are <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> possible configurations where <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> is the number of states and <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> the number of nodes (<span><span class="MathJax_Preview">x_{\pi_i}</span><script type="math/tex">x_{\pi_i}</script></span>)</p>
<div>
<div class="MathJax_Preview">
\therefore \text{number of possible configurations} = k^9
</div>
<script type="math/tex; mode=display">
\therefore \text{number of possible configurations} = k^9
</script>
</div>
<h3 id="question-4">Question 4<a class="headerlink" href="#question-4" title="Permanent link">&para;</a></h3>
<p>Consider the Hidden Markov Model</p>
<p><img alt="" src="../img/sample_midterm_4.png" /></p>
<p>(a) Assume you are able to sample from these conditional distributions, i.e.</p>
<div>
<div class="MathJax_Preview">
x_i \sim p(X_i \ | \ \text{parents of } X_i)
</div>
<script type="math/tex; mode=display">
x_i \sim p(X_i \ | \ \text{parents of } X_i)
</script>
</div>
<p>Write down a step-by-step process to produce a sample observation from this model, i.e. <span><span class="MathJax_Preview">(x_1, x_2, x_3, ..., x_T)</span><script type="math/tex">(x_1, x_2, x_3, ..., x_T)</script></span> in terms of samples from the individual factors.</p>
<p><strong>ANSWER</strong></p>
<p>We want to sample a sequence of observations <span><span class="MathJax_Preview">x_1, x_2, x_3, ..., x_T</span><script type="math/tex">x_1, x_2, x_3, ..., x_T</script></span> from the model according to</p>
<div>
<div class="MathJax_Preview">
x_{1:T} \sim \prod_{t=1}^T p(X_t \ | \ \text{parents of } X_t)
</div>
<script type="math/tex; mode=display">
x_{1:T} \sim \prod_{t=1}^T p(X_t \ | \ \text{parents of } X_t)
</script>
</div>
<p>since observations <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> are independent of one another. Notice that this forms a chain, with probability</p>
<div>
<div class="MathJax_Preview">
p(x_{1:T}) \sim \bigg [ \prod_{t=1}^T p(X_t | z_t) \bigg ] \bigg [ p(z_1) \prod_{t=2}^T p(Z_t | z_{t-1}) \bigg ]
</div>
<script type="math/tex; mode=display">
p(x_{1:T}) \sim \bigg [ \prod_{t=1}^T p(X_t | z_t) \bigg ] \bigg [ p(z_1) \prod_{t=2}^T p(Z_t | z_{t-1}) \bigg ]
</script>
</div>
<p><em>Step-by-step</em></p>
<ol>
<li>Start with <span><span class="MathJax_Preview">t=1</span><script type="math/tex">t=1</script></span></li>
<li>Sample <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span> according to <span><span class="MathJax_Preview">z_t \sim p(z_1) \prod_{i=2}^{t} p(Z_i | z_{i-1})</span><script type="math/tex">z_t \sim p(z_1) \prod_{i=2}^{t} p(Z_i | z_{i-1})</script></span></li>
<li>Given the sampled <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span>, sample <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> according to <span><span class="MathJax_Preview">x_t \sim \ p(X_t | z_t)</span><script type="math/tex">x_t \sim \ p(X_t | z_t)</script></span></li>
<li>Increment <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> by 1</li>
<li>Repeat steps 2-4 until <span><span class="MathJax_Preview">t=T</span><script type="math/tex">t=T</script></span></li>
</ol>
<h2 id="week-4-undirected-graphical-models">Week 4: Undirected Graphical Models<a class="headerlink" href="#week-4-undirected-graphical-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_3">Question 1<a class="headerlink" href="#question-1_3" title="Permanent link">&para;</a></h3>
<p>(a) State the Global, Local and Pairwise Markov properties used to determine conditional independence in a undirected graphical model.</p>
<p>Given the following UGMs:</p>
<p>(b)</p>
<p><img alt="" src="../img/lecture_4_4.png" /></p>
<p>(c)</p>
<p><img alt="" src="../img/lecture_4_3.png" /></p>
<p>use each to Markov property to give an example of two sets of conditionally independent nodes in the graph.</p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p><em>def</em>. <strong>Global Markov Property</strong> (G): <span><span class="MathJax_Preview">X_A \bot X_B | X_C</span><script type="math/tex">X_A \bot X_B | X_C</script></span> iff <span><span class="MathJax_Preview">X_C</span><script type="math/tex">X_C</script></span> separates <span><span class="MathJax_Preview">X_A</span><script type="math/tex">X_A</script></span> from <span><span class="MathJax_Preview">X_B</span><script type="math/tex">X_B</script></span></p>
<p><em>def</em>. <strong>Local Markov Property (Markov Blanket)</strong> (L): The set of nodes that renders a node <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> conditionally independent of all the other nodes in the graph</p>
<div>
<div class="MathJax_Preview">
t \bot \mathcal V \setminus cl(t) | mb(t)
</div>
<script type="math/tex; mode=display">
t \bot \mathcal V \setminus cl(t) | mb(t)
</script>
</div>
<p><em>def</em>. <strong>Pairwise (Markov) Property</strong> (P): The set of nodes that renders two nodes, <span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> and <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>, conditionally independent of each other.</p>
<div>
<div class="MathJax_Preview">
s \bot t | \mathcal V \setminus \{s, t\} \Leftrightarrow G_{st} = 0
</div>
<script type="math/tex; mode=display">
s \bot t | \mathcal V \setminus \{s, t\} \Leftrightarrow G_{st} = 0
</script>
</div>
<p>(b)</p>
<ul>
<li>Global: <span><span class="MathJax_Preview">\{1, 2\} \bot \{6, 7\} | \{3, 4, 5\}</span><script type="math/tex">\{1, 2\} \bot \{6, 7\} | \{3, 4, 5\}</script></span></li>
<li>Local: <span><span class="MathJax_Preview">1 \bot \text{rest} | \{2, 3\}</span><script type="math/tex">1 \bot \text{rest} | \{2, 3\}</script></span></li>
<li>Pairwise: <span><span class="MathJax_Preview">1 \bot 7 | \text{rest}</span><script type="math/tex">1 \bot 7 | \text{rest}</script></span></li>
</ul>
<p>(c)</p>
<ul>
<li>Global: <span><span class="MathJax_Preview">\{X_1, X_2\} \bot \{X_{15}, X_{20}\} | \{X_3, X_6, X_7\}</span><script type="math/tex">\{X_1, X_2\} \bot \{X_{15}, X_{20}\} | \{X_3, X_6, X_7\}</script></span></li>
<li>Local: <span><span class="MathJax_Preview">1 \bot \text{rest} | \{X_2, X_6\}</span><script type="math/tex">1 \bot \text{rest} | \{X_2, X_6\}</script></span></li>
<li>Pairwise: <span><span class="MathJax_Preview">1 \bot 20 | \text{rest}</span><script type="math/tex">1 \bot 20 | \text{rest}</script></span></li>
</ul>
<h3 id="question-2_1">Question 2<a class="headerlink" href="#question-2_1" title="Permanent link">&para;</a></h3>
<p>Given the following graph:</p>
<p><img alt="" src="../img/lecture_4_4.png" /></p>
<p>(a) What is a <em>maximal</em> clique? State one example from the graph.</p>
<p>(b) What is a <em>maximum</em> clique? State on example from the graph.</p>
<p>(c) Write down the factorized joint distribution that this graphical model represents</p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p><em>def</em>. A <a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)#Definitions"><strong>maximal clique</strong></a> is a clique that cannot be extended by including one more adjacent vertex.</p>
<p><em>def</em>. A <a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)#Definitions"><strong>maximum clique</strong></a> is a clique of the <em>largest possible size</em> in a given graph.</p>
<p>(b)</p>
<p>A <em>maximal clique</em> is show in blue, while a <em>maximum clique</em> is shown in green.</p>
<p><img alt="" src="../img/lecture_4_7.png" /></p>
<p>(c)</p>
<div>
<div class="MathJax_Preview">
p(x_1, ..., x_7) \propto \psi_{1, 2, 3}(x_1, x_2, x_3) \psi_{2, 3, 5}(x_2, x_3, x_5) \psi_{2, 4, 5}(x_2, x_4, x_5) \psi_{3, 5, 6}(x_3, x_5, x_6) \psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7)
</div>
<script type="math/tex; mode=display">
p(x_1, ..., x_7) \propto \psi_{1, 2, 3}(x_1, x_2, x_3) \psi_{2, 3, 5}(x_2, x_3, x_5) \psi_{2, 4, 5}(x_2, x_4, x_5) \psi_{3, 5, 6}(x_3, x_5, x_6) \psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7)
</script>
</div>
<h3 id="question-3_1">Question 3<a class="headerlink" href="#question-3_1" title="Permanent link">&para;</a></h3>
<p>Compare and contrast directed vs undirected graphical models:</p>
<p><strong>ANSWER</strong></p>
<p><center></p>
<table>
<thead>
<tr>
<th></th>
<th>DGMs</th>
<th>UGMs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Represented by</td>
<td>Directed graph</td>
<td>Undirected Graphs</td>
</tr>
<tr>
<td>Nodes specify</td>
<td>Random variables</td>
<td>Random variables</td>
</tr>
<tr>
<td>Edges specify</td>
<td>Conditional dependence between variables</td>
<td>Probabilistic interactions</td>
</tr>
<tr>
<td>Graph factorizes according to</td>
<td>Local conditional probabilities</td>
<td>Potential functions (or factors), one per maximal clique</td>
</tr>
<tr>
<td>Parameterized by</td>
<td>Conditional probability tables (if random variables are discrete)</td>
<td>Tables of non-negative, relative affinities (if random variables are discrete)</td>
</tr>
</tbody>
</table>
<p></center></p>
<h2 id="week-5-exact-inference">Week 5: Exact Inference<a class="headerlink" href="#week-5-exact-inference" title="Permanent link">&para;</a></h2>
<h3 id="question-1_4">Question 1<a class="headerlink" href="#question-1_4" title="Permanent link">&para;</a></h3>
<p>Given the graph</p>
<p><img alt="" src="../img/sample_midterm_3.png" /></p>
<p>(a) Suppose we want to compute the partition function (<span><span class="MathJax_Preview">Z(\theta)</span><script type="math/tex">Z(\theta)</script></span>, see <a href="../lectures/week_4/#parameterization-of-an-ugm">here</a>) using the elimination ordering <span><span class="MathJax_Preview">\prec= (1, 2, 3, 4, 5, 6)</span><script type="math/tex">\prec= (1, 2, 3, 4, 5, 6)</script></span>. If we use the <a href="../lectures/week_5/#variable-elimination">variable elimination algorithm</a>, we will create new intermediate factors. What is the largest intermediate factor?</p>
<p>(b) Add an edge to the original MRF between every pair of variables that end up in the same factor (These are called fill in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph?</p>
<p>(c) Now consider elimination ordering <span><span class="MathJax_Preview">\prec= (4, 1, 2, 3, 5, 6)</span><script type="math/tex">\prec= (4, 1, 2, 3, 5, 6)</script></span>. If we use the <a href="../lectures/week_5/#variable-elimination">variable elimination algorithm</a>, we will create new intermediate factors. What is the largest intermediate factor?</p>
<p>(d) Add an edge to the original MRF between every pair of variables that end up in the same factor (These are called fill in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph?</p>
<p><strong>ANSWER</strong></p>
<p>a) The size of the maximum factor is 3.</p>
<p>The set of potentials given by the graph is</p>
<div>
<div class="MathJax_Preview">
\Psi = \{\psi_{X_1, X_2}(X_1, X_2), \psi_{X_1, X_3}(X_1, X_3), \psi_{X_2, X_4}(X_2, X_4), \psi_{X_3, X_4}(X_3, X_4), \psi_{X_4, X_5}(X_4, X_5), \psi_{X_5, X_6}(X_5, X_6) \}
</div>
<script type="math/tex; mode=display">
\Psi = \{\psi_{X_1, X_2}(X_1, X_2), \psi_{X_1, X_3}(X_1, X_3), \psi_{X_2, X_4}(X_2, X_4), \psi_{X_3, X_4}(X_3, X_4), \psi_{X_4, X_5}(X_4, X_5), \psi_{X_5, X_6}(X_5, X_6) \}
</script>
</div>
<p>and the joint probability is therefore</p>
<div>
<div class="MathJax_Preview">
p(X_1, ..., X_6) \propto \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
</div>
<script type="math/tex; mode=display">
p(X_1, ..., X_6) \propto \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
</script>
</div>
<p>finally, the partition function with elimination ordering <span><span class="MathJax_Preview">\prec= (1, 2, 3, 4, 5, 6)</span><script type="math/tex">\prec= (1, 2, 3, 4, 5, 6)</script></span> is given by</p>
<div>
<div class="MathJax_Preview">\begin{align}
\tau(X) &amp;= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&amp;= \sum_{x_6}\sum_{x_5}\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\tau(X) &= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&= \sum_{x_6}\sum_{x_5}\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</script>
</div>
<p>Carrying out the elimination, (not shown here), we get intermediate factors</p>
<div>
<div class="MathJax_Preview">
\{\tau_1(x_2, x_3), \tau_2(x_3, x_4), \tau_3(x_4), \tau_4(x_5), \tau_5(x_6)\}
</div>
<script type="math/tex; mode=display">
\{\tau_1(x_2, x_3), \tau_2(x_3, x_4), \tau_3(x_4), \tau_4(x_5), \tau_5(x_6)\}
</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> the maximum factor is of size 3.</p>
<p>b) The only edge that does not already exist is the edge between <span><span class="MathJax_Preview">X_2</span><script type="math/tex">X_2</script></span> and <span><span class="MathJax_Preview">X_3</span><script type="math/tex">X_3</script></span> (added by intermediate factor <span><span class="MathJax_Preview">\tau_1(x_2, x_3)</span><script type="math/tex">\tau_1(x_2, x_3)</script></span>). The largest maximal clique is now of size 3 (<span><span class="MathJax_Preview">\{x_2, x_3, x_4\}</span><script type="math/tex">\{x_2, x_3, x_4\}</script></span>).</p>
<p>(c)</p>
<p>The partition function with elimination ordering <span><span class="MathJax_Preview">\prec= (4, 1, 2, 3, 5, 6)</span><script type="math/tex">\prec= (4, 1, 2, 3, 5, 6)</script></span> is given by</p>
<div>
<div class="MathJax_Preview">\begin{align}
\tau(X) &amp;= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&amp;= \sum_{x_6}\sum_{x_5}\sum_{x_3}\sum_{x_2}\sum_{x_1}\sum_{x_4} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\tau(X) &= \sum_z \prod_{\psi \in \Psi} \psi(z_{Scope[\psi] \cap Z}, x_{Scope[\psi] \cap X}) \\
&= \sum_{x_6}\sum_{x_5}\sum_{x_3}\sum_{x_2}\sum_{x_1}\sum_{x_4} \psi_{X_1, X_2}(X_1, X_2)\psi_{X_1, X_3}(X_1, X_3)\psi_{X_2, X_4}(X_2, X_4)\psi_{X_3, X_4}(X_3, X_4)\psi_{X_4, X_5}(X_4, X_5)\psi_{X_5, X_6}(X_5, X_6)
\end{align}</script>
</div>
<p>Carrying out the elimination, (not shown here), we get intermediate factors</p>
<div>
<div class="MathJax_Preview">
\{\tau_1(x_2, x_3, x_5), \tau_2(x_2, x_3, x_5), \tau_3(x_3, x_5), \tau_4(x_5), \tau_5(x_6)\}
</div>
<script type="math/tex; mode=display">
\{\tau_1(x_2, x_3, x_5), \tau_2(x_2, x_3, x_5), \tau_3(x_3, x_5), \tau_4(x_5), \tau_5(x_6)\}
</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> the maximum factor is of size 4.</p>
<p>d) The added edges are between</p>
<ul>
<li><span><span class="MathJax_Preview">X_2</span><script type="math/tex">X_2</script></span> and <span><span class="MathJax_Preview">X_3</span><script type="math/tex">X_3</script></span> (added by intermediate factors <span><span class="MathJax_Preview">\tau_1(x_2, x_3, x_5)</span><script type="math/tex">\tau_1(x_2, x_3, x_5)</script></span> / <span><span class="MathJax_Preview">\tau_2(x_2, x_3, x_5)</span><script type="math/tex">\tau_2(x_2, x_3, x_5)</script></span>)</li>
<li><span><span class="MathJax_Preview">X_2</span><script type="math/tex">X_2</script></span> and <span><span class="MathJax_Preview">X_5</span><script type="math/tex">X_5</script></span> (added by intermediate factors <span><span class="MathJax_Preview">\tau_1(x_2, x_3, x_5)</span><script type="math/tex">\tau_1(x_2, x_3, x_5)</script></span> / <span><span class="MathJax_Preview">\tau_2(x_2, x_3, x_5)</span><script type="math/tex">\tau_2(x_2, x_3, x_5)</script></span>)</li>
<li><span><span class="MathJax_Preview">X_3</span><script type="math/tex">X_3</script></span> and <span><span class="MathJax_Preview">X_5</span><script type="math/tex">X_5</script></span> (added by intermediate factors <span><span class="MathJax_Preview">\tau_1(x_2, x_3, x_5)</span><script type="math/tex">\tau_1(x_2, x_3, x_5)</script></span> , <span><span class="MathJax_Preview">\tau_2(x_2, x_3, x_5)</span><script type="math/tex">\tau_2(x_2, x_3, x_5)</script></span>, <span><span class="MathJax_Preview">\tau_3(x_3, x_5)</span><script type="math/tex">\tau_3(x_3, x_5)</script></span>)</li>
</ul>
<p>The largest maximal clique is now of size 4 (<span><span class="MathJax_Preview">\{x_2, x_3, x_4, x_5\}</span><script type="math/tex">\{x_2, x_3, x_4, x_5\}</script></span>).</p>
<h2 id="week-6-variational-inference">Week 6: Variational Inference<a class="headerlink" href="#week-6-variational-inference" title="Permanent link">&para;</a></h2>
<h3 id="question-1_5">Question 1<a class="headerlink" href="#question-1_5" title="Permanent link">&para;</a></h3>
<p>For the given tree</p>
<p><img alt="" src="../img/lecture_6_1.png" /></p>
<p>(a) Compute <span><span class="MathJax_Preview">p(x_1)</span><script type="math/tex">p(x_1)</script></span>, with <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span> as root.</p>
<p>(b) Assume each node can take on 1 on <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> discrete states. What is the runtime of this algorithm?</p>
<p><strong>ANSWER</strong></p>
<p>(a)</p>
<p>First, pass messages from leafs to <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span></p>
<div>
<div class="MathJax_Preview">\begin{align}
  m_{5 \rightarrow 1}(x_1) &amp;= \sum_{x_5} \phi_5(x_5)\phi_{15}(x_1, x_5) \\
  m_{3 \rightarrow 2}(x_2) &amp;= \sum_{x_3} \phi_3(x_3)\phi_{23}(x_2, x_3) \\
  m_{4 \rightarrow 2}(x_2) &amp;= \sum_{x_4} \phi_4(x_4)\phi_{24}(x_2, x_4) \\
  m_{2 \rightarrow 1}(x_1) &amp;= \sum_{x_2} \phi_2(x_2)\phi_{12}(x_1, x_2)m_{3 \rightarrow 2}(x_2)m_{4 \rightarrow 2}(x_2)
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
  m_{5 \rightarrow 1}(x_1) &= \sum_{x_5} \phi_5(x_5)\phi_{15}(x_1, x_5) \\
  m_{3 \rightarrow 2}(x_2) &= \sum_{x_3} \phi_3(x_3)\phi_{23}(x_2, x_3) \\
  m_{4 \rightarrow 2}(x_2) &= \sum_{x_4} \phi_4(x_4)\phi_{24}(x_2, x_4) \\
  m_{2 \rightarrow 1}(x_1) &= \sum_{x_2} \phi_2(x_2)\phi_{12}(x_1, x_2)m_{3 \rightarrow 2}(x_2)m_{4 \rightarrow 2}(x_2)
\end{align}</script>
</div>
<p>Finally, compute</p>
<div>
<div class="MathJax_Preview">
p(x_1) \propto \phi_1(x_1)m_{2 \rightarrow 1}(x_1)m_{5 \rightarrow 1}(x_1)
</div>
<script type="math/tex; mode=display">
p(x_1) \propto \phi_1(x_1)m_{2 \rightarrow 1}(x_1)m_{5 \rightarrow 1}(x_1)
</script>
</div>
<p>Note that</p>
<div>
<div class="MathJax_Preview">
p(x_1) = \frac{1}{Z}\phi_1(x_1)m_{2 \rightarrow 1}(x_1)m_{5 \rightarrow 1}(x_1)
</div>
<script type="math/tex; mode=display">
p(x_1) = \frac{1}{Z}\phi_1(x_1)m_{2 \rightarrow 1}(x_1)m_{5 \rightarrow 1}(x_1)
</script>
</div>
<p>where</p>
<div>
<div class="MathJax_Preview">
Z = \sum_{x_1} p(x_1)
</div>
<script type="math/tex; mode=display">
Z = \sum_{x_1} p(x_1)
</script>
</div>
<p>To compute <span><span class="MathJax_Preview">p(x_i) \ \forall_i</span><script type="math/tex">p(x_i) \ \forall_i</script></span> we would need to pass messages from the root to the leafs and compute</p>
<div>
<div class="MathJax_Preview">
p(x_i) \propto \phi(x_i) \prod_{j \in N(i)} m_{j \rightarrow i} (x_i), \ \forall_i.
</div>
<script type="math/tex; mode=display">
p(x_i) \propto \phi(x_i) \prod_{j \in N(i)} m_{j \rightarrow i} (x_i), \ \forall_i.
</script>
</div>
<p>(b)</p>
<p>Because we made only a single pass, runtime is that of variable elimination, i.e. <span><span class="MathJax_Preview">\mathcal O(nk^2) = O(5k^2)</span><script type="math/tex">\mathcal O(nk^2) = O(5k^2)</script></span> where <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> is our number of nodes and <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> is the number of discrete states of each node.</p>
<p>If we were to compute <span><span class="MathJax_Preview">p(x_i) \ \forall_i</span><script type="math/tex">p(x_i) \ \forall_i</script></span>, the runtime would be 2 times the runtime of variable elimination, i.e. <span><span class="MathJax_Preview">\mathcal O(nk^2) = O(5k^2)</span><script type="math/tex">\mathcal O(nk^2) = O(5k^2)</script></span>.</p>
<h3 id="question-2_2">Question 2<a class="headerlink" href="#question-2_2" title="Permanent link">&para;</a></h3>
<p>Imagine that we have two distributions: the true distribution <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span>, and the approximate distribution <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span>. Say further that we are trying to minimize the KL-Divergence between the true and approximate distribution.</p>
<p>Explain the difference between the <em>forward</em></p>
<div>
<div class="MathJax_Preview">
D_{KL}(P(X) || Q(X)) = \int P(x) \log \frac{P(x)}{Q(x)}
</div>
<script type="math/tex; mode=display">
D_{KL}(P(X) || Q(X)) = \int P(x) \log \frac{P(x)}{Q(x)}
</script>
</div>
<p>and <em>reverse</em></p>
<div>
<div class="MathJax_Preview">
D_{KL}(Q(X) || P(X)) = \int Q(x) \log \frac{Q(x)}{P(x)}
</div>
<script type="math/tex; mode=display">
D_{KL}(Q(X) || P(X)) = \int Q(x) \log \frac{Q(x)}{P(x)}
</script>
</div>
<p>KL-Divergence for these two distributions.</p>
<p><strong>ANSWER</strong></p>
<p>In the <strong>forward KL-Divergence</strong>, the difference between two distributions <span><span class="MathJax_Preview">P(X)</span><script type="math/tex">P(X)</script></span> and <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> is weighted by <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span></p>
<div>
<div class="MathJax_Preview">
D_{KL}(P(X) || Q(X)) = \int P(x) \log \frac{P(x)}{Q(x)}
</div>
<script type="math/tex; mode=display">
D_{KL}(P(X) || Q(X)) = \int P(x) \log \frac{P(x)}{Q(x)}
</script>
</div>
<p>The optimization procedure only penalizes a difference in density between <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span> and <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> for regions where <span><span class="MathJax_Preview">P(x) &gt; 0</span><script type="math/tex">P(x) > 0</script></span>. A difference in density between <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> and <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span> do not contribute to the KL-Divergence when <span><span class="MathJax_Preview">P(x) = 0</span><script type="math/tex">P(x) = 0</script></span>.</p>
<p><img alt="" src="../img/sample_midterm_5.png" /></p>
<p>For this reason, the forward KL-Divergence is referred to as <em>zero-avoiding</em> as it is avoiding <span><span class="MathJax_Preview">Q(x) = 0</span><script type="math/tex">Q(x) = 0</script></span> whenever <span><span class="MathJax_Preview">P(x) &gt; 0</span><script type="math/tex">P(x) > 0</script></span>.</p>
<p>In the <strong>backward KL-Divergence</strong>, the difference between two distributions <span><span class="MathJax_Preview">P(X)</span><script type="math/tex">P(X)</script></span> and <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> is weighted by <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span></p>
<div>
<div class="MathJax_Preview">
D_{KL}(Q(X) || P(X)) = \int Q(x) \log \frac{Q(x)}{P(x)}
</div>
<script type="math/tex; mode=display">
D_{KL}(Q(X) || P(X)) = \int Q(x) \log \frac{Q(x)}{P(x)}
</script>
</div>
<p>The optimization procedure only penalizes a difference in density between <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span> and <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> for regions where <span><span class="MathJax_Preview">Q(x) &gt; 0</span><script type="math/tex">Q(x) > 0</script></span>. A difference in density between <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> and <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span> does not contribute to the KL-Divergence when <span><span class="MathJax_Preview">Q(x) = 0</span><script type="math/tex">Q(x) = 0</script></span>.</p>
<p><img alt="" src="../img/sample_midterm_6.png" /></p>
<p>For this reason, the forward KL-Divergence is referred to as <em>zero-forcing</em> as it forces <span><span class="MathJax_Preview">Q(x)</span><script type="math/tex">Q(x)</script></span> to be 0 on some areas, even if <span><span class="MathJax_Preview">P(x)&gt;0</span><script type="math/tex">P(x)>0</script></span>.</p>
<div class="admonition cite">
<p class="admonition-title">Cite</p>
<p>Images came from <a href="https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/">this</a> blog.</p>
</div>
<h2 id="week-8-sampling-and-monte-carlo-methods">Week 8: Sampling and Monte Carlo Methods<a class="headerlink" href="#week-8-sampling-and-monte-carlo-methods" title="Permanent link">&para;</a></h2>
<h3 id="question-1_6">Question 1<a class="headerlink" href="#question-1_6" title="Permanent link">&para;</a></h3>
<p>Given some data <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1} \sim p(x)</span><script type="math/tex">\{x^{(r)}\}^R_{r=1} \sim p(x)</script></span>, the simple Monte Carlo estimator is</p>
<div>
<div class="MathJax_Preview">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</div>
<script type="math/tex; mode=display">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</script>
</div>
<p>(a) Show that this estimator is unbiased.</p>
<p>(b) Show that, as the number of samples of <span><span class="MathJax_Preview">R</span><script type="math/tex">R</script></span> increases, the variance of <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> will decrease proportional to <span><span class="MathJax_Preview">\frac{1}{R}</span><script type="math/tex">\frac{1}{R}</script></span>.</p>
<p><strong>ANSWER</strong></p>
<p>(a) To show that <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> is an unbiased estimator of <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span> we must show that for random samples <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1}</span><script type="math/tex">\{x^{(r)}\}^R_{r=1}</script></span> generated from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>, the expectation of <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> is <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span>:</p>
<div>
<div class="MathJax_Preview">
\mathbb E [\hat \Phi]_{x \sim p(\{x^{(r)}\}^R_{r=1})} = \mathbb E \bigg [ \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \bigg ] \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E \big [ \phi(x^{(r)}) \big ]  \\
= \frac{1}{R} \sum_{r=1}^R \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \frac{R}{R} \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \Phi \quad \square
</div>
<script type="math/tex; mode=display">
\mathbb E [\hat \Phi]_{x \sim p(\{x^{(r)}\}^R_{r=1})} = \mathbb E \bigg [ \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \bigg ] \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E \big [ \phi(x^{(r)}) \big ]  \\
= \frac{1}{R} \sum_{r=1}^R \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \frac{R}{R} \underset{x \sim p(x)}{\operatorname{\mathbb E}}  \big [ \phi(x) \big ] \\
= \Phi \quad \square
</script>
</div>
<p>(b)</p>
<div>
<div class="MathJax_Preview">
\text{var}[\hat \Phi] = \text{var} \bigg [ \frac{1}{R}\sum^R_{r=1}\phi(x^{(r)}) \bigg ] \\
= \frac{1}{R^2} \text{var} \bigg [\sum^R_{r=1}\phi(x^{(r)}) \bigg ]\\
= \frac{1}{R^2} \sum^R_{r=1} \text{var} \bigg [\phi(x^{(r)}) \bigg ] \\
= \frac{R}{R^2} \text{var} [\phi(x) ] \\
= \frac{1}{R} \text{var} [\phi(x) ] \quad \square
</div>
<script type="math/tex; mode=display">
\text{var}[\hat \Phi] = \text{var} \bigg [ \frac{1}{R}\sum^R_{r=1}\phi(x^{(r)}) \bigg ] \\
= \frac{1}{R^2} \text{var} \bigg [\sum^R_{r=1}\phi(x^{(r)}) \bigg ]\\
= \frac{1}{R^2} \sum^R_{r=1} \text{var} \bigg [\phi(x^{(r)}) \bigg ] \\
= \frac{R}{R^2} \text{var} [\phi(x) ] \\
= \frac{1}{R} \text{var} [\phi(x) ] \quad \square
</script>
</div>
<h3 id="question-2_3">Question 2<a class="headerlink" href="#question-2_3" title="Permanent link">&para;</a></h3>
<p>Starting from <span><span class="MathJax_Preview">\Phi = \int \phi(x)p(x)dx</span><script type="math/tex">\Phi = \int \phi(x)p(x)dx</script></span>, derive the importance weighted estimator <span><span class="MathJax_Preview">\hat \Phi_{iw}</span><script type="math/tex">\hat \Phi_{iw}</script></span> given</p>
<div>
<div class="MathJax_Preview">
\tilde w_r = \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}
</div>
<script type="math/tex; mode=display">
\tilde w_r = \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}
</script>
</div>
<p><strong>ANSWER</strong></p>
<div>
<div class="MathJax_Preview">\begin{align}
\Phi &amp;= \int \phi(x)p(x)dx \\
&amp;= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x)dx \\
&amp;\approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)})\frac{p(x^{(r)})}{q(x^{(r)})} \\
&amp;= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})} \\
&amp;= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\
&amp;= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\
&amp;= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  w_r \\
&amp;= \hat \Phi_{iw}
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\Phi &= \int \phi(x)p(x)dx \\
&= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x)dx \\
&\approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)})\frac{p(x^{(r)})}{q(x^{(r)})} \\
&= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})} \\
&= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\
&= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\
&= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  w_r \\
&= \hat \Phi_{iw}
\end{align}</script>
</div>
<p>where <span><span class="MathJax_Preview">\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R \tilde w_r</span><script type="math/tex">\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R \tilde w_r</script></span>, <span><span class="MathJax_Preview">w_r = \frac{\tilde w_r}{\sum_{r=1}^R \tilde w_r}</span><script type="math/tex">w_r = \frac{\tilde w_r}{\sum_{r=1}^R \tilde w_r}</script></span> and <span><span class="MathJax_Preview">\hat \Phi_{iw}</span><script type="math/tex">\hat \Phi_{iw}</script></span> is our importance weighted estimator.</p>
<h2 id="week-9-hidden-markov-models">Week 9: Hidden Markov Models<a class="headerlink" href="#week-9-hidden-markov-models" title="Permanent link">&para;</a></h2>
<h3 id="question-1_7">Question 1<a class="headerlink" href="#question-1_7" title="Permanent link">&para;</a></h3>
<p>Assume <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a discrete random variable with <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> states. How many parameters are needed to parameterize</p>
<p>(a) <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span>?</p>
<p>(b) A <em>first-order</em> Markov chain?</p>
<p>(c) An <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span><em>-ordered</em> Markov chain?</p>
<p><strong>ANSWER</strong></p>
<p>(a) <span><span class="MathJax_Preview">k-1</span><script type="math/tex">k-1</script></span>, as the last state is implicit.</p>
<p>(b) <span><span class="MathJax_Preview">k(k-1)</span><script type="math/tex">k(k-1)</script></span>, as we need <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> number of parameters for each parameter of <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span></p>
<p>(c) <span><span class="MathJax_Preview">k^m(k-1)</span><script type="math/tex">k^m(k-1)</script></span>, as we need <span><span class="MathJax_Preview">k^m</span><script type="math/tex">k^m</script></span> number of parameters for each parameter of <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span></p>
<h3 id="question-2_4">Question 2<a class="headerlink" href="#question-2_4" title="Permanent link">&para;</a></h3>
<p>Say we have the following simple chain</p>
<p><img alt="" src="../img/lecture_8_3.png" /></p>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">x_t \in [N, Z, A]</span><script type="math/tex">x_t \in [N, Z, A]</script></span></li>
<li><span><span class="MathJax_Preview">z_t \in [H, S]</span><script type="math/tex">z_t \in [H, S]</script></span></li>
</ul>
<p>where our observed states are whether or not we are watching Netflix (<span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>), sleeping (<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>), or working on the assignment (<span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>) and our hidden states are whether we are happy (<span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span>) or sad (<span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span>). Say futher that we are given the initial (<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>), transition (<span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>), and emission probabilities (<span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span>)</p>
<p><center></p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.70</td>
</tr>
<tr>
<td>S</td>
<td>0.30</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span></th>
<th>N</th>
<th>Z</th>
<th>A</th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.40</td>
<td>0.50</td>
<td>0.10</td>
</tr>
<tr>
<td>S</td>
<td>0.10</td>
<td>0.30</td>
<td>0.60</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th>T</th>
<th>H</th>
<th>S</th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.80</td>
<td>0.20</td>
</tr>
<tr>
<td>S</td>
<td>0.10</td>
<td>0.90</td>
</tr>
</tbody>
</table>
<p></center></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is the <em>rows</em> of these tables that need to sum to 1, not the columns!</p>
</div>
<p>From these conditional probabilities, compute</p>
<ul>
<li><span><span class="MathJax_Preview">p(z_3 = H | z_1 = S) = ?</span><script type="math/tex">p(z_3 = H | z_1 = S) = ?</script></span></li>
<li><span><span class="MathJax_Preview">p(x_3 = A | z_1 = S) = ?</span><script type="math/tex">p(x_3 = A | z_1 = S) = ?</script></span></li>
</ul>
<p><strong>ANSWER</strong></p>
<div>
<div class="MathJax_Preview">
p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\
= (0.80)(0.1) + (0.10)(0.90) \\
= 0.17 \\
</div>
<script type="math/tex; mode=display">
p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\
= (0.80)(0.1) + (0.10)(0.90) \\
= 0.17 \\
</script>
</div>
<p>and</p>
<div>
<div class="MathJax_Preview">
p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\
= (0.10)(0.17) + (0.60)(1 - 0.17) \\
= 0.515
</div>
<script type="math/tex; mode=display">
p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\
= (0.10)(0.17) + (0.60)(1 - 0.17) \\
= 0.515
</script>
</div>
<h2 id="week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi">Week 10: Stochastic Variational Inference / Automatic Differentiation Variation Inference (SAD VI)<a class="headerlink" href="#week-10-stochastic-variational-inference-automatic-differentiation-variation-inference-sad-vi" title="Permanent link">&para;</a></h2>
<h3 id="question-1_8">Question 1<a class="headerlink" href="#question-1_8" title="Permanent link">&para;</a></h3>
<p>Starting from the Kullback–Leibler divergence (<span><span class="MathJax_Preview">D_{KL}</span><script type="math/tex">D_{KL}</script></span>), derive the Evidence Lower Bound (ELBO) for a true distribution <span><span class="MathJax_Preview">p_\theta (z | x)</span><script type="math/tex">p_\theta (z | x)</script></span> and approximate distribution <span><span class="MathJax_Preview">q_\phi(z|x)</span><script type="math/tex">q_\phi(z|x)</script></span> and show that maximizing the ELBO is equivalent to minimizing <span><span class="MathJax_Preview">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</span><script type="math/tex">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</script></span>.</p>
<p><strong>ANSWER</strong></p>
<div>
<div class="MathJax_Preview">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &amp;= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z | x)} \\
  &amp;= E_{z_\phi \sim q_\phi} \Bigg [ \log \Bigg ( q_\phi(z | x) \cdot \frac{p_\theta(x)}{p_\theta(z, x)} \Bigg ) \Bigg ] \\
  &amp;= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z, x)}  + E_{z_\phi \sim q_\phi} \log p_\theta(x) \\
  &amp;= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z | x)} \\
  &= E_{z_\phi \sim q_\phi} \Bigg [ \log \Bigg ( q_\phi(z | x) \cdot \frac{p_\theta(x)}{p_\theta(z, x)} \Bigg ) \Bigg ] \\
  &= E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(z, x)}  + E_{z_\phi \sim q_\phi} \log p_\theta(x) \\
  &= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
\end{align}</script>
</div>
<p>Where <span><span class="MathJax_Preview">\mathcal L(\theta, \phi ; x)</span><script type="math/tex">\mathcal L(\theta, \phi ; x)</script></span> is the <strong>ELBO</strong>. Rearranging,</p>
<div>
<div class="MathJax_Preview">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &amp;= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
  \Rightarrow \mathcal L(\theta, \phi ; x) + D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &amp;= \log p_\theta(x) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
  D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &= -\mathcal L(\theta, \phi ; x)  + \log p_\theta(x) \\
  \Rightarrow \mathcal L(\theta, \phi ; x) + D_{KL}(q_\phi (z | x) || p_\theta (z | x)) &= \log p_\theta(x) \\
\end{align}</script>
</div>
<p>because <span><span class="MathJax_Preview">D_{KL}(q_\phi (z | x) || p_\theta (z | x)) \ge 0</span><script type="math/tex">D_{KL}(q_\phi (z | x) || p_\theta (z | x)) \ge 0</script></span></p>
<div>
<div class="MathJax_Preview">
\mathcal L(\theta, \phi ; x) \le \log p_\theta(x)
</div>
<script type="math/tex; mode=display">
\mathcal L(\theta, \phi ; x) \le \log p_\theta(x)
</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> maximizing the ELBO <span><span class="MathJax_Preview">\Rightarrow</span><script type="math/tex">\Rightarrow</script></span> minimizing <span><span class="MathJax_Preview">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</span><script type="math/tex">D_{KL}(q_\phi (z | x) || p_\theta (z | x))</script></span>.</p>
<h2 id="week-12-generative-adversarial-networks-gans">Week 12: Generative Adversarial Networks (GANs)<a class="headerlink" href="#week-12-generative-adversarial-networks-gans" title="Permanent link">&para;</a></h2>
<h3 id="question-1_9">Question 1<a class="headerlink" href="#question-1_9" title="Permanent link">&para;</a></h3>
<p>Compare and contrast <em>explicit</em> vs. <em>implicit</em> density models.</p>
<p><strong>ANSWER</strong></p>
<p><strong>Explicit density models</strong> define an explicit density function <span><span class="MathJax_Preview">p_{model}(x ; \theta)</span><script type="math/tex">p_{model}(x ; \theta)</script></span> which is used to train the model, typically via maximum likelihood estimation. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill.</p>
<p>e.g. for the i.i.d case:</p>
<div>
<div class="MathJax_Preview">
\hat \theta_{MLE} = \underset{\theta}{\operatorname{argmax}} \sum \log p_{model} (x^{(i)} ; \theta)
</div>
<script type="math/tex; mode=display">
\hat \theta_{MLE} = \underset{\theta}{\operatorname{argmax}} \sum \log p_{model} (x^{(i)} ; \theta)
</script>
</div>
<p>which is equivalent to the minimizing the KL divergence between <span><span class="MathJax_Preview">p_{model}(x)</span><script type="math/tex">p_{model}(x)</script></span> and <span><span class="MathJax_Preview">p_{model}(x ; \theta)</span><script type="math/tex">p_{model}(x ; \theta)</script></span></p>
<div>
<div class="MathJax_Preview">
\underset{\theta}{\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \theta))
</div>
<script type="math/tex; mode=display">
\underset{\theta}{\operatorname{argmin}} D_{KL}(p_{data}(x) || p_{model}(x ; \theta))
</script>
</div>
<p>when <span><span class="MathJax_Preview">p_{model}\theta(x ; \theta)</span><script type="math/tex">p_{model}\theta(x ; \theta)</script></span> is intractable, we use variational (e.g. VAEs) or MCMC approximations to compute the likelihood.</p>
<p>In contrast, <strong>implicit density models</strong> are trained without explicitly defining a density function. The way we interact with <span><span class="MathJax_Preview">p_{model}</span><script type="math/tex">p_{model}</script></span> is through samples. An example a General Adversarial Networks (GANs).</p>
<h3 id="question-2_5">Question 2<a class="headerlink" href="#question-2_5" title="Permanent link">&para;</a></h3>
<p>The goal of the discriminator is to minimize</p>
<div>
<div class="MathJax_Preview">
J^{(D)}(\theta_D, \theta_G) = -\mathbb E_{x \sim p_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</div>
<script type="math/tex; mode=display">
J^{(D)}(\theta_D, \theta_G) = -\mathbb E_{x \sim p_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</script>
</div>
<p>with respect to <span><span class="MathJax_Preview">\theta_D</span><script type="math/tex">\theta_D</script></span>. Imagine that the discriminator can be optimized in function space, so the value of <span><span class="MathJax_Preview">D(x)</span><script type="math/tex">D(x)</script></span> is specified independently for every value of x.</p>
<p>(a) What is the optimal strategy for the discriminator, <span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>?</p>
<p>(b) What assumptions need to be made to obtain this result?</p>
<p><strong>ANSWER</strong></p>
<p>(b)</p>
<p>We begin by assuming that both <span><span class="MathJax_Preview">p_{data}</span><script type="math/tex">p_{data}</script></span> and <span><span class="MathJax_Preview">p_{model}</span><script type="math/tex">p_{model}</script></span> are nonzero everywhere.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we do not make this assumption, then some points are never visited during training, and have undefined behavior.</p>
</div>
<p>(a)</p>
<p>To minimize <span><span class="MathJax_Preview">J^{(D)}</span><script type="math/tex">J^{(D)}</script></span> with respect to <span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>, we can write down the functional
derivatives with respect to a single entry <span><span class="MathJax_Preview">D^{(x)}</span><script type="math/tex">D^{(x)}</script></span>, and set them equal to zero:</p>
<div>
<div class="MathJax_Preview">
\frac{\delta}{\delta D(x)}J^{(D)} = 0
</div>
<script type="math/tex; mode=display">
\frac{\delta}{\delta D(x)}J^{(D)} = 0
</script>
</div>
<p>By solving this equation, we obtain</p>
<div>
<div class="MathJax_Preview">
D^\star(x)  = \frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)}
</div>
<script type="math/tex; mode=display">
D^\star(x)  = \frac{p_{data}(x)}{p_{data}(x) + p_{model}(x)}
</script>
</div>
<p>Estimating this ratio is the key approximation mechanism used by GANs.</p>
<h3 id="question-3_2">Question 3<a class="headerlink" href="#question-3_2" title="Permanent link">&para;</a></h3>
<p>The goal of the generator is to minimize</p>
<div>
<div class="MathJax_Preview">
J^{(G)}(\theta_D, \theta_G) = -\mathbb E_{x \sim p_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</div>
<script type="math/tex; mode=display">
J^{(G)}(\theta_D, \theta_G) = -\mathbb E_{x \sim p_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</script>
</div>
<p>with respect to <span><span class="MathJax_Preview">\theta_G</span><script type="math/tex">\theta_G</script></span>.</p>
<p>What is the optimal strategy for the generator, <span><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span>?</p>
<p><strong>ANSWER</strong></p>
<p>The optimal behavior of the generator is to <em>try and confuse the discriminator</em>. We can frame this as a <strong>zero-sum game</strong>, in which the sum of all player’s costs is always zero. In this version of the game,</p>
<div>
<div class="MathJax_Preview">
J(^{(G)}) = -J^{(D)}
</div>
<script type="math/tex; mode=display">
J(^{(G)}) = -J^{(D)}
</script>
</div>
<p>Because <span><span class="MathJax_Preview">J^{(G)}</span><script type="math/tex">J^{(G)}</script></span> is tied directly to <span><span class="MathJax_Preview">J^{(D)}</span><script type="math/tex">J^{(D)}</script></span>, we can summarize the entire game
with a value function specifying the discriminator’s payoff:</p>
<div>
<div class="MathJax_Preview">
V(\theta^{(D)}, \theta^{(G)}) = -J^{(D)}(\theta^{(D)}, \theta^{(G)})
</div>
<script type="math/tex; mode=display">
V(\theta^{(D)}, \theta^{(G)}) = -J^{(D)}(\theta^{(D)}, \theta^{(G)})
</script>
</div>
<p>Zero-sum games are also called <strong>minimax</strong> games because their solution involves minimization in an outer loop and maximization in an inner loop:</p>
<div>
<div class="MathJax_Preview">
\theta^{(G)\star} = \underset{\theta^{(G)}}{\operatorname{argmin}} \underset{\theta^{(D)}}{\operatorname{argmax}} V(\theta^{(D)}, \theta^{(G)})
</div>
<script type="math/tex; mode=display">
\theta^{(G)\star} = \underset{\theta^{(G)}}{\operatorname{argmin}} \underset{\theta^{(D)}}{\operatorname{argmax}} V(\theta^{(D)}, \theta^{(G)})
</script>
</div>
<p>Where we want to maximize <span><span class="MathJax_Preview">\theta_D</span><script type="math/tex">\theta_D</script></span> such that</p>
<ul>
<li><span><span class="MathJax_Preview">D_{\theta_D}(x) = 1</span><script type="math/tex">D_{\theta_D}(x) = 1</script></span></li>
<li><span><span class="MathJax_Preview">D_{\theta_D}(G(z)) = 0</span><script type="math/tex">D_{\theta_D}(G(z)) = 0</script></span></li>
</ul>
<p>and minimize <span><span class="MathJax_Preview">\theta_G</span><script type="math/tex">\theta_G</script></span> such that</p>
<ul>
<li><span><span class="MathJax_Preview">D_{\theta_D}(G_{\theta_G}(z)) \rightarrow 1</span><script type="math/tex">D_{\theta_D}(G_{\theta_G}(z)) \rightarrow 1</script></span></li>
</ul>
<h3 id="question-4_1">Question 4<a class="headerlink" href="#question-4_1" title="Permanent link">&para;</a></h3>
<p>The cost used for the generator in the minimax game, <span><span class="MathJax_Preview">J(^{(G)}) = -J^{(D)}</span><script type="math/tex">J(^{(G)}) = -J^{(D)}</script></span> is useful for theoretical analysis, but does not perform especially well in practice.</p>
<p>(a) Explain what the <em>saturating problem</em> in GANs is.</p>
<p>(b) Explain the heuristic, non-saturating game used to solve this problem.</p>
<p><strong>ANSWER</strong></p>
<p>(a) In the minimax game, the discriminator <em>minimizes</em> a cross-entropy, but the generator <em>maximizes</em> the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, e.g. <span><span class="MathJax_Preview">D(G(z)) = 0</span><script type="math/tex">D(G(z)) = 0</script></span>, the generator’s gradient vanishes (the cost function becomes flat!). This is known as the <em>saturation problem</em>.</p>
<p><img alt="" src="../img/lecture_12_5.png" /></p>
<p>(b) To solve this problem, we introduce the non-saturating game, in which we continue to use cross-entropy minimization for the generator, but instead of flipping the sign on the discriminator’s cost to obtain a cost for the generator, we flip the target used to construct the cross-entropy cost</p>
<div>
<div class="MathJax_Preview">
J^{(G)} = -\underset{z \sim p(z)}{\operatorname{\mathbb E}} \log(D(G(z)))
</div>
<script type="math/tex; mode=display">
J^{(G)} = -\underset{z \sim p(z)}{\operatorname{\mathbb E}} \log(D(G(z)))
</script>
</div>
<p>This leads to a strong gradient signal at <span><span class="MathJax_Preview">D(G(z)) = 0</span><script type="math/tex">D(G(z)) = 0</script></span>.</p>
<p><img alt="" src="../img/lecture_12_6.png" /></p>
<p>In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the log- probability of the discriminator being mistaken.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../sample_midterm/" title="Sample Midterm" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Sample Midterm
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>